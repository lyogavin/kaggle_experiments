{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "roberta-large-tweet-sentiment-roberta-with-pp-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyogavin/kaggle_experiments/blob/master/roberta_large_tweet_sentiment_roberta_with_pp_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SnNGHZ9mqPc",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuosFIsJmvAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8437267-e5c3-4cbd-97f2-6d706e283d6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWX-2S837JK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir -p /content/drive/My\\ Drive/kaggleinput/roberta-large\n",
        "\n",
        "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/ahotrod/roberta_large_squad2/config.json\n",
        "#!wget https://cdn.huggingface.co/ahotrod/roberta_large_squad2/pytorch_model.bin\n",
        "\n",
        "#!wget https://cdn.huggingface.co/ahotrod/roberta_large_squad2/tokenizer_config.json\n",
        "#!wget https://cdn.huggingface.co/ahotrod/roberta_large_squad2/vocab.json\n",
        "\n",
        "#!cp config.json /content/drive/My\\ Drive/kaggleinput/roberta-large/\n",
        "#!cp pytorch_model.bin /content/drive/My\\ Drive/kaggleinput/roberta-large/\n",
        "#!cp tokenizer_config.json /content/drive/My\\ Drive/kaggleinput/roberta-large/\n",
        "#!cp vocab.json /content/drive/My\\ Drive/kaggleinput/roberta-large/\n",
        "\n",
        "!mv /content/drive/My\\ Drive/kaggleinput/roberta-large /content/drive/My\\ Drive/kaggle/input/"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE-fp6Obpkma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT_PATH=\"/content/drive/My Drive/kaggle\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vb_2PjzTym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1111452d-cd76-4199-a06c-58dffc0deeb4"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAtgWfjbyfoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flags:\n",
        "FOLD_COUNT = 5\n",
        "\n",
        "NOTEBOOK_ID = 0\n",
        "\n",
        "\n",
        "DEBUG_LOAD = False\n",
        "DEBUG_FOLD = False\n",
        "USE_MULTI_SAMPLE_DROPOUT = False #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L119\n",
        "USE_MULTI_SAMPLE_DROPOUT_RATE = 0.5\n",
        "USE_MULTI_SAMPLE_DROPOUT_SAMPLE = 4\n",
        "\n",
        "USE_BERT_ALL_LAYERS = False # https://www.kaggle.com/c/google-quest-challenge/discussion/129840\n",
        "#https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L76\n",
        "\n",
        "USE_BERT_LAST_N_LAYERS = 8\n",
        "\n",
        "USE_MULTIPLE_LEARNING_RATE = False #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L132\n",
        "USE_MULTIPLE_LEARNING_RATE_TIMES = 8\n",
        "\n",
        "\n",
        "GEN_PSUEDO_LABELS = False\n",
        "TRAIN_WITH_PSUEDO_LABELS = False\n",
        "\n",
        "USE_TRY_MULTI_SEEDING = False\n",
        "\n",
        "USE_SEQUENCE_BUCKETING = False\n",
        "USE_SMOOTH_LABELING = False\n",
        "LABEL_SMOOTH = 0.1\n",
        "\n",
        "\n",
        "PAD_ID = 1\n",
        "MAX_LEN = 96\n",
        "\n",
        "USE_SEED = 42\n",
        "\n",
        "USE_TRAIN_ROBERTA_LARGE = True\n",
        "USE_INFER_ROBERTA_LARGE = True"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvHmeSfLqaxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "bbccfc56-661d-4138-9e6f-a070a17217b9"
      },
      "source": [
        "!pip install tokenizers transformers"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "VP8YyNajmqPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "if USE_ROBERTA_LARGE:\n",
        "  from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JxIBCKatmqPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3f2caf7b-0c74-4fad-ed2f-095f365cfedc"
      },
      "source": [
        "import re\n",
        "\n",
        "USE_PP = False\n",
        "\n",
        "def pp(filtered_output, real_tweet):\n",
        "    #if not USE_PP:\n",
        "    #    return filtered_output\n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "            if filtered_output.endswith(\"..\"):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n",
        "                return filtered_output\n",
        "            if filtered_output.endswith('!!'):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n",
        "                return filtered_output\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "    return filtered_output\n",
        "\n",
        "def prp(filtered_output, real_tweet):\n",
        "    if not USE_PP:\n",
        "        return filtered_output\n",
        "    \n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "\n",
        "            st = real_tweet.find(filtered_output)\n",
        "            fl = real_tweet.find(\"  \")\n",
        "            end = st + len(filtered_output)\n",
        "            \n",
        "            #if fl != -1 and fl < st:\n",
        "            while st != -1 and end < len(real_tweet) and (real_tweet[end] == '.' or real_tweet[end] == '!'):\n",
        "                filtered_output = filtered_output + real_tweet[end]\n",
        "                end += 1\n",
        "                    \n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            #print(filtered_output)\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            \n",
        "            \n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "        \n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "\n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "    return filtered_output\n",
        "\n",
        "tweet = \"  ROFLMAO for the funny web portal  =D\"\n",
        "pred = \"funny\"\n",
        "answer = \"e funny\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "tweet = \" yea i just got outta one too....i want him back tho  but i feel the same way...i`m cool on dudes for a lil while\"\n",
        "pred = \"cool\"\n",
        "answer = \"m cool\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \"Ow... My shoulder muscle (I can`t remember the name :p) hurts... What did I do?  I don`t even know\"\n",
        "pred = \"hurts...\"\n",
        "answer = \"hurts..\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \" yep... or it should b automatic that if u fall 4 someone that person does 2!or smthng like that... but the way it is sucks!\"\n",
        "pred = \"SUCKS!\"\n",
        "answer = \"SUCKS!\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "\n",
        "tweet = \" hi holly i`ll volunteer to try it out first for u! hope ur having a fab weekend xoxox...\"\n",
        "pred = \"fab\"\n",
        "answer = \"g a\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e funny\n",
            "e funny\n",
            "m cool\n",
            "m cool\n",
            "hurts..\n",
            "hurts..\n",
            "SUCKS!\n",
            "SUCKS!\n",
            " fab \n",
            "g a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8-G9tYSXmqPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c00b9a0-bfc1-4b2d-b394-ba7ceb956fa9"
      },
      "source": [
        "\"  \" in tweet"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qb1WhQoSmqPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fifth_prp(select, tweet, sentiment, offsets):\n",
        "    #print('entering fifth prp')\n",
        "    ss = tweet.find(select)\n",
        "    if tweet[max(ss - 2, 0):ss] == '  ':\n",
        "        ss -= 2\n",
        "    if ss > 0  and tweet[ss - 1] == ' ':\n",
        "        ss -= 1\n",
        "\n",
        "    ee = ss + len(select)\n",
        "\n",
        "    if re.match(r' [^ ]', tweet) is not None:\n",
        "        ee -= 1\n",
        "\n",
        "    ss = max(0, ss)\n",
        "    if '  ' in tweet[:ss] and sentiment != 'neutral':\n",
        "        text1 = \" \".join(tweet.split())\n",
        "        sel = text1[ss:ee].strip()\n",
        "        if len(sel) > 1 and sel[-2] == ' ':\n",
        "            sel = sel[:-2]\n",
        "\n",
        "        select = sel\n",
        "\n",
        "    text1 = \" \"+\" \".join(tweet.split())\n",
        "    text2 = \" \".join(select.split()).lstrip(\".,;:\")\n",
        "\n",
        "    idx = text1.find(text2)\n",
        "    if idx != -1:\n",
        "        chars = np.zeros((len(text1)))\n",
        "        chars[idx:idx+len(text2)]=1\n",
        "        if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    else:\n",
        "        import pdb;pdb.set_trace()\n",
        "        chars = np.ones((len(text1)))\n",
        "        \n",
        "        \n",
        "    #tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    #        vocab_file='../input/roberta-base/vocab.json', \n",
        "    #        merges_file='../input/roberta-base/merges.txt', \n",
        "    #        lowercase=True,\n",
        "    #        add_prefix_space=True)\n",
        "    \n",
        "    #enc = tokenizer.encode(text1) \n",
        "\n",
        "    # ID_OFFSETS\n",
        "    #offsets = enc.offsets\n",
        "\n",
        "    # START END TOKENS\n",
        "    _toks = []\n",
        "\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.mean(chars[a:b])\n",
        "        #if (sm > 0.6 and chars[a] != 0):  # こうすると若干伸びるけど...\n",
        "        if (sm > 0.5 and chars[a] != 0): \n",
        "            _toks.append(i)\n",
        "            \n",
        "    #print(\"returnning fifth prep\")\n",
        "    return _toks[0], _toks[-1]\n",
        "\n",
        "    '''\n",
        "    toks = _toks\n",
        "    s_tok = sentiment_id[sentiments[k]]\n",
        "    input_ids[k, :len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask[k,:len(enc.ids)+3] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+2] = 1\n",
        "        end_tokens[k,toks[-1]+2] = 1  \n",
        "    '''"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85-gyl4mqP0",
        "colab_type": "text"
      },
      "source": [
        "# Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "pGNExtgrmqP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed = USE_SEED #42\n",
        "seed_everything(seed)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQXvA_r9mqP4",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QY9vlBLVmqP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, max_len=MAX_LEN, use_fifth=True):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.labeled = 'selected_text' in df\n",
        "        if False: #USE_ROBERTA_LARGE:\n",
        "          self.tokenizer = AutoTokenizer.from_pretrained(ROOT_PATH + '/input/roberta-large/')\n",
        "\n",
        "        else:\n",
        "          self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "              vocab_file= ROOT_PATH + '/input/roberta-base/vocab.json', \n",
        "              merges_file= ROOT_PATH + '/input/roberta-base/merges.txt', \n",
        "              lowercase=True,\n",
        "              add_prefix_space=True)\n",
        "        self.use_fifth = use_fifth\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = {}\n",
        "        row = self.df.iloc[index]\n",
        "        \n",
        "        #print(f\"getting {index}\")\n",
        "        \n",
        "        ids, masks, tweet, offsets, enc_offsets, padding_len = self.get_input_data(row)\n",
        "        data['ids'] = ids\n",
        "        data['original_tweet'] = row.text\n",
        "        data['masks'] = masks\n",
        "        data['tweet'] = tweet\n",
        "        data['offsets'] = offsets\n",
        "        #data['select'] = row.selected_text\n",
        "        data['sentiment'] = row.sentiment\n",
        "        \n",
        "        if self.labeled:\n",
        "            try:\n",
        "                #start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "                if self.use_fifth:\n",
        "                    start_idx, end_idx = fifth_prp(row.selected_text, row.text, row.sentiment, offsets)\n",
        "                else:\n",
        "                    start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "            \n",
        "            except:\n",
        "                #print(f\"tweet:[{tweet}], selected: [{row.selected_text}] prp: [{prp(row.selected_text, tweet)}]\")\n",
        "                start_idx = 4\n",
        "                end_idx = len(ids) - 4 - 2\n",
        "                \n",
        "            if USE_SEQUENCE_BUCKETING:\n",
        "              data['start_idx'] = min(start_idx, self.max_len - 1 - padding_len) # start_idx\n",
        "              data['end_idx'] = min(end_idx, self.max_len - 1 - padding_len) # end_idx\n",
        "            else:\n",
        "              data['start_idx'] = start_idx\n",
        "              data['end_idx'] = end_idx\n",
        "        \n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def get_input_data(self, row):\n",
        "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "        encoding = self.tokenizer.encode(tweet)\n",
        "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
        "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
        "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "                \n",
        "        pad_len = self.max_len - len(ids)\n",
        "        if pad_len > 0:\n",
        "            ids += [PAD_ID] * pad_len\n",
        "            offsets += [(0, 0)] * pad_len\n",
        "        \n",
        "        ids = torch.tensor(ids)\n",
        "        masks = torch.where(ids != PAD_ID, torch.tensor(1), torch.tensor(0))\n",
        "        offsets = torch.tensor(offsets)\n",
        "        \n",
        "        return ids, masks, tweet, offsets, encoding.offsets, pad_len\n",
        "        \n",
        "    def get_target_idx(self, row, tweet, offsets):\n",
        "        \n",
        "        #pp:\n",
        "        \n",
        "        selected_text = prp(row.selected_text.lower(), tweet)\n",
        "        #selected_text = row.selected_text\n",
        "        \n",
        "        if len(selected_text) == 0:\n",
        "            selected_text = row.selected_text\n",
        "        \n",
        "        selected_text = \" \" +  \" \".join(selected_text.lower().split())\n",
        "\n",
        "        len_st = len(selected_text) - 1\n",
        "        idx0 = None\n",
        "        idx1 = None\n",
        "\n",
        "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "                idx0 = ind\n",
        "                idx1 = ind + len_st - 1\n",
        "                break\n",
        "\n",
        "        char_targets = [0] * len(tweet)\n",
        "        if idx0 != None and idx1 != None:\n",
        "            for ct in range(idx0, idx1 + 1):\n",
        "                char_targets[ct] = 1\n",
        "\n",
        "        target_idx = []\n",
        "        for j, (offset1, offset2) in enumerate(offsets):\n",
        "            if sum(char_targets[offset1: offset2]) > 0:\n",
        "                target_idx.append(j)\n",
        "\n",
        "        start_idx = target_idx[0]\n",
        "        end_idx = target_idx[-1]\n",
        "        \n",
        "        return start_idx, end_idx\n",
        "        \n",
        "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    val_df = df.iloc[val_idx]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(train_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=2,\n",
        "        drop_last=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(val_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)\n",
        "\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "    return dataloaders_dict\n",
        "\n",
        "def get_test_loader(df, batch_size=32):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)    \n",
        "    return loader"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Vm0BqKmqP8",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bQIG2zlRmqP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TweetModel(nn.Module):\n",
        "    def __init__(self, model_type='roberta-base'):\n",
        "        super(TweetModel, self).__init__()\n",
        "        \n",
        "\n",
        "        if False: #USE_ROBERTA_LARGE:\n",
        "\n",
        "          config = AutoConfig.from_pretrained(ROOT_PATH + '/input/roberta-large/config.json')\n",
        "          self.roberta = AutoModelForQuestionAnswering.from_pretrained(ROOT_PATH + '/input/roberta-large/pytorch_model.bin', config=config)\n",
        "\n",
        "\n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            ROOT_PATH + '/input/%s/config.json' % model_type, output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            ROOT_PATH + '/input/%s/pytorch_model.bin' % model_type, config=config)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.high_dropout = nn.Dropout(USE_MULTI_SAMPLE_DROPOUT_RATE)\n",
        "        self.fc = nn.Linear(config.hidden_size, 2)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "\n",
        "        if USE_BERT_LAST_N_LAYERS == -1:\n",
        "          n_weights = config.num_hidden_layers\n",
        "        else:\n",
        "          n_weights = USE_BERT_LAST_N_LAYERS #config.num_hidden_layers + 1\n",
        "\n",
        "        self.n_layers = n_weights\n",
        "\n",
        "\n",
        "        weights_init = torch.zeros(n_weights).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "\n",
        "        self.multi_layer_dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        if USE_SEQUENCE_BUCKETING:\n",
        "          #padding = torch.eq(input_ids, PAD_ID).int()\n",
        "          padding = torch.where(input_ids == PAD_ID, torch.ones_like(input_ids), torch.zeros_like(input_ids))\n",
        "          lens = MAX_LEN - torch.sum(padding, -1)\n",
        "          #max_len = MAX_LEN -1  #torch.max(lens) - 1\n",
        "          max_len = torch.max(lens)# - 1\n",
        "\n",
        "          #max_len = torch.clamp(max_len, min=10, max=MAX_LEN)\n",
        "\n",
        "          #input_ids_last = torch.unsqueeze(input_ids[:, -1], 1)\n",
        "          #attention_mask_last = torch.unsqueeze(attention_mask[:, -1], 1)\n",
        "          input_ids = input_ids[:, :max_len]\n",
        "          attention_mask = attention_mask[:, :max_len]\n",
        "          #input_ids = torch.cat([input_ids, input_ids_last], dim=1)\n",
        "          #attention_mask = torch.cat([attention_mask, attention_mask_last], dim=1)\n",
        "          #tok_ = tok[:, :max_len]\n",
        "\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
        "         \n",
        "        if not USE_BERT_ALL_LAYERS:\n",
        "          x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
        "          x = torch.mean(x, 0)\n",
        "        else:\n",
        "\n",
        "          #print(f\"layers: { [hs[i].shape for i in range(-USE_BERT_LAST_N_LAYERS, 0, 1)]}\")\n",
        "\n",
        "\n",
        "\n",
        "          x = torch.stack(\n",
        "              [self.multi_layer_dropout(layer[:, :, :]) for layer in [hs[i] for i in range(-self.n_layers, 0, 1)]], dim=3\n",
        "          )\n",
        "          x = (torch.softmax(self.layer_weights, dim=0) * x).sum(-1)\n",
        "\n",
        "        if not USE_MULTI_SAMPLE_DROPOUT:\n",
        "          x = self.dropout(x)\n",
        "          x = self.fc(x)\n",
        "        else:\n",
        "          # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
        "          x = torch.mean(\n",
        "              torch.stack(\n",
        "                  [self.fc(self.high_dropout(x)) for _ in range(USE_MULTI_SAMPLE_DROPOUT_SAMPLE)],\n",
        "                  dim=0,\n",
        "              ),\n",
        "              dim=0,\n",
        "          )\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "                \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teaVSs9UmqQD",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "60TWdOqKmqQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# from: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
        "# This loss is similar to CrossEntropyLoss, i.e. it expects logits. Don't use a softmax as your last layer therefore.\n",
        "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
        "                    device=targets.device) \\\n",
        "                .fill_(smoothing /(n_classes-1)) \\\n",
        "                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n",
        "            self.smoothing)\n",
        "        lsm = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            lsm = lsm * self.weight.unsqueeze(0)\n",
        "\n",
        "        loss = -(targets * lsm).sum(-1)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    if not USE_SMOOTH_LABELING:\n",
        "      ce_loss = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "      ce_loss = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTH)\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)    \n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jXmN6ImqQH",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5mHp1NUWmqQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_selected_text(text, start_idx, end_idx, offsets):\n",
        "    selected_text = \"\"\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            selected_text += \" \"\n",
        "    #selected_text = pp(selected_text, text)\n",
        "    \n",
        "    return selected_text\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
        "    start_pred = np.argmax(start_logits)\n",
        "    end_pred = np.argmax(end_logits)\n",
        "    if start_pred > end_pred:\n",
        "        pred = text\n",
        "    else:\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
        "        \n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
        "    \n",
        "    return jaccard(true, pred)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3LRjNPdmqQM",
        "colab_type": "text"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DxZrmVbJmqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    jc_ret = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            epoch_jaccard = 0.0\n",
        "            \n",
        "            \n",
        "            \n",
        "            for data in (dataloaders_dict[phase]):\n",
        "                ids = data['ids'].cuda()\n",
        "                masks = data['masks'].cuda()\n",
        "                tweet = data['tweet']\n",
        "                offsets = data['offsets'].numpy()\n",
        "                start_idx = data['start_idx'].cuda()\n",
        "                end_idx = data['end_idx'].cuda()\n",
        "                \n",
        "                #print((start_idx,end_idx))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    start_logits, end_logits = model(ids, masks)\n",
        "\n",
        "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    epoch_loss += loss.item() * len(ids)\n",
        "                    \n",
        "                    start_idx = start_idx.cpu().detach().numpy()\n",
        "                    end_idx = end_idx.cpu().detach().numpy()\n",
        "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
        "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
        "                    \n",
        "                    for i in range(len(ids)):                        \n",
        "                        jaccard_score = compute_jaccard_score(\n",
        "                            tweet[i],\n",
        "                            start_idx[i],\n",
        "                            end_idx[i],\n",
        "                            start_logits[i], \n",
        "                            end_logits[i], \n",
        "                            offsets[i])\n",
        "                        epoch_jaccard += jaccard_score\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
        "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
        "            \n",
        "            if epoch == num_epochs - 1:\n",
        "              jc_ret = epoch_jaccard\n",
        "    \n",
        "    torch.save(model.state_dict(), filename)\n",
        "    return jc_ret"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbZ1-zxSmqQX",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hWp_K8EymqQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1s4mfoomqQl",
        "colab_type": "text"
      },
      "source": [
        "# **post process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "twfns9m1mqQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def modify_punc_length(text, selected_text):\n",
        "    m = re.search(r'[!\\.\\?]+$', selected_text)        \n",
        "    if m is None:\n",
        "        return selected_text\n",
        "    \n",
        "    conti_punc = len(m.group())\n",
        "\n",
        "    if conti_punc >= 4:\n",
        "        selected_text = selected_text[:-(conti_punc-2)]\n",
        "    elif conti_punc == 1:# 元のtextを探しに行く\n",
        "        tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", selected_text)\n",
        "        pat = re.sub(r\" \", \" +\", tmp)\n",
        "        m = re.search(pat, text)\n",
        "        f_idx0 = m.start()\n",
        "        f_idx1 = m.end()\n",
        "\n",
        "        if f_idx1 != len(text) and text[f_idx1] in (\"!\", \".\", \"?\"):\n",
        "            f_idx1 += 1\n",
        "            selected_text = text[f_idx0:f_idx1]\n",
        "    return selected_text\n",
        "\n",
        "\n",
        "import math\n",
        "def postprocess(tweet, offsets, aa, bb, sentiment):\n",
        "    text0 = tweet\n",
        "    text1 = \" \" + \" \".join(tweet.split())\n",
        "    #enc = tokenizer.encode(text1)\n",
        "\n",
        "    #aa = np.argmax(start_tokens[k])\n",
        "    #bb = np.argmax(end_tokens[k])\n",
        "\n",
        "    ss = offsets[aa][0]\n",
        "    ee = offsets[bb][1] \n",
        "    #st = text1[ss:ee].strip()\n",
        "    from collections import namedtuple\n",
        "    Row = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\n",
        "    row = Row(\n",
        "        original_text=text0,\n",
        "        normalized_text=text1,\n",
        "        sentiment=sentiment,\n",
        "        y_start_char=ss,\n",
        "        y_end_char=ee,\n",
        "    )\n",
        "    \n",
        "    if row.original_text == '':\n",
        "        return row.normalized_text.strip()\n",
        "    original_text = row.original_text.replace('\\t', '')\n",
        "    y_start_char = row.y_start_char\n",
        "    y_end_char = row.y_end_char\n",
        "    try:\n",
        "        y_selected_text = row.normalized_text[y_start_char:y_end_char].strip()\n",
        "    except:\n",
        "        print(f\"err: {row.normalized_text} {(y_start_char,y_end_char)}\")\n",
        "    if (y_end_char < len(row.normalized_text) and row.sentiment != 'neutral' and\n",
        "        y_selected_text[-1] == '.' and\n",
        "        (row.normalized_text[y_end_char] == '.' or \n",
        "         y_selected_text[-2] == '.')):\n",
        "        y_selected_text = re.sub('\\.+$', '..', y_selected_text)\n",
        "\n",
        "    tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", y_selected_text)\n",
        "    pat = re.sub(r\" \", \" +\", tmp)\n",
        "    m = re.search(pat, original_text)\n",
        "    if m is None:\n",
        "        print(row.normalized_text[y_start_char:y_end_char].strip())\n",
        "        print(row.normalized_text)\n",
        "        print(y_selected_text)\n",
        "    ss2 = m.start()\n",
        "    ee2 = m.end()\n",
        "    \n",
        "    # 'neutral' およびほぼ文書全体が抜き出されるもの\n",
        "    if row.sentiment == 'neutral' or ((ee2 - ss2) / len(original_text) > 0.75 and  (ee2 - ss2) > 9):\n",
        "        if len(original_text) > 0 and original_text[0] != '_' and ss2 < 5:\n",
        "            ss2 = 0 \n",
        "        if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "            ee2 += 1\n",
        "        st =  original_text[ss2:ee2].lstrip(' ½¿')\n",
        "        y_selected_text = st #re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "                \n",
        "    else:\n",
        "        if original_text[:int((ss2+ee2) * 0.5) + 1].count('  ') > 0:\n",
        "            ss = y_start_char\n",
        "            ee = y_end_char + 1\n",
        "            if ss > 1 and original_text[ss-1:ss+1] == '..' and  original_text[ss+1] != '.':\n",
        "                ss -= 1\n",
        "            st = original_text[ss:ee]#.lstrip(' ½¿')\n",
        "            y_selected_text = re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "        else:\n",
        "            if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "                ee2 += 1\n",
        "            # 先頭の空白分後退\n",
        "            if  original_text[0] == ' ':\n",
        "                ss2 -= 1\n",
        "\n",
        "            y_selected_text = original_text[ss2:ee2].strip(' ½')\n",
        "\n",
        "            if row.normalized_text[:y_end_char + 5] == \" \" + row.original_text[:ee2 + 4]: # 簡単のため、長さが同じ場合に限定している\n",
        "                y_selected_text = modify_punc_length(original_text, y_selected_text)\n",
        "            \n",
        "            \n",
        "    return y_selected_text"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P54ebs-lq5Z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f69379b-67e6-4823-d7fd-1ae054e8186d"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 501200538 Aug 17 18:52 '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j8BSIJWumqQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "009eddb4-7f0d-4631-d8c4-1ee4e5ea3f52"
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "if DEBUG_LOAD:\n",
        "    train_df = train_df.head(n=1000)\n",
        "train_df['text'] = train_df['text'].astype(str)\n",
        "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
        "\n",
        "if USE_TRY_MULTI_SEEDING:\n",
        "  seed_jaccard = pd.read_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)\n",
        "  seeds = seed_jaccard['seed'].tolist()\n",
        "  kfoldseeds = seed_jaccard['kfoldseed'].tolist()\n",
        "  CVs = seed_jaccard['CV'].tolist()\n",
        "\n",
        "seeding_id = 0\n",
        "\n",
        "if not USE_TRY_MULTI_SEEDING:\n",
        "  skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "while(True):\n",
        "  jc_sum = []\n",
        "  seed = int(time.time())\n",
        "\n",
        "  seeding_id += 1\n",
        "\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    print(f\"-----seeding {seeding_id}, seed:{seed}\")\n",
        "    seed_everything(seed)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
        "      print(f'Fold: {fold}')\n",
        "\n",
        "      if TRAIN_WITH_PSUEDO_LABELS:\n",
        "            pl_df= pd.read_csv(\"psuedo_labels_fold_%d.csv\" % (fold - 1))\n",
        "            if DEBUG_LOAD:\n",
        "                pl_df = pl_df.head(n=1000)\n",
        "            pl_df['text'] = pl_df['text'].astype(str)\n",
        "            pl_df['selected_text'] = pl_df['selected_text'].astype(str)\n",
        "            #del pl_df['score']\n",
        "            train_df = train_df.append(pl_df)\n",
        "\n",
        "      \n",
        "      if DEBUG_FOLD:\n",
        "          if fold != 1:\n",
        "              print(f\"DEBUG skip fold:{fold}\")\n",
        "              continue\n",
        "\n",
        "      if USE_TRAIN_ROBERTA_LARGE:\n",
        "        model = TweetModel(model_type=\"roberta-large\")\n",
        "      else:\n",
        "        model = TweetModel()\n",
        "\n",
        "      prefix = \"roberta\"\n",
        "      def is_backbone(n):\n",
        "          return prefix in n\n",
        "\n",
        "      lr=3e-5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if USE_MULTIPLE_LEARNING_RATE:\n",
        "        params = list(model.named_parameters())\n",
        "\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lr},\n",
        "            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\": lr * USE_MULTIPLE_LEARNING_RATE_TIMES},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "          optimizer_grouped_parameters, lr=lr, betas=(0.9, 0.999)\n",
        "        )\n",
        "      else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "      criterion = loss_fn    \n",
        "      dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
        "\n",
        "      jc = train_model(\n",
        "          model, \n",
        "          dataloaders_dict,\n",
        "          criterion, \n",
        "          optimizer, \n",
        "          num_epochs,\n",
        "          f'roberta_fold{fold}.pth')\n",
        "      \n",
        "      jc_sum.append(jc)\n",
        "\n",
        "  print(\"average jaccard: {:.4f}\".format(sum(jc_sum)/len(jc_sum)))\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    seeds.append(seed)\n",
        "    kfoldseeds.append(seed)\n",
        "    CVs.append(sum(jc_sum)/len(jc_sum))\n",
        "\n",
        "    seed_jaccard = pd.DataFrame({'seed':seeds,'CV': CVs, 'kfoldseed':kfoldseeds})\n",
        "\n",
        "    seed_jaccard.to_csv(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "    seed_jaccard.to_csv('seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "\n",
        "    print(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv saved' % NOTEBOOK_ID)\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "      print(seed_jaccard)\n",
        "\n",
        "  if not USE_TRY_MULTI_SEEDING:\n",
        "    break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 1.9763 | Jaccard: 0.6706\n",
            "Epoch 1/3 |  val  | Loss: 1.5712 | Jaccard: 0.7176\n",
            "Epoch 2/3 | train | Loss: 1.5503 | Jaccard: 0.7195\n",
            "Epoch 2/3 |  val  | Loss: 1.5050 | Jaccard: 0.7231\n",
            "Epoch 3/3 | train | Loss: 1.4611 | Jaccard: 0.7301\n",
            "Epoch 3/3 |  val  | Loss: 1.5676 | Jaccard: 0.7285\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 2.0653 | Jaccard: 0.6513\n",
            "Epoch 1/3 |  val  | Loss: 1.7483 | Jaccard: 0.7008\n",
            "Epoch 2/3 | train | Loss: 1.5391 | Jaccard: 0.7188\n",
            "Epoch 2/3 |  val  | Loss: 1.5564 | Jaccard: 0.7125\n",
            "Epoch 3/3 | train | Loss: 1.3615 | Jaccard: 0.7432\n",
            "Epoch 3/3 |  val  | Loss: 1.5447 | Jaccard: 0.7153\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 1.8870 | Jaccard: 0.6809\n",
            "Epoch 1/3 |  val  | Loss: 1.5286 | Jaccard: 0.7248\n",
            "Epoch 2/3 | train | Loss: 1.4671 | Jaccard: 0.7261\n",
            "Epoch 2/3 |  val  | Loss: 1.5251 | Jaccard: 0.7271\n",
            "Epoch 3/3 | train | Loss: 1.2968 | Jaccard: 0.7523\n",
            "Epoch 3/3 |  val  | Loss: 1.5708 | Jaccard: 0.7217\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 1.9933 | Jaccard: 0.6715\n",
            "Epoch 1/3 |  val  | Loss: 1.5513 | Jaccard: 0.7077\n",
            "Epoch 2/3 | train | Loss: 1.5474 | Jaccard: 0.7180\n",
            "Epoch 2/3 |  val  | Loss: 1.5430 | Jaccard: 0.7254\n",
            "Epoch 3/3 | train | Loss: 1.3874 | Jaccard: 0.7421\n",
            "Epoch 3/3 |  val  | Loss: 1.5671 | Jaccard: 0.7328\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 1.8925 | Jaccard: 0.6778\n",
            "Epoch 1/3 |  val  | Loss: 1.5758 | Jaccard: 0.7248\n",
            "Epoch 2/3 | train | Loss: 1.4655 | Jaccard: 0.7264\n",
            "Epoch 2/3 |  val  | Loss: 1.5121 | Jaccard: 0.7267\n",
            "Epoch 3/3 | train | Loss: 1.3048 | Jaccard: 0.7524\n",
            "Epoch 3/3 |  val  | Loss: 1.6198 | Jaccard: 0.7295\n",
            "average jaccard: 0.7256\n",
            "CPU times: user 1h 51min 36s, sys: 1h 4min 35s, total: 2h 56min 11s\n",
            "Wall time: 2h 58min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFL3hBPKcyub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/drive/My\\ Drive/kaggle/input/modelpths/roberta-large/\n",
        "!cp roberta_fold*.pth /content/drive/My\\ Drive/kaggle/input/modelpths/roberta-large/"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHBPq11q99H4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "no multi-sample dropout:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.3472 | Jaccard: 0.6409\n",
        "Epoch 1/3 |  val  | Loss: 1.6409 | Jaccard: 0.7040\n",
        "Epoch 2/3 | train | Loss: 1.5979 | Jaccard: 0.7133\n",
        "Epoch 2/3 |  val  | Loss: 1.5697 | Jaccard: 0.7138\n",
        "Epoch 3/3 | train | Loss: 1.4090 | Jaccard: 0.7414\n",
        "Epoch 3/3 |  val  | Loss: 1.5755 | Jaccard: 0.7171\n",
        "Fold: 2\n",
        "DEBUG skip fold:2\n",
        "CPU times: user 45.9 s, sys: 25.9 s, total: 1min 11s\n",
        "Wall time: 1min 16s\n",
        "\n",
        "with multi-sample dropout .2*5:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.2051 | Jaccard: 0.6481\n",
        "Epoch 1/3 |  val  | Loss: 1.5972 | Jaccard: 0.7131\n",
        "Epoch 2/3 | train | Loss: 1.4994 | Jaccard: 0.7283\n",
        "Epoch 2/3 |  val  | Loss: 1.5758 | Jaccard: 0.7133\n",
        "Epoch 3/3 | train | Loss: 1.3283 | Jaccard: 0.7484\n",
        "Epoch 3/3 |  val  | Loss: 1.5696 | Jaccard: 0.7183\n",
        "\n",
        "\n",
        "with multi-sample dropout and all layers:\n",
        "Epoch 1/3 | train | Loss: 2.2779 | Jaccard: 0.6477\n",
        "Epoch 1/3 |  val  | Loss: 1.5889 | Jaccard: 0.7085\n",
        "Epoch 2/3 | train | Loss: 1.5759 | Jaccard: 0.7224\n",
        "Epoch 2/3 |  val  | Loss: 1.5932 | Jaccard: 0.7108\n",
        "Epoch 3/3 | train | Loss: 1.3961 | Jaccard: 0.7394\n",
        "Epoch 3/3 |  val  | Loss: 1.5826 | Jaccard: 0.7175\n",
        "\n",
        "with multi-sample dropout and multi-learning rate 500:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.0589 | Jaccard: 0.6682\n",
        "Epoch 1/3 |  val  | Loss: 1.5709 | Jaccard: 0.7098\n",
        "Epoch 2/3 | train | Loss: 1.6758 | Jaccard: 0.7055\n",
        "Epoch 2/3 |  val  | Loss: 1.6627 | Jaccard: 0.6960\n",
        "Epoch 3/3 | train | Loss: 1.4640 | Jaccard: 0.7355\n",
        "Epoch 3/3 |  val  | Loss: 1.5935 | Jaccard: 0.7173\n",
        "\n",
        "\n",
        "multisample dropout .3*5:\n",
        "\n",
        "223.6s\n",
        "31\n",
        "Epoch 1/3 | train | Loss: 2.2905 | Jaccard: 0.6341\n",
        "283.8s\n",
        "32\n",
        "Epoch 1/3 |  val  | Loss: 1.6386 | Jaccard: 0.7064\n",
        "418.3s\n",
        "33\n",
        "Epoch 2/3 | train | Loss: 1.5642 | Jaccard: 0.7169\n",
        "479.0s\n",
        "34\n",
        "Epoch 2/3 |  val  | Loss: 1.5885 | Jaccard: 0.7112\n",
        "613.7s\n",
        "35\n",
        "Epoch 3/3 | train | Loss: 1.3810 | Jaccard: 0.7443\n",
        "674.8s\n",
        "36\n",
        "Epoch 3/3 |  val  | Loss: 1.5906 | Jaccard: 0.7222\n",
        "\n",
        "multisample dropout .3*8:\n",
        "\n",
        "221.7s\n",
        "31\n",
        "Epoch 1/3 | train | Loss: 2.2384 | Jaccard: 0.6376\n",
        "281.9s\n",
        "32\n",
        "Epoch 1/3 |  val  | Loss: 1.5844 | Jaccard: 0.7118\n",
        "416.4s\n",
        "33\n",
        "Epoch 2/3 | train | Loss: 1.5052 | Jaccard: 0.7242\n",
        "476.1s\n",
        "34\n",
        "Epoch 2/3 |  val  | Loss: 1.5753 | Jaccard: 0.7121\n",
        "610.1s\n",
        "35\n",
        "Epoch 3/3 | train | Loss: 1.3466 | Jaccard: 0.7469\n",
        "668.5s\n",
        "36\n",
        "Epoch 3/3 |  val  | Loss: 1.5563 | Jaccard: 0.7204\n",
        "\n",
        "ML4 layer + MSD.3*5\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.3425 | Jaccard: 0.6331\n",
        "Epoch 1/3 |  val  | Loss: 1.6264 | Jaccard: 0.7057\n",
        "Epoch 2/3 | train | Loss: 1.6000 | Jaccard: 0.7132\n",
        "Epoch 2/3 |  val  | Loss: 1.5563 | Jaccard: 0.7164\n",
        "Epoch 3/3 | train | Loss: 1.4459 | Jaccard: 0.7348\n",
        "Epoch 3/3 |  val  | Loss: 1.5472 | Jaccard: 0.7266\n",
        "\n",
        "multi-layer-4:\n",
        "\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.4450 | Jaccard: 0.6277\n",
        "Epoch 1/3 |  val  | Loss: 1.6236 | Jaccard: 0.7080\n",
        "Epoch 2/3 | train | Loss: 1.5886 | Jaccard: 0.7169\n",
        "Epoch 2/3 |  val  | Loss: 1.5905 | Jaccard: 0.7137\n",
        "Epoch 3/3 | train | Loss: 1.4059 | Jaccard: 0.7393\n",
        "Epoch 3/3 |  val  | Loss: 1.5506 | Jaccard: 0.7203\n",
        "\n",
        "ML8 layer + MSD.3*5\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.3470 | Jaccard: 0.6376\n",
        "Epoch 1/3 |  val  | Loss: 1.6091 | Jaccard: 0.7061\n",
        "Epoch 2/3 | train | Loss: 1.5642 | Jaccard: 0.7200\n",
        "Epoch 2/3 |  val  | Loss: 1.5585 | Jaccard: 0.7129\n",
        "Epoch 3/3 | train | Loss: 1.3812 | Jaccard: 0.7438\n",
        "Epoch 3/3 |  val  | Loss: 1.5480 | Jaccard: 0.7228\n",
        "\n",
        "ML8 layer + MSD.3*5 + Multi LR 3:\n",
        "Epoch 1/3 | train | Loss: 2.2526 | Jaccard: 0.6451\n",
        "Epoch 1/3 |  val  | Loss: 1.5831 | Jaccard: 0.7119\n",
        "Epoch 2/3 | train | Loss: 1.5375 | Jaccard: 0.7217\n",
        "Epoch 2/3 |  val  | Loss: 1.5731 | Jaccard: 0.7127\n",
        "Epoch 3/3 | train | Loss: 1.3621 | Jaccard: 0.7431\n",
        "Epoch 3/3 |  val  | Loss: 1.5527 | Jaccard: 0.7232\n",
        "\n",
        "ML8 layer + MSD.3*5 + Multi LR 8:\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.2921 | Jaccard: 0.6418\n",
        "Epoch 1/3 |  val  | Loss: 1.5859 | Jaccard: 0.7150\n",
        "Epoch 2/3 | train | Loss: 1.5481 | Jaccard: 0.7219\n",
        "Epoch 2/3 |  val  | Loss: 1.5601 | Jaccard: 0.7138\n",
        "Epoch 3/3 | train | Loss: 1.3870 | Jaccard: 0.7398\n",
        "Epoch 3/3 |  val  | Loss: 1.5497 | Jaccard: 0.7217\n",
        "\n",
        "multi-layer-8:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.4506 | Jaccard: 0.6319\n",
        "Epoch 1/3 |  val  | Loss: 1.6295 | Jaccard: 0.7055\n",
        "Epoch 2/3 | train | Loss: 1.6330 | Jaccard: 0.7106\n",
        "Epoch 2/3 |  val  | Loss: 1.5786 | Jaccard: 0.7183\n",
        "Epoch 3/3 | train | Loss: 1.4762 | Jaccard: 0.7282\n",
        "Epoch 3/3 |  val  | Loss: 1.5342 | Jaccard: 0.7215\n",
        "\n",
        "multi-layer-all but embedding layer:\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.2776 | Jaccard: 0.6499\n",
        "Epoch 1/3 |  val  | Loss: 1.5854 | Jaccard: 0.7095\n",
        "Epoch 2/3 | train | Loss: 1.5341 | Jaccard: 0.7229\n",
        "Epoch 2/3 |  val  | Loss: 1.5664 | Jaccard: 0.7231\n",
        "Epoch 3/3 | train | Loss: 1.3625 | Jaccard: 0.7441\n",
        "Epoch 3/3 |  val  | Loss: 1.5995 | Jaccard: 0.7146\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PL1nfl-BmqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXPLORE = False\n",
        "\n",
        "if EXPLORE:\n",
        "\n",
        "    train_df_explore = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "    train_df_explore.head()\n",
        "    \n",
        "    train_df_explore['text'] = train_df_explore['text'].astype(str)\n",
        "    train_df_explore['selected_text'] = train_df_explore['selected_text'].astype(str)\n",
        "    \n",
        "    ds = TweetDataset(train_df_explore)\n",
        "    \n",
        "    \n",
        "    ds1 = TweetDataset(train_df_explore, use_fifth=False)\n",
        "    \n",
        "    same_count = 0\n",
        "    diff_count = 0\n",
        "    \n",
        "    for d1, d2 in zip(ds, ds1):\n",
        "        if d1['start_idx'] != d2['start_idx'] or  d1['end_idx'] != d2['end_idx']:\n",
        "            print(f\"tweet:[{d1['tweet']}] d1!=d2, select: [{d1['select']}]\")\n",
        "            print(\"d1:%s\" % d1['tweet'][d1['offsets'][d1['start_idx']][0]:d1['offsets'][d1['end_idx']][1]])\n",
        "            print(\"d2:%s\\n\" % d2['tweet'][d2['offsets'][d2['start_idx']][0]:d2['offsets'][d2['end_idx']][1]])\n",
        "            diff_count += 1\n",
        "        else:\n",
        "            same_count += 1\n",
        "    \n",
        "    print(f\"diff:{diff_count}, same:{same_count}\")\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ru7zVFpfmqQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(ds1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1x9WP5zhmqQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "if EXPLORE:\n",
        "    train_df_explore = train_df_explore.fillna(\" \")\n",
        "    train_df_explore[train_df_explore['text'].isna()]\n",
        "\n",
        "    print(train_df_explore.shape)\n",
        "\n",
        "    train_df_explore[train_df_explore['text'].str.contains(\"  \")].shape\n",
        "\n",
        "    half_word_ids = []\n",
        "\n",
        "    PRINT_CUT_OFF = 300\n",
        "\n",
        "    for aa, dd in zip(train_df_explore.iterrows(),ds) :\n",
        "        index, row = aa\n",
        "    \n",
        "        # first word in selected_text:\n",
        "        space_index = row['selected_text'].find(\" \")\n",
        "        first_word = \"\"\n",
        "        if space_index == -1:\n",
        "            first_word = row['selected_text']\n",
        "        else:\n",
        "            first_word = row['selected_text'][:space_index]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} first word:{first_word}\")\n",
        "        if len(first_word) == 0:\n",
        "            continue\n",
        "\n",
        "        # last word\n",
        "        space_index = row['selected_text'].rfind(\" \")\n",
        "        if space_index == -1:\n",
        "            last_word = row['selected_text']\n",
        "        else:\n",
        "            last_word = row['selected_text'][space_index+1:]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} last word:{last_word}\")\n",
        "\n",
        "        half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        not_found = False\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(first_word, id_start)\n",
        "            if id0 == -1:\n",
        "                # not found continue\n",
        "                not_found = True\n",
        "                break\n",
        "\n",
        "            if id0 == 0 or not row['text'][id0-1].isalpha():\n",
        "                half_word = False\n",
        "                break\n",
        "            else:\n",
        "                half_word = True\n",
        "\n",
        "            id_start = id0+len(row['text'])\n",
        "\n",
        "        if not_found:\n",
        "            continue\n",
        "\n",
        "        if half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half first word found in [{row['text']}], word:[{first_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][1]])\n",
        "                \n",
        "\n",
        "            half_word_ids.append(index)\n",
        "\n",
        "        # for last word:\n",
        "        last_half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(last_word, id_start)\n",
        "            if index == 50:\n",
        "                '''\n",
        "                tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "                    vocab_file='../input/roberta-base/vocab.json', \n",
        "                    merges_file='../input/roberta-base/merges.txt', \n",
        "                    lowercase=True,\n",
        "                    add_prefix_space=True)\n",
        "\n",
        "                tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "                encoding = tokenizer.encode(tweet)\n",
        "                print(f\"encoding:{encoding}\")\n",
        "                print(f\"encoding:{tweet}\")\n",
        "                print(f\"encoding:{encoding.ids}\")\n",
        "                print(f\"encoding:{encoding.tokens}\")\n",
        "                print(f\"encoding:{encoding.offsets}\")\n",
        "\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[34:41])]}\")\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[-7:])]}\")\n",
        "\n",
        "\n",
        "                print(f\"\\n{index}-id0:{id0}, found:{row['text'][id0:id0+len(last_word)]}; char:{row['text'][id0+len(last_word)]}, isalpha:{row['text'][id0+len(last_word)].isalpha()}\\n\")\n",
        "                '''\n",
        "                pass\n",
        "\n",
        "            if id0 == -1:\n",
        "                break\n",
        "            if id0+len(last_word)< len(row['text']) and not row['text'][id0+len(last_word)].isalpha():\n",
        "                last_half_word = False\n",
        "                break\n",
        "            elif id0+len(last_word) == len(row['text']):\n",
        "                last_half_word = False\n",
        "                break\n",
        "            else:\n",
        "                last_half_word = True\n",
        "\n",
        "            id_start = id0+len(last_word)\n",
        "\n",
        "        if last_half_word and not half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half last word found in [{row['text']}], word:[{last_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][0]])\n",
        "                \n",
        "            half_word_ids.append(index)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A64hmGU1mqRA",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rx8jYSScmqRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "\n",
        "test_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "test_df['text'] = test_df['text'].astype(str)\n",
        "test_loader = get_test_loader(test_df)\n",
        "predictions = []\n",
        "fold_predictions = []\n",
        "fold_prediction_scores = []\n",
        "\n",
        "for i in range(FOLD_COUNT):\n",
        "  fold_predictions.append([])\n",
        "  fold_prediction_scores.append([])\n",
        "\n",
        "model_types = [\"roberta-base\"]\n",
        "model_type_paths = [\"\"]\n",
        "if USE_INFER_ROBERTA_LARGE:\n",
        "  model_types.append(\"roberta-large\")\n",
        "  model_type_paths.append(\"ROOT_PATH + '/input/modelpths/roberta-large/\")\n",
        "\n",
        "models = []\n",
        "for model_type, model_path in zip(model_types, model_type_paths):\n",
        "  for fold in range(skf.n_splits):\n",
        "      \n",
        "      if DEBUG_FOLD:\n",
        "          if fold != 0:\n",
        "              print(f\"DEBUG infer skip fold:{fold}\")\n",
        "              continue\n",
        "              \n",
        "      model = TweetModel(model_type=model_type)\n",
        "      model.cuda()\n",
        "      model.load_state_dict(torch.load(f'{model_path}roberta_fold{fold+1}.pth'))\n",
        "      model.eval()\n",
        "      models.append(model)\n",
        "    \n",
        "same_count = 0\n",
        "diff_count = 0\n",
        "\n",
        "print(\"loading data...\")\n",
        "    \n",
        "fold_2_app_count = 0\n",
        "for data in test_loader:\n",
        "    ids = data['ids'].cuda()\n",
        "    masks = data['masks'].cuda()\n",
        "    tweet = data['tweet']\n",
        "    original_tweet = data['original_tweet']\n",
        "    offsets = data['offsets'].numpy()\n",
        "\n",
        "    start_logits_list = []\n",
        "    end_logits_list = []\n",
        "\n",
        "    #print(f\"{start_logits_list}\")\n",
        "    #print(f\"processing {data}\")\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            output = model(ids, masks)\n",
        "            start_logits_list.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
        "            end_logits_list.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
        "\n",
        "    start_logits = np.mean(start_logits_list, axis=0)\n",
        "    end_logits = np.mean(end_logits_list, axis=0)\n",
        "\n",
        "    \n",
        "    for i in range(len(ids)):    \n",
        "        start_pred = np.argmax(start_logits[i])\n",
        "        end_pred = np.argmax(end_logits[i])\n",
        "        \n",
        "        #print(f\"processing {i}\")\n",
        "        if start_pred > end_pred:\n",
        "            pred = tweet[i]\n",
        "        else:\n",
        "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "            try:\n",
        "                fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "            except:\n",
        "                fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                \n",
        "            \n",
        "            if pred!=fifth_pred:\n",
        "                diff_count += 1\n",
        "                #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "            else:\n",
        "                same_count += 1\n",
        "\n",
        "            pp_pred = pp(pred, original_tweet[i])\n",
        "            \n",
        "            pred = fifth_pred\n",
        "        predictions.append(pred)\n",
        "\n",
        "\n",
        "    # generate psuedo label lists from models:\n",
        "\n",
        "    #print(f\"len start {len(start_logits_list)}\")\n",
        "\n",
        "\n",
        "    for fold, start_logits, end_logits in zip(range(len(start_logits_list)), start_logits_list, end_logits_list):\n",
        "      #print(f\"fold: {fold} start_logits:{len(start_logits)} end_logits:{len(end_logits)}\")\n",
        "      for i in range(len(ids)):    \n",
        "          start_pred = np.argmax(start_logits[i])\n",
        "          start_pred_socre = np.amax(start_logits[i])\n",
        "          end_pred = np.argmax(end_logits[i])\n",
        "          end_pred_score = np.amax(end_logits[i])\n",
        "          \n",
        "          #print(f\"processing {i}\")\n",
        "          if start_pred > end_pred:\n",
        "              pred = tweet[i]\n",
        "          else:\n",
        "              pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "              try:\n",
        "                  fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "              except:\n",
        "                  fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                  \n",
        "              \n",
        "              if pred!=fifth_pred:\n",
        "                  diff_count += 1\n",
        "                  #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "              else:\n",
        "                  same_count += 1\n",
        "\n",
        "              pp_pred = pp(pred, original_tweet[i])\n",
        "              \n",
        "              pred = fifth_pred\n",
        "          fold_predictions[fold].append(pred)\n",
        "          fold_prediction_scores[fold].append((start_pred_socre + end_pred_score)/2.)\n",
        "\n",
        "print(f\"same:{same_count} diff:{diff_count}\")\n",
        "\n",
        "print(f\"fold_2_app_count:{fold_2_app_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L51vQ_V3XmCW",
        "colab_type": "text"
      },
      "source": [
        "# Gen psuedo label files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b3eS3TkXw2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if GEN_PSUEDO_LABELS:\n",
        "  ori_sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "\n",
        "  for fold, predictions, scores in zip(range(len(fold_predictions)), fold_predictions, fold_prediction_scores):\n",
        "    sub_df = ori_sub_df.copy(True)\n",
        "\n",
        "    print(sub_df)\n",
        "    print(len(fold_predictions[0]))\n",
        "    print(len(fold_predictions[1]))\n",
        "    print(len(fold_predictions[2]))\n",
        "    print(len(fold_predictions[3]))\n",
        "    print(len(fold_predictions[4]))\n",
        "    sub_df['selected_text'] = predictions\n",
        "    sub_df['score'] = scores\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "    sub_df = sub_df[sub_df.score > .35]\n",
        "\n",
        "    del sub_df['score']\n",
        "    sub_df.to_csv(\"psuedo_labels_fold_%d.csv\" % fold,  index=False)\n",
        "    sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0U922cmqRO",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0euGDHjmqRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "sub_df['selected_text'] = predictions\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}