{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "seed_finding_1-tweet-sentiment-roberta-with-pp-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyogavin/kaggle_experiments/blob/master/seed_finding_1_tweet_sentiment_roberta_with_pp_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SnNGHZ9mqPc",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuosFIsJmvAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aeaab30c-b456-4506-829b-4e98af8ecdbc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE-fp6Obpkma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT_PATH=\"/content/drive/My Drive/kaggle\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vb_2PjzTym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e7eda2e-d80b-45a3-db6d-082f97362d41"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAtgWfjbyfoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flags:\n",
        "FOLD_COUNT = 5\n",
        "\n",
        "NOTEBOOK_ID = 1\n",
        "\n",
        "\n",
        "DEBUG_LOAD = False\n",
        "DEBUG_FOLD = False\n",
        "USE_MULTI_SAMPLE_DROPOUT = True #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L119\n",
        "USE_MULTI_SAMPLE_DROPOUT_RATE = 0.5\n",
        "USE_MULTI_SAMPLE_DROPOUT_SAMPLE = 4\n",
        "\n",
        "USE_BERT_ALL_LAYERS = False # https://www.kaggle.com/c/google-quest-challenge/discussion/129840\n",
        "#https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L76\n",
        "\n",
        "USE_BERT_LAST_N_LAYERS = 4\n",
        "\n",
        "USE_MULTIPLE_LEARNING_RATE = True #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L132\n",
        "USE_MULTIPLE_LEARNING_RATE_TIMES = 3\n",
        "\n",
        "\n",
        "GEN_PSUEDO_LABELS = False\n",
        "TRAIN_WITH_PSUEDO_LABELS = False\n",
        "\n",
        "USE_TRY_MULTI_SEEDING = True\n",
        "\n",
        "USE_SEQUENCE_BUCKETING = False\n",
        "USE_SMOOTH_LABELING = True\n",
        "LABEL_SMOOTH = 0.1\n",
        "\n",
        "\n",
        "PAD_ID = 1\n",
        "MAX_LEN = 96\n",
        "\n",
        "USE_SEED = 42"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvHmeSfLqaxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "4ae489ce-83f0-422f-9567-01b6dad64b3c"
      },
      "source": [
        "!pip install tokenizers transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.9MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6873d998f86cd715d5f489bb0ab431a7941f38d2eb8de80aa3985e902b208bdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "VP8YyNajmqPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JxIBCKatmqPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ef958b4b-5377-467f-b676-d63c9bbbd825"
      },
      "source": [
        "import re\n",
        "\n",
        "USE_PP = False\n",
        "\n",
        "def pp(filtered_output, real_tweet):\n",
        "    #if not USE_PP:\n",
        "    #    return filtered_output\n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "            if filtered_output.endswith(\"..\"):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n",
        "                return filtered_output\n",
        "            if filtered_output.endswith('!!'):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n",
        "                return filtered_output\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "    return filtered_output\n",
        "\n",
        "def prp(filtered_output, real_tweet):\n",
        "    if not USE_PP:\n",
        "        return filtered_output\n",
        "    \n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "\n",
        "            st = real_tweet.find(filtered_output)\n",
        "            fl = real_tweet.find(\"  \")\n",
        "            end = st + len(filtered_output)\n",
        "            \n",
        "            #if fl != -1 and fl < st:\n",
        "            while st != -1 and end < len(real_tweet) and (real_tweet[end] == '.' or real_tweet[end] == '!'):\n",
        "                filtered_output = filtered_output + real_tweet[end]\n",
        "                end += 1\n",
        "                    \n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            #print(filtered_output)\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            \n",
        "            \n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "        \n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "\n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "    return filtered_output\n",
        "\n",
        "tweet = \"  ROFLMAO for the funny web portal  =D\"\n",
        "pred = \"funny\"\n",
        "answer = \"e funny\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "tweet = \" yea i just got outta one too....i want him back tho  but i feel the same way...i`m cool on dudes for a lil while\"\n",
        "pred = \"cool\"\n",
        "answer = \"m cool\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \"Ow... My shoulder muscle (I can`t remember the name :p) hurts... What did I do?  I don`t even know\"\n",
        "pred = \"hurts...\"\n",
        "answer = \"hurts..\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \" yep... or it should b automatic that if u fall 4 someone that person does 2!or smthng like that... but the way it is sucks!\"\n",
        "pred = \"SUCKS!\"\n",
        "answer = \"SUCKS!\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "\n",
        "tweet = \" hi holly i`ll volunteer to try it out first for u! hope ur having a fab weekend xoxox...\"\n",
        "pred = \"fab\"\n",
        "answer = \"g a\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e funny\n",
            "e funny\n",
            "m cool\n",
            "m cool\n",
            "hurts..\n",
            "hurts..\n",
            "SUCKS!\n",
            "SUCKS!\n",
            " fab \n",
            "g a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8-G9tYSXmqPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccc27e68-4348-49b2-d33d-3e9ecb8e2577"
      },
      "source": [
        "\"  \" in tweet"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qb1WhQoSmqPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fifth_prp(select, tweet, sentiment, offsets):\n",
        "    #print('entering fifth prp')\n",
        "    ss = tweet.find(select)\n",
        "    if tweet[max(ss - 2, 0):ss] == '  ':\n",
        "        ss -= 2\n",
        "    if ss > 0  and tweet[ss - 1] == ' ':\n",
        "        ss -= 1\n",
        "\n",
        "    ee = ss + len(select)\n",
        "\n",
        "    if re.match(r' [^ ]', tweet) is not None:\n",
        "        ee -= 1\n",
        "\n",
        "    ss = max(0, ss)\n",
        "    if '  ' in tweet[:ss] and sentiment != 'neutral':\n",
        "        text1 = \" \".join(tweet.split())\n",
        "        sel = text1[ss:ee].strip()\n",
        "        if len(sel) > 1 and sel[-2] == ' ':\n",
        "            sel = sel[:-2]\n",
        "\n",
        "        select = sel\n",
        "\n",
        "    text1 = \" \"+\" \".join(tweet.split())\n",
        "    text2 = \" \".join(select.split()).lstrip(\".,;:\")\n",
        "\n",
        "    idx = text1.find(text2)\n",
        "    if idx != -1:\n",
        "        chars = np.zeros((len(text1)))\n",
        "        chars[idx:idx+len(text2)]=1\n",
        "        if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    else:\n",
        "        import pdb;pdb.set_trace()\n",
        "        chars = np.ones((len(text1)))\n",
        "        \n",
        "        \n",
        "    #tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    #        vocab_file='../input/roberta-base/vocab.json', \n",
        "    #        merges_file='../input/roberta-base/merges.txt', \n",
        "    #        lowercase=True,\n",
        "    #        add_prefix_space=True)\n",
        "    \n",
        "    #enc = tokenizer.encode(text1) \n",
        "\n",
        "    # ID_OFFSETS\n",
        "    #offsets = enc.offsets\n",
        "\n",
        "    # START END TOKENS\n",
        "    _toks = []\n",
        "\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.mean(chars[a:b])\n",
        "        #if (sm > 0.6 and chars[a] != 0):  # こうすると若干伸びるけど...\n",
        "        if (sm > 0.5 and chars[a] != 0): \n",
        "            _toks.append(i)\n",
        "            \n",
        "    #print(\"returnning fifth prep\")\n",
        "    return _toks[0], _toks[-1]\n",
        "\n",
        "    '''\n",
        "    toks = _toks\n",
        "    s_tok = sentiment_id[sentiments[k]]\n",
        "    input_ids[k, :len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask[k,:len(enc.ids)+3] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+2] = 1\n",
        "        end_tokens[k,toks[-1]+2] = 1  \n",
        "    '''"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85-gyl4mqP0",
        "colab_type": "text"
      },
      "source": [
        "# Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "pGNExtgrmqP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed = USE_SEED #42\n",
        "seed_everything(seed)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQXvA_r9mqP4",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QY9vlBLVmqP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, max_len=MAX_LEN, use_fifth=True):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.labeled = 'selected_text' in df\n",
        "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file= ROOT_PATH + '/input/roberta-base/vocab.json', \n",
        "            merges_file= ROOT_PATH + '/input/roberta-base/merges.txt', \n",
        "            lowercase=True,\n",
        "            add_prefix_space=True)\n",
        "        self.use_fifth = use_fifth\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = {}\n",
        "        row = self.df.iloc[index]\n",
        "        \n",
        "        #print(f\"getting {index}\")\n",
        "        \n",
        "        ids, masks, tweet, offsets, enc_offsets, padding_len = self.get_input_data(row)\n",
        "        data['ids'] = ids\n",
        "        data['original_tweet'] = row.text\n",
        "        data['masks'] = masks\n",
        "        data['tweet'] = tweet\n",
        "        data['offsets'] = offsets\n",
        "        #data['select'] = row.selected_text\n",
        "        data['sentiment'] = row.sentiment\n",
        "        \n",
        "        if self.labeled:\n",
        "            try:\n",
        "                #start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "                if self.use_fifth:\n",
        "                    start_idx, end_idx = fifth_prp(row.selected_text, row.text, row.sentiment, offsets)\n",
        "                else:\n",
        "                    start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "            \n",
        "            except:\n",
        "                #print(f\"tweet:[{tweet}], selected: [{row.selected_text}] prp: [{prp(row.selected_text, tweet)}]\")\n",
        "                start_idx = 4\n",
        "                end_idx = len(ids) - 4 - 2\n",
        "                \n",
        "            if USE_SEQUENCE_BUCKETING:\n",
        "              data['start_idx'] = min(start_idx, self.max_len - 1 - padding_len) # start_idx\n",
        "              data['end_idx'] = min(end_idx, self.max_len - 1 - padding_len) # end_idx\n",
        "            else:\n",
        "              data['start_idx'] = start_idx\n",
        "              data['end_idx'] = end_idx\n",
        "        \n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def get_input_data(self, row):\n",
        "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "        encoding = self.tokenizer.encode(tweet)\n",
        "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
        "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
        "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "                \n",
        "        pad_len = self.max_len - len(ids)\n",
        "        if pad_len > 0:\n",
        "            ids += [PAD_ID] * pad_len\n",
        "            offsets += [(0, 0)] * pad_len\n",
        "        \n",
        "        ids = torch.tensor(ids)\n",
        "        masks = torch.where(ids != PAD_ID, torch.tensor(1), torch.tensor(0))\n",
        "        offsets = torch.tensor(offsets)\n",
        "        \n",
        "        return ids, masks, tweet, offsets, encoding.offsets, pad_len\n",
        "        \n",
        "    def get_target_idx(self, row, tweet, offsets):\n",
        "        \n",
        "        #pp:\n",
        "        \n",
        "        selected_text = prp(row.selected_text.lower(), tweet)\n",
        "        #selected_text = row.selected_text\n",
        "        \n",
        "        if len(selected_text) == 0:\n",
        "            selected_text = row.selected_text\n",
        "        \n",
        "        selected_text = \" \" +  \" \".join(selected_text.lower().split())\n",
        "\n",
        "        len_st = len(selected_text) - 1\n",
        "        idx0 = None\n",
        "        idx1 = None\n",
        "\n",
        "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "                idx0 = ind\n",
        "                idx1 = ind + len_st - 1\n",
        "                break\n",
        "\n",
        "        char_targets = [0] * len(tweet)\n",
        "        if idx0 != None and idx1 != None:\n",
        "            for ct in range(idx0, idx1 + 1):\n",
        "                char_targets[ct] = 1\n",
        "\n",
        "        target_idx = []\n",
        "        for j, (offset1, offset2) in enumerate(offsets):\n",
        "            if sum(char_targets[offset1: offset2]) > 0:\n",
        "                target_idx.append(j)\n",
        "\n",
        "        start_idx = target_idx[0]\n",
        "        end_idx = target_idx[-1]\n",
        "        \n",
        "        return start_idx, end_idx\n",
        "        \n",
        "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    val_df = df.iloc[val_idx]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(train_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=2,\n",
        "        drop_last=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(val_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)\n",
        "\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "    return dataloaders_dict\n",
        "\n",
        "def get_test_loader(df, batch_size=32):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)    \n",
        "    return loader"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Vm0BqKmqP8",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bQIG2zlRmqP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TweetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TweetModel, self).__init__()\n",
        "        \n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            ROOT_PATH + '/input/roberta-base/config.json', output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            ROOT_PATH + '/input/roberta-base/pytorch_model.bin', config=config)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.high_dropout = nn.Dropout(USE_MULTI_SAMPLE_DROPOUT_RATE)\n",
        "        self.fc = nn.Linear(config.hidden_size, 2)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "\n",
        "        if USE_BERT_LAST_N_LAYERS == -1:\n",
        "          n_weights = config.num_hidden_layers\n",
        "        else:\n",
        "          n_weights = USE_BERT_LAST_N_LAYERS #config.num_hidden_layers + 1\n",
        "\n",
        "        self.n_layers = n_weights\n",
        "\n",
        "\n",
        "        weights_init = torch.zeros(n_weights).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "\n",
        "        self.multi_layer_dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        if USE_SEQUENCE_BUCKETING:\n",
        "          #padding = torch.eq(input_ids, PAD_ID).int()\n",
        "          padding = torch.where(input_ids == PAD_ID, torch.ones_like(input_ids), torch.zeros_like(input_ids))\n",
        "          lens = MAX_LEN - torch.sum(padding, -1)\n",
        "          #max_len = MAX_LEN -1  #torch.max(lens) - 1\n",
        "          max_len = torch.max(lens)# - 1\n",
        "\n",
        "          #max_len = torch.clamp(max_len, min=10, max=MAX_LEN)\n",
        "\n",
        "          #input_ids_last = torch.unsqueeze(input_ids[:, -1], 1)\n",
        "          #attention_mask_last = torch.unsqueeze(attention_mask[:, -1], 1)\n",
        "          input_ids = input_ids[:, :max_len]\n",
        "          attention_mask = attention_mask[:, :max_len]\n",
        "          #input_ids = torch.cat([input_ids, input_ids_last], dim=1)\n",
        "          #attention_mask = torch.cat([attention_mask, attention_mask_last], dim=1)\n",
        "          #tok_ = tok[:, :max_len]\n",
        "\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
        "         \n",
        "        if not USE_BERT_ALL_LAYERS:\n",
        "          x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
        "          x = torch.mean(x, 0)\n",
        "        else:\n",
        "\n",
        "          #print(f\"layers: { [hs[i].shape for i in range(-USE_BERT_LAST_N_LAYERS, 0, 1)]}\")\n",
        "\n",
        "\n",
        "\n",
        "          x = torch.stack(\n",
        "              [self.multi_layer_dropout(layer[:, :, :]) for layer in [hs[i] for i in range(-self.n_layers, 0, 1)]], dim=3\n",
        "          )\n",
        "          x = (torch.softmax(self.layer_weights, dim=0) * x).sum(-1)\n",
        "\n",
        "        if not USE_MULTI_SAMPLE_DROPOUT:\n",
        "          x = self.dropout(x)\n",
        "          x = self.fc(x)\n",
        "        else:\n",
        "          # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
        "          x = torch.mean(\n",
        "              torch.stack(\n",
        "                  [self.fc(self.high_dropout(x)) for _ in range(USE_MULTI_SAMPLE_DROPOUT_SAMPLE)],\n",
        "                  dim=0,\n",
        "              ),\n",
        "              dim=0,\n",
        "          )\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "                \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teaVSs9UmqQD",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "60TWdOqKmqQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# from: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
        "# This loss is similar to CrossEntropyLoss, i.e. it expects logits. Don't use a softmax as your last layer therefore.\n",
        "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
        "                    device=targets.device) \\\n",
        "                .fill_(smoothing /(n_classes-1)) \\\n",
        "                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n",
        "            self.smoothing)\n",
        "        lsm = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            lsm = lsm * self.weight.unsqueeze(0)\n",
        "\n",
        "        loss = -(targets * lsm).sum(-1)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    if not USE_SMOOTH_LABELING:\n",
        "      ce_loss = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "      ce_loss = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTH)\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)    \n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jXmN6ImqQH",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5mHp1NUWmqQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_selected_text(text, start_idx, end_idx, offsets):\n",
        "    selected_text = \"\"\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            selected_text += \" \"\n",
        "    #selected_text = pp(selected_text, text)\n",
        "    \n",
        "    return selected_text\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
        "    start_pred = np.argmax(start_logits)\n",
        "    end_pred = np.argmax(end_logits)\n",
        "    if start_pred > end_pred:\n",
        "        pred = text\n",
        "    else:\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
        "        \n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
        "    \n",
        "    return jaccard(true, pred)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3LRjNPdmqQM",
        "colab_type": "text"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DxZrmVbJmqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    jc_ret = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            epoch_jaccard = 0.0\n",
        "            \n",
        "            \n",
        "            \n",
        "            for data in (dataloaders_dict[phase]):\n",
        "                ids = data['ids'].cuda()\n",
        "                masks = data['masks'].cuda()\n",
        "                tweet = data['tweet']\n",
        "                offsets = data['offsets'].numpy()\n",
        "                start_idx = data['start_idx'].cuda()\n",
        "                end_idx = data['end_idx'].cuda()\n",
        "                \n",
        "                #print((start_idx,end_idx))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    start_logits, end_logits = model(ids, masks)\n",
        "\n",
        "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    epoch_loss += loss.item() * len(ids)\n",
        "                    \n",
        "                    start_idx = start_idx.cpu().detach().numpy()\n",
        "                    end_idx = end_idx.cpu().detach().numpy()\n",
        "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
        "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
        "                    \n",
        "                    for i in range(len(ids)):                        \n",
        "                        jaccard_score = compute_jaccard_score(\n",
        "                            tweet[i],\n",
        "                            start_idx[i],\n",
        "                            end_idx[i],\n",
        "                            start_logits[i], \n",
        "                            end_logits[i], \n",
        "                            offsets[i])\n",
        "                        epoch_jaccard += jaccard_score\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
        "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
        "            \n",
        "            if epoch == num_epochs - 1:\n",
        "              jc_ret = epoch_jaccard\n",
        "    \n",
        "    torch.save(model.state_dict(), filename)\n",
        "    return jc_ret"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbZ1-zxSmqQX",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hWp_K8EymqQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1s4mfoomqQl",
        "colab_type": "text"
      },
      "source": [
        "# **post process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "twfns9m1mqQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def modify_punc_length(text, selected_text):\n",
        "    m = re.search(r'[!\\.\\?]+$', selected_text)        \n",
        "    if m is None:\n",
        "        return selected_text\n",
        "    \n",
        "    conti_punc = len(m.group())\n",
        "\n",
        "    if conti_punc >= 4:\n",
        "        selected_text = selected_text[:-(conti_punc-2)]\n",
        "    elif conti_punc == 1:# 元のtextを探しに行く\n",
        "        tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", selected_text)\n",
        "        pat = re.sub(r\" \", \" +\", tmp)\n",
        "        m = re.search(pat, text)\n",
        "        f_idx0 = m.start()\n",
        "        f_idx1 = m.end()\n",
        "\n",
        "        if f_idx1 != len(text) and text[f_idx1] in (\"!\", \".\", \"?\"):\n",
        "            f_idx1 += 1\n",
        "            selected_text = text[f_idx0:f_idx1]\n",
        "    return selected_text\n",
        "\n",
        "\n",
        "import math\n",
        "def postprocess(tweet, offsets, aa, bb, sentiment):\n",
        "    text0 = tweet\n",
        "    text1 = \" \" + \" \".join(tweet.split())\n",
        "    #enc = tokenizer.encode(text1)\n",
        "\n",
        "    #aa = np.argmax(start_tokens[k])\n",
        "    #bb = np.argmax(end_tokens[k])\n",
        "\n",
        "    ss = offsets[aa][0]\n",
        "    ee = offsets[bb][1] \n",
        "    #st = text1[ss:ee].strip()\n",
        "    from collections import namedtuple\n",
        "    Row = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\n",
        "    row = Row(\n",
        "        original_text=text0,\n",
        "        normalized_text=text1,\n",
        "        sentiment=sentiment,\n",
        "        y_start_char=ss,\n",
        "        y_end_char=ee,\n",
        "    )\n",
        "    \n",
        "    if row.original_text == '':\n",
        "        return row.normalized_text.strip()\n",
        "    original_text = row.original_text.replace('\\t', '')\n",
        "    y_start_char = row.y_start_char\n",
        "    y_end_char = row.y_end_char\n",
        "    try:\n",
        "        y_selected_text = row.normalized_text[y_start_char:y_end_char].strip()\n",
        "    except:\n",
        "        print(f\"err: {row.normalized_text} {(y_start_char,y_end_char)}\")\n",
        "    if (y_end_char < len(row.normalized_text) and row.sentiment != 'neutral' and\n",
        "        y_selected_text[-1] == '.' and\n",
        "        (row.normalized_text[y_end_char] == '.' or \n",
        "         y_selected_text[-2] == '.')):\n",
        "        y_selected_text = re.sub('\\.+$', '..', y_selected_text)\n",
        "\n",
        "    tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", y_selected_text)\n",
        "    pat = re.sub(r\" \", \" +\", tmp)\n",
        "    m = re.search(pat, original_text)\n",
        "    if m is None:\n",
        "        print(row.normalized_text[y_start_char:y_end_char].strip())\n",
        "        print(row.normalized_text)\n",
        "        print(y_selected_text)\n",
        "    ss2 = m.start()\n",
        "    ee2 = m.end()\n",
        "    \n",
        "    # 'neutral' およびほぼ文書全体が抜き出されるもの\n",
        "    if row.sentiment == 'neutral' or ((ee2 - ss2) / len(original_text) > 0.75 and  (ee2 - ss2) > 9):\n",
        "        if len(original_text) > 0 and original_text[0] != '_' and ss2 < 5:\n",
        "            ss2 = 0 \n",
        "        if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "            ee2 += 1\n",
        "        st =  original_text[ss2:ee2].lstrip(' ½¿')\n",
        "        y_selected_text = st #re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "                \n",
        "    else:\n",
        "        if original_text[:int((ss2+ee2) * 0.5) + 1].count('  ') > 0:\n",
        "            ss = y_start_char\n",
        "            ee = y_end_char + 1\n",
        "            if ss > 1 and original_text[ss-1:ss+1] == '..' and  original_text[ss+1] != '.':\n",
        "                ss -= 1\n",
        "            st = original_text[ss:ee]#.lstrip(' ½¿')\n",
        "            y_selected_text = re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "        else:\n",
        "            if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "                ee2 += 1\n",
        "            # 先頭の空白分後退\n",
        "            if  original_text[0] == ' ':\n",
        "                ss2 -= 1\n",
        "\n",
        "            y_selected_text = original_text[ss2:ee2].strip(' ½')\n",
        "\n",
        "            if row.normalized_text[:y_end_char + 5] == \" \" + row.original_text[:ee2 + 4]: # 簡単のため、長さが同じ場合に限定している\n",
        "                y_selected_text = modify_punc_length(original_text, y_selected_text)\n",
        "            \n",
        "            \n",
        "    return y_selected_text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P54ebs-lq5Z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b362e861-d7e8-4642-bc1c-3d45cf72216c"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 501200538 Aug 17 18:52 '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j8BSIJWumqQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c724a106-d736-4470-baa6-e7396de78c1c"
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "if DEBUG_LOAD:\n",
        "    train_df = train_df.head(n=1000)\n",
        "train_df['text'] = train_df['text'].astype(str)\n",
        "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
        "\n",
        "seed_jaccard = pd.read_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)\n",
        "seeds = seed_jaccard['seed'].tolist()\n",
        "kfoldseeds = seed_jaccard['kfoldseed'].tolist()\n",
        "CVs = seed_jaccard['CV'].tolist()\n",
        "\n",
        "MSDs = seed_jaccard['MSD'].tolist()\n",
        "MSDrates = seed_jaccard['MSDrate'].tolist()\n",
        "MSDsamples = seed_jaccard['MSDsample'].tolist()\n",
        "MLs = seed_jaccard['ML'].tolist()\n",
        "MLns = seed_jaccard['MLn'].tolist()\n",
        "MLs = seed_jaccard['ML'].tolist()\n",
        "MLRs = seed_jaccard['MLR'].tolist()\n",
        "MLRtimess = seed_jaccard['MLRtimes'].tolist()\n",
        "SLs = seed_jaccard['SL'].tolist()\n",
        "SLfactors = seed_jaccard['SLfactor'].tolist()\n",
        "\n",
        "seeding_id = 0\n",
        "\n",
        "if not USE_TRY_MULTI_SEEDING:\n",
        "  skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "while(True):\n",
        "  jc_sum = []\n",
        "  seed = int(time.time())\n",
        "\n",
        "  seeding_id += 1\n",
        "\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    print(f\"-----seeding {seeding_id}, seed:{seed}\")\n",
        "    seed_everything(seed)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
        "      print(f'Fold: {fold}')\n",
        "\n",
        "      if TRAIN_WITH_PSUEDO_LABELS:\n",
        "            pl_df= pd.read_csv(\"psuedo_labels_fold_%d.csv\" % (fold - 1))\n",
        "            if DEBUG_LOAD:\n",
        "                pl_df = pl_df.head(n=1000)\n",
        "            pl_df['text'] = pl_df['text'].astype(str)\n",
        "            pl_df['selected_text'] = pl_df['selected_text'].astype(str)\n",
        "            #del pl_df['score']\n",
        "            train_df = train_df.append(pl_df)\n",
        "\n",
        "      \n",
        "      if DEBUG_FOLD:\n",
        "          if fold != 1:\n",
        "              print(f\"DEBUG skip fold:{fold}\")\n",
        "              continue\n",
        "\n",
        "      model = TweetModel()\n",
        "\n",
        "      prefix = \"roberta\"\n",
        "      def is_backbone(n):\n",
        "          return prefix in n\n",
        "\n",
        "      lr=3e-5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if USE_MULTIPLE_LEARNING_RATE:\n",
        "        params = list(model.named_parameters())\n",
        "\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lr},\n",
        "            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\": lr * USE_MULTIPLE_LEARNING_RATE_TIMES},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "          optimizer_grouped_parameters, lr=lr, betas=(0.9, 0.999)\n",
        "        )\n",
        "      else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "      criterion = loss_fn    \n",
        "      dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
        "\n",
        "      jc = train_model(\n",
        "          model, \n",
        "          dataloaders_dict,\n",
        "          criterion, \n",
        "          optimizer, \n",
        "          num_epochs,\n",
        "          f'roberta_fold{fold}.pth')\n",
        "      \n",
        "      jc_sum.append(jc)\n",
        "\n",
        "  print(\"average jaccard: {:.4f}\".format(sum(jc_sum)/len(jc_sum)))\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    seeds.append(seed)\n",
        "    kfoldseeds.append(seed)\n",
        "    CVs.append(sum(jc_sum)/len(jc_sum))\n",
        "\n",
        "    MSDs.append(USE_MULTI_SAMPLE_DROPOUT)\n",
        "    MSDrates.append(USE_MULTI_SAMPLE_DROPOUT_RATE)\n",
        "    MSDsamples.append(USE_MULTI_SAMPLE_DROPOUT_SAMPLE)\n",
        "    MLs.append(USE_BERT_ALL_LAYERS)\n",
        "    MLns.append(USE_BERT_LAST_N_LAYERS)\n",
        "    MLRs.append(USE_MULTIPLE_LEARNING_RATE)\n",
        "    MLRtimess.append(USE_MULTIPLE_LEARNING_RATE_TIMES)\n",
        "    SLs.append(USE_SMOOTH_LABELING)\n",
        "    SLfactors.append(LABEL_SMOOTH)\n",
        "\n",
        "\n",
        "    seed_jaccard = pd.DataFrame({'seed':seeds,\n",
        "                                 'CV': CVs, \n",
        "                                 'kfoldseed':kfoldseeds,\n",
        "                                 'MSD':MSDs,\n",
        "                                 'MSDrate':MSDrates,\n",
        "                                 'MSDsample':MSDsamples,\n",
        "                                 'ML':MLs,\n",
        "                                 'MLn':MLns,\n",
        "                                 'MLR':MLRs,\n",
        "                                 'MLRtimes':MLRtimess,\n",
        "                                 'SL':SLs,\n",
        "                                 'SLfactor':SLfactors\n",
        "                                 })\n",
        "\n",
        "    seed_jaccard.to_csv(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "    seed_jaccard.to_csv('seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "\n",
        "    print(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv saved' % NOTEBOOK_ID)\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "      print(seed_jaccard)\n",
        "\n",
        "  if not USE_TRY_MULTI_SEEDING:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----seeding 1, seed:1598113266\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.2805 | Jaccard: 0.6693\n",
            "Epoch 1/3 |  val  | Loss: 2.9079 | Jaccard: 0.7203\n",
            "Epoch 2/3 | train | Loss: 2.8350 | Jaccard: 0.7314\n",
            "Epoch 2/3 |  val  | Loss: 2.8678 | Jaccard: 0.7284\n",
            "Epoch 3/3 | train | Loss: 2.7078 | Jaccard: 0.7530\n",
            "Epoch 3/3 |  val  | Loss: 2.8917 | Jaccard: 0.7299\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.2737 | Jaccard: 0.6751\n",
            "Epoch 1/3 |  val  | Loss: 2.9026 | Jaccard: 0.7250\n",
            "Epoch 2/3 | train | Loss: 2.8329 | Jaccard: 0.7281\n",
            "Epoch 2/3 |  val  | Loss: 2.8780 | Jaccard: 0.7230\n",
            "Epoch 3/3 | train | Loss: 2.7007 | Jaccard: 0.7542\n",
            "Epoch 3/3 |  val  | Loss: 2.9024 | Jaccard: 0.7261\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.3036 | Jaccard: 0.6739\n",
            "Epoch 1/3 |  val  | Loss: 2.8818 | Jaccard: 0.7294\n",
            "Epoch 2/3 | train | Loss: 2.8397 | Jaccard: 0.7311\n",
            "Epoch 2/3 |  val  | Loss: 2.8861 | Jaccard: 0.7344\n",
            "Epoch 3/3 | train | Loss: 2.8557 | Jaccard: 0.7333\n",
            "Epoch 3/3 |  val  | Loss: 2.9429 | Jaccard: 0.7205\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2767 | Jaccard: 0.6682\n",
            "Epoch 1/3 |  val  | Loss: 2.9224 | Jaccard: 0.7134\n",
            "Epoch 2/3 | train | Loss: 2.8274 | Jaccard: 0.7313\n",
            "Epoch 2/3 |  val  | Loss: 2.8986 | Jaccard: 0.7167\n",
            "Epoch 3/3 | train | Loss: 2.7074 | Jaccard: 0.7518\n",
            "Epoch 3/3 |  val  | Loss: 2.9772 | Jaccard: 0.7197\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.2609 | Jaccard: 0.6812\n",
            "Epoch 1/3 |  val  | Loss: 2.9134 | Jaccard: 0.7159\n",
            "Epoch 2/3 | train | Loss: 2.8312 | Jaccard: 0.7312\n",
            "Epoch 2/3 |  val  | Loss: 2.9256 | Jaccard: 0.7190\n",
            "Epoch 3/3 | train | Loss: 2.7111 | Jaccard: 0.7523\n",
            "Epoch 3/3 |  val  | Loss: 2.9328 | Jaccard: 0.7227\n",
            "average jaccard: 0.7238\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "-----seeding 2, seed:1598116829\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.3050 | Jaccard: 0.6688\n",
            "Epoch 1/3 |  val  | Loss: 2.8855 | Jaccard: 0.7142\n",
            "Epoch 2/3 | train | Loss: 2.8429 | Jaccard: 0.7287\n",
            "Epoch 2/3 |  val  | Loss: 2.8622 | Jaccard: 0.7240\n",
            "Epoch 3/3 | train | Loss: 2.7177 | Jaccard: 0.7491\n",
            "Epoch 3/3 |  val  | Loss: 2.8918 | Jaccard: 0.7199\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.2929 | Jaccard: 0.6733\n",
            "Epoch 1/3 |  val  | Loss: 2.8994 | Jaccard: 0.7253\n",
            "Epoch 2/3 | train | Loss: 2.8543 | Jaccard: 0.7259\n",
            "Epoch 2/3 |  val  | Loss: 2.8999 | Jaccard: 0.7316\n",
            "Epoch 3/3 | train | Loss: 2.7251 | Jaccard: 0.7489\n",
            "Epoch 3/3 |  val  | Loss: 2.8915 | Jaccard: 0.7324\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2902 | Jaccard: 0.6711\n",
            "Epoch 1/3 |  val  | Loss: 2.8769 | Jaccard: 0.7206\n",
            "Epoch 2/3 | train | Loss: 2.8484 | Jaccard: 0.7265\n",
            "Epoch 2/3 |  val  | Loss: 2.8610 | Jaccard: 0.7276\n",
            "Epoch 3/3 | train | Loss: 2.7155 | Jaccard: 0.7499\n",
            "Epoch 3/3 |  val  | Loss: 2.8835 | Jaccard: 0.7267\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.3237 | Jaccard: 0.6677\n",
            "Epoch 1/3 |  val  | Loss: 2.9209 | Jaccard: 0.7159\n",
            "Epoch 2/3 | train | Loss: 2.9050 | Jaccard: 0.7196\n",
            "Epoch 2/3 |  val  | Loss: 2.9332 | Jaccard: 0.7061\n",
            "Epoch 3/3 | train | Loss: 2.8530 | Jaccard: 0.7306\n",
            "Epoch 3/3 |  val  | Loss: 2.9494 | Jaccard: 0.7142\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.3465 | Jaccard: 0.6627\n",
            "Epoch 1/3 |  val  | Loss: 2.9633 | Jaccard: 0.7184\n",
            "Epoch 2/3 | train | Loss: 2.8660 | Jaccard: 0.7257\n",
            "Epoch 2/3 |  val  | Loss: 2.9061 | Jaccard: 0.7272\n",
            "Epoch 3/3 | train | Loss: 2.7475 | Jaccard: 0.7464\n",
            "Epoch 3/3 |  val  | Loss: 2.9182 | Jaccard: 0.7230\n",
            "average jaccard: 0.7232\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "-----seeding 3, seed:1598120362\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.3507 | Jaccard: 0.6578\n",
            "Epoch 1/3 |  val  | Loss: 2.9283 | Jaccard: 0.7147\n",
            "Epoch 2/3 | train | Loss: 2.8755 | Jaccard: 0.7266\n",
            "Epoch 2/3 |  val  | Loss: 2.8810 | Jaccard: 0.7237\n",
            "Epoch 3/3 | train | Loss: 2.7443 | Jaccard: 0.7458\n",
            "Epoch 3/3 |  val  | Loss: 2.8791 | Jaccard: 0.7241\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.3004 | Jaccard: 0.6718\n",
            "Epoch 1/3 |  val  | Loss: 2.9064 | Jaccard: 0.7072\n",
            "Epoch 2/3 | train | Loss: 2.8418 | Jaccard: 0.7300\n",
            "Epoch 2/3 |  val  | Loss: 2.8574 | Jaccard: 0.7228\n",
            "Epoch 3/3 | train | Loss: 2.7092 | Jaccard: 0.7527\n",
            "Epoch 3/3 |  val  | Loss: 2.8835 | Jaccard: 0.7187\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.3159 | Jaccard: 0.6660\n",
            "Epoch 1/3 |  val  | Loss: 2.8985 | Jaccard: 0.7180\n",
            "Epoch 2/3 | train | Loss: 2.8319 | Jaccard: 0.7281\n",
            "Epoch 2/3 |  val  | Loss: 2.9089 | Jaccard: 0.7208\n",
            "Epoch 3/3 | train | Loss: 2.7110 | Jaccard: 0.7504\n",
            "Epoch 3/3 |  val  | Loss: 2.9064 | Jaccard: 0.7255\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2553 | Jaccard: 0.6762\n",
            "Epoch 1/3 |  val  | Loss: 2.8864 | Jaccard: 0.7215\n",
            "Epoch 2/3 | train | Loss: 2.8321 | Jaccard: 0.7316\n",
            "Epoch 2/3 |  val  | Loss: 2.8724 | Jaccard: 0.7203\n",
            "Epoch 3/3 | train | Loss: 2.6979 | Jaccard: 0.7537\n",
            "Epoch 3/3 |  val  | Loss: 2.8845 | Jaccard: 0.7274\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.2936 | Jaccard: 0.6715\n",
            "Epoch 1/3 |  val  | Loss: 2.8895 | Jaccard: 0.7258\n",
            "Epoch 2/3 | train | Loss: 2.8685 | Jaccard: 0.7253\n",
            "Epoch 2/3 |  val  | Loss: 2.8586 | Jaccard: 0.7245\n",
            "Epoch 3/3 | train | Loss: 2.7580 | Jaccard: 0.7432\n",
            "Epoch 3/3 |  val  | Loss: 2.8460 | Jaccard: 0.7294\n",
            "average jaccard: 0.7250\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "-----seeding 4, seed:1598123921\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.2875 | Jaccard: 0.6754\n",
            "Epoch 1/3 |  val  | Loss: 2.9131 | Jaccard: 0.7143\n",
            "Epoch 2/3 | train | Loss: 2.8406 | Jaccard: 0.7291\n",
            "Epoch 2/3 |  val  | Loss: 2.8659 | Jaccard: 0.7239\n",
            "Epoch 3/3 | train | Loss: 2.7110 | Jaccard: 0.7502\n",
            "Epoch 3/3 |  val  | Loss: 2.9092 | Jaccard: 0.7255\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.3204 | Jaccard: 0.6681\n",
            "Epoch 1/3 |  val  | Loss: 2.9151 | Jaccard: 0.7154\n",
            "Epoch 2/3 | train | Loss: 2.8970 | Jaccard: 0.7234\n",
            "Epoch 2/3 |  val  | Loss: 2.8548 | Jaccard: 0.7272\n",
            "Epoch 3/3 | train | Loss: 2.7496 | Jaccard: 0.7454\n",
            "Epoch 3/3 |  val  | Loss: 2.8845 | Jaccard: 0.7285\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2389 | Jaccard: 0.6771\n",
            "Epoch 1/3 |  val  | Loss: 2.8827 | Jaccard: 0.7189\n",
            "Epoch 2/3 | train | Loss: 2.8295 | Jaccard: 0.7309\n",
            "Epoch 2/3 |  val  | Loss: 2.8717 | Jaccard: 0.7315\n",
            "Epoch 3/3 | train | Loss: 2.7015 | Jaccard: 0.7547\n",
            "Epoch 3/3 |  val  | Loss: 2.8924 | Jaccard: 0.7195\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.4254 | Jaccard: 0.6510\n",
            "Epoch 1/3 |  val  | Loss: 2.9426 | Jaccard: 0.7199\n",
            "Epoch 2/3 | train | Loss: 2.8818 | Jaccard: 0.7234\n",
            "Epoch 2/3 |  val  | Loss: 2.9122 | Jaccard: 0.7271\n",
            "Epoch 3/3 | train | Loss: 2.7837 | Jaccard: 0.7379\n",
            "Epoch 3/3 |  val  | Loss: 2.9166 | Jaccard: 0.7064\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.3054 | Jaccard: 0.6710\n",
            "Epoch 1/3 |  val  | Loss: 2.9293 | Jaccard: 0.7157\n",
            "Epoch 2/3 | train | Loss: 2.8552 | Jaccard: 0.7259\n",
            "Epoch 2/3 |  val  | Loss: 2.8733 | Jaccard: 0.7253\n",
            "Epoch 3/3 | train | Loss: 2.7200 | Jaccard: 0.7514\n",
            "Epoch 3/3 |  val  | Loss: 2.8903 | Jaccard: 0.7262\n",
            "average jaccard: 0.7212\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "58  1598123921  0.721201  1598123921   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "58   True         3   True       0.1  \n",
            "-----seeding 5, seed:1598127479\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.2684 | Jaccard: 0.6711\n",
            "Epoch 1/3 |  val  | Loss: 2.9436 | Jaccard: 0.7212\n",
            "Epoch 2/3 | train | Loss: 2.8354 | Jaccard: 0.7308\n",
            "Epoch 2/3 |  val  | Loss: 2.8975 | Jaccard: 0.7308\n",
            "Epoch 3/3 | train | Loss: 2.7140 | Jaccard: 0.7495\n",
            "Epoch 3/3 |  val  | Loss: 2.9143 | Jaccard: 0.7237\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.3022 | Jaccard: 0.6747\n",
            "Epoch 1/3 |  val  | Loss: 2.8983 | Jaccard: 0.7131\n",
            "Epoch 2/3 | train | Loss: 2.8865 | Jaccard: 0.7236\n",
            "Epoch 2/3 |  val  | Loss: 2.8466 | Jaccard: 0.7324\n",
            "Epoch 3/3 | train | Loss: 2.7696 | Jaccard: 0.7424\n",
            "Epoch 3/3 |  val  | Loss: 2.8849 | Jaccard: 0.7262\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2822 | Jaccard: 0.6736\n",
            "Epoch 1/3 |  val  | Loss: 2.9124 | Jaccard: 0.7161\n",
            "Epoch 2/3 | train | Loss: 2.8569 | Jaccard: 0.7261\n",
            "Epoch 2/3 |  val  | Loss: 2.8740 | Jaccard: 0.7268\n",
            "Epoch 3/3 | train | Loss: 2.7421 | Jaccard: 0.7450\n",
            "Epoch 3/3 |  val  | Loss: 2.8818 | Jaccard: 0.7275\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2952 | Jaccard: 0.6710\n",
            "Epoch 1/3 |  val  | Loss: 2.9259 | Jaccard: 0.7205\n",
            "Epoch 2/3 | train | Loss: 2.8485 | Jaccard: 0.7287\n",
            "Epoch 2/3 |  val  | Loss: 2.8892 | Jaccard: 0.7307\n",
            "Epoch 3/3 | train | Loss: 2.7431 | Jaccard: 0.7473\n",
            "Epoch 3/3 |  val  | Loss: 2.9121 | Jaccard: 0.7250\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.3090 | Jaccard: 0.6684\n",
            "Epoch 1/3 |  val  | Loss: 2.9459 | Jaccard: 0.7109\n",
            "Epoch 2/3 | train | Loss: 2.8703 | Jaccard: 0.7247\n",
            "Epoch 2/3 |  val  | Loss: 2.9229 | Jaccard: 0.7206\n",
            "Epoch 3/3 | train | Loss: 2.7146 | Jaccard: 0.7507\n",
            "Epoch 3/3 |  val  | Loss: 2.9152 | Jaccard: 0.7240\n",
            "average jaccard: 0.7253\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "58  1598123921  0.721201  1598123921   True      0.5          4  False    4   \n",
            "59  1598127479  0.725263  1598127479   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "58   True         3   True       0.1  \n",
            "59   True         3   True       0.1  \n",
            "-----seeding 6, seed:1598131045\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.2568 | Jaccard: 0.6813\n",
            "Epoch 1/3 |  val  | Loss: 2.8863 | Jaccard: 0.7205\n",
            "Epoch 2/3 | train | Loss: 2.8441 | Jaccard: 0.7318\n",
            "Epoch 2/3 |  val  | Loss: 2.8588 | Jaccard: 0.7216\n",
            "Epoch 3/3 | train | Loss: 2.7100 | Jaccard: 0.7524\n",
            "Epoch 3/3 |  val  | Loss: 2.9400 | Jaccard: 0.7245\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.2939 | Jaccard: 0.6721\n",
            "Epoch 1/3 |  val  | Loss: 2.8922 | Jaccard: 0.7216\n",
            "Epoch 2/3 | train | Loss: 2.8333 | Jaccard: 0.7266\n",
            "Epoch 2/3 |  val  | Loss: 2.8759 | Jaccard: 0.7239\n",
            "Epoch 3/3 | train | Loss: 2.7098 | Jaccard: 0.7529\n",
            "Epoch 3/3 |  val  | Loss: 2.9109 | Jaccard: 0.7236\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.3577 | Jaccard: 0.6633\n",
            "Epoch 1/3 |  val  | Loss: 2.9028 | Jaccard: 0.7145\n",
            "Epoch 2/3 | train | Loss: 2.8635 | Jaccard: 0.7251\n",
            "Epoch 2/3 |  val  | Loss: 2.8745 | Jaccard: 0.7249\n",
            "Epoch 3/3 | train | Loss: 2.7396 | Jaccard: 0.7430\n",
            "Epoch 3/3 |  val  | Loss: 2.8878 | Jaccard: 0.7268\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2547 | Jaccard: 0.6771\n",
            "Epoch 1/3 |  val  | Loss: 2.8734 | Jaccard: 0.7273\n",
            "Epoch 2/3 | train | Loss: 2.8480 | Jaccard: 0.7267\n",
            "Epoch 2/3 |  val  | Loss: 2.8962 | Jaccard: 0.7118\n",
            "Epoch 3/3 | train | Loss: 2.7219 | Jaccard: 0.7471\n",
            "Epoch 3/3 |  val  | Loss: 2.8705 | Jaccard: 0.7303\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.3782 | Jaccard: 0.6533\n",
            "Epoch 1/3 |  val  | Loss: 3.0215 | Jaccard: 0.7019\n",
            "Epoch 2/3 | train | Loss: 2.8988 | Jaccard: 0.7163\n",
            "Epoch 2/3 |  val  | Loss: 2.9227 | Jaccard: 0.7192\n",
            "Epoch 3/3 | train | Loss: 2.7502 | Jaccard: 0.7437\n",
            "Epoch 3/3 |  val  | Loss: 2.9000 | Jaccard: 0.7170\n",
            "average jaccard: 0.7245\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "58  1598123921  0.721201  1598123921   True      0.5          4  False    4   \n",
            "59  1598127479  0.725263  1598127479   True      0.5          4  False    4   \n",
            "60  1598131045  0.724458  1598131045   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "58   True         3   True       0.1  \n",
            "59   True         3   True       0.1  \n",
            "60   True         3   True       0.1  \n",
            "-----seeding 7, seed:1598134613\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.4234 | Jaccard: 0.6461\n",
            "Epoch 1/3 |  val  | Loss: 2.9684 | Jaccard: 0.7140\n",
            "Epoch 2/3 | train | Loss: 2.9322 | Jaccard: 0.7103\n",
            "Epoch 2/3 |  val  | Loss: 2.9157 | Jaccard: 0.7221\n",
            "Epoch 3/3 | train | Loss: 2.8136 | Jaccard: 0.7293\n",
            "Epoch 3/3 |  val  | Loss: 2.9100 | Jaccard: 0.7210\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.2915 | Jaccard: 0.6726\n",
            "Epoch 1/3 |  val  | Loss: 2.8732 | Jaccard: 0.7226\n",
            "Epoch 2/3 | train | Loss: 2.8442 | Jaccard: 0.7264\n",
            "Epoch 2/3 |  val  | Loss: 2.8430 | Jaccard: 0.7280\n",
            "Epoch 3/3 | train | Loss: 2.7117 | Jaccard: 0.7482\n",
            "Epoch 3/3 |  val  | Loss: 2.8638 | Jaccard: 0.7314\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2605 | Jaccard: 0.6782\n",
            "Epoch 1/3 |  val  | Loss: 2.9404 | Jaccard: 0.7133\n",
            "Epoch 2/3 | train | Loss: 2.8295 | Jaccard: 0.7324\n",
            "Epoch 2/3 |  val  | Loss: 2.8944 | Jaccard: 0.7162\n",
            "Epoch 3/3 | train | Loss: 2.7053 | Jaccard: 0.7533\n",
            "Epoch 3/3 |  val  | Loss: 2.9254 | Jaccard: 0.7215\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2950 | Jaccard: 0.6746\n",
            "Epoch 1/3 |  val  | Loss: 2.9278 | Jaccard: 0.7205\n",
            "Epoch 2/3 | train | Loss: 2.8570 | Jaccard: 0.7264\n",
            "Epoch 2/3 |  val  | Loss: 2.8788 | Jaccard: 0.7283\n",
            "Epoch 3/3 | train | Loss: 2.7772 | Jaccard: 0.7412\n",
            "Epoch 3/3 |  val  | Loss: 2.9186 | Jaccard: 0.7203\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.3406 | Jaccard: 0.6623\n",
            "Epoch 1/3 |  val  | Loss: 2.9044 | Jaccard: 0.7223\n",
            "Epoch 2/3 | train | Loss: 2.9087 | Jaccard: 0.7228\n",
            "Epoch 2/3 |  val  | Loss: 2.8605 | Jaccard: 0.7180\n",
            "Epoch 3/3 | train | Loss: 2.7678 | Jaccard: 0.7427\n",
            "Epoch 3/3 |  val  | Loss: 2.8995 | Jaccard: 0.7261\n",
            "average jaccard: 0.7240\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "58  1598123921  0.721201  1598123921   True      0.5          4  False    4   \n",
            "59  1598127479  0.725263  1598127479   True      0.5          4  False    4   \n",
            "60  1598131045  0.724458  1598131045   True      0.5          4  False    4   \n",
            "61  1598134613  0.724037  1598134613   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "58   True         3   True       0.1  \n",
            "59   True         3   True       0.1  \n",
            "60   True         3   True       0.1  \n",
            "61   True         3   True       0.1  \n",
            "-----seeding 8, seed:1598138153\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.3275 | Jaccard: 0.6626\n",
            "Epoch 1/3 |  val  | Loss: 2.9611 | Jaccard: 0.7038\n",
            "Epoch 2/3 | train | Loss: 2.8883 | Jaccard: 0.7165\n",
            "Epoch 2/3 |  val  | Loss: 2.9438 | Jaccard: 0.7031\n",
            "Epoch 3/3 | train | Loss: 2.7662 | Jaccard: 0.7363\n",
            "Epoch 3/3 |  val  | Loss: 2.9435 | Jaccard: 0.7082\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.4319 | Jaccard: 0.6589\n",
            "Epoch 1/3 |  val  | Loss: 3.0642 | Jaccard: 0.6910\n",
            "Epoch 2/3 | train | Loss: 2.9356 | Jaccard: 0.7131\n",
            "Epoch 2/3 |  val  | Loss: 2.8848 | Jaccard: 0.7134\n",
            "Epoch 3/3 | train | Loss: 2.8094 | Jaccard: 0.7336\n",
            "Epoch 3/3 |  val  | Loss: 2.8847 | Jaccard: 0.7213\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2757 | Jaccard: 0.6731\n",
            "Epoch 1/3 |  val  | Loss: 2.9083 | Jaccard: 0.7181\n",
            "Epoch 2/3 | train | Loss: 2.8303 | Jaccard: 0.7287\n",
            "Epoch 2/3 |  val  | Loss: 2.9001 | Jaccard: 0.7253\n",
            "Epoch 3/3 | train | Loss: 2.7059 | Jaccard: 0.7488\n",
            "Epoch 3/3 |  val  | Loss: 2.9429 | Jaccard: 0.7181\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2740 | Jaccard: 0.6731\n",
            "Epoch 1/3 |  val  | Loss: 2.9222 | Jaccard: 0.7196\n",
            "Epoch 2/3 | train | Loss: 2.8301 | Jaccard: 0.7311\n",
            "Epoch 2/3 |  val  | Loss: 2.8575 | Jaccard: 0.7308\n",
            "Epoch 3/3 | train | Loss: 2.7057 | Jaccard: 0.7508\n",
            "Epoch 3/3 |  val  | Loss: 2.8891 | Jaccard: 0.7200\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.2858 | Jaccard: 0.6747\n",
            "Epoch 1/3 |  val  | Loss: 2.9087 | Jaccard: 0.7201\n",
            "Epoch 2/3 | train | Loss: 2.8446 | Jaccard: 0.7295\n",
            "Epoch 2/3 |  val  | Loss: 2.8621 | Jaccard: 0.7296\n",
            "Epoch 3/3 | train | Loss: 2.7294 | Jaccard: 0.7467\n",
            "Epoch 3/3 |  val  | Loss: 2.8657 | Jaccard: 0.7208\n",
            "average jaccard: 0.7177\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_1.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597852473  0.720900          42  False      0.5          4  False    8   \n",
            "1   1597856336  0.723000          42  False      0.5          4  False    8   \n",
            "2   1597860185  0.723400          42  False      0.5          4  False    8   \n",
            "3   1597864055  0.722800          42  False      0.5          4  False    8   \n",
            "4   1597867921  0.725200          42  False      0.5          4  False    8   \n",
            "5   1597871792  0.725300          42  False      0.5          4  False    8   \n",
            "6   1597875670  0.724100          42  False      0.5          4  False    8   \n",
            "7   1597879528  0.724500          42  False      0.5          4  False    8   \n",
            "8   1597883393  0.718700          42  False      0.5          4  False    8   \n",
            "9   1597887279  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597891179  0.722600          42  False      0.5          4  False    8   \n",
            "11  1597895078  0.723700          42  False      0.5          4  False    8   \n",
            "12  1597935012  0.723185          42  False      0.5          4  False    8   \n",
            "13  1597938551  0.722899          42  False      0.5          4  False    8   \n",
            "14  1597942065  0.720784          42  False      0.5          4  False    8   \n",
            "15  1597945601  0.723713          42  False      0.5          4  False    8   \n",
            "16  1597949146  0.723991          42  False      0.5          4  False    8   \n",
            "17  1597952690  0.724415          42  False      0.5          4  False    8   \n",
            "18  1597956223  0.727105          42  False      0.5          4  False    8   \n",
            "19  1597959750  0.725856          42  False      0.5          4  False    8   \n",
            "20  1597972749  0.725193          42  False      0.5          4  False    8   \n",
            "21  1597976276  0.727241          42  False      0.5          4  False    8   \n",
            "22  1597979801  0.725777          42  False      0.5          4  False    8   \n",
            "23  1597983328  0.724713          42  False      0.5          4  False    8   \n",
            "24  1597986859  0.722020          42  False      0.5          4  False    8   \n",
            "25  1597990392  0.725880          42  False      0.5          4  False    8   \n",
            "26  1597993929  0.720924          42  False      0.5          4  False    8   \n",
            "27  1597997466  0.722504          42  False      0.5          4  False    8   \n",
            "28  1598000994  0.724439          42  False      0.5          4  False    8   \n",
            "29  1598004523  0.725113          42  False      0.5          4  False    8   \n",
            "30  1598008060  0.722997          42  False      0.5          4  False    8   \n",
            "31  1598011599  0.718007          42  False      0.5          4  False    8   \n",
            "32  1598015143  0.721919          42  False      0.5          4  False    8   \n",
            "33  1598026093  0.724527  1598026093  False      0.5          4  False    8   \n",
            "34  1598029934  0.725588  1598029934  False      0.5          4  False    8   \n",
            "35  1598035029  0.724653  1598035029   True      0.5          4  False    4   \n",
            "36  1598038851  0.721379  1598038851   True      0.5          4  False    4   \n",
            "37  1598042683  0.722565  1598042683   True      0.5          4  False    4   \n",
            "38  1598046510  0.723758  1598046510   True      0.5          4  False    4   \n",
            "39  1598050342  0.725403  1598050342   True      0.5          4  False    4   \n",
            "40  1598054180  0.725029  1598054180   True      0.5          4  False    4   \n",
            "41  1598058009  0.727703  1598058009   True      0.5          4  False    4   \n",
            "42  1598061849  0.725012  1598061849   True      0.5          4  False    4   \n",
            "43  1598065685  0.725373  1598065685   True      0.5          4  False    4   \n",
            "44  1598069521  0.724929  1598069521   True      0.5          4  False    4   \n",
            "45  1598073376  0.723769  1598073376   True      0.5          4  False    4   \n",
            "46  1598077230  0.724819  1598077230   True      0.5          4  False    4   \n",
            "47  1598081090  0.726039  1598081090   True      0.5          4  False    4   \n",
            "48  1598084952  0.725705  1598084952   True      0.5          4  False    4   \n",
            "49  1598088817  0.727207  1598088817   True      0.5          4  False    4   \n",
            "50  1598092681  0.724106  1598092681   True      0.5          4  False    4   \n",
            "51  1598096546  0.724567  1598096546   True      0.5          4  False    4   \n",
            "52  1598100401  0.717380  1598100401   True      0.5          4  False    4   \n",
            "53  1598104258  0.723530  1598104258   True      0.5          4  False    4   \n",
            "54  1598108103  0.727064  1598108103   True      0.5          4  False    4   \n",
            "55  1598113266  0.723796  1598113266   True      0.5          4  False    4   \n",
            "56  1598116829  0.723224  1598116829   True      0.5          4  False    4   \n",
            "57  1598120362  0.725018  1598120362   True      0.5          4  False    4   \n",
            "58  1598123921  0.721201  1598123921   True      0.5          4  False    4   \n",
            "59  1598127479  0.725263  1598127479   True      0.5          4  False    4   \n",
            "60  1598131045  0.724458  1598131045   True      0.5          4  False    4   \n",
            "61  1598134613  0.724037  1598134613   True      0.5          4  False    4   \n",
            "62  1598138153  0.717678  1598138153   True      0.5          4  False    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor  \n",
            "0   False         3  False       0.1  \n",
            "1   False         3  False       0.1  \n",
            "2   False         3  False       0.1  \n",
            "3   False         3  False       0.1  \n",
            "4   False         3  False       0.1  \n",
            "5   False         3  False       0.1  \n",
            "6   False         3  False       0.1  \n",
            "7   False         3  False       0.1  \n",
            "8   False         3  False       0.1  \n",
            "9   False         3  False       0.1  \n",
            "10  False         3  False       0.1  \n",
            "11  False         3  False       0.1  \n",
            "12  False         3  False       0.1  \n",
            "13  False         3  False       0.1  \n",
            "14  False         3  False       0.1  \n",
            "15  False         3  False       0.1  \n",
            "16  False         3  False       0.1  \n",
            "17  False         3  False       0.1  \n",
            "18  False         3  False       0.1  \n",
            "19  False         3  False       0.1  \n",
            "20  False         3  False       0.1  \n",
            "21  False         3  False       0.1  \n",
            "22  False         3  False       0.1  \n",
            "23  False         3  False       0.1  \n",
            "24  False         3  False       0.1  \n",
            "25  False         3  False       0.1  \n",
            "26  False         3  False       0.1  \n",
            "27  False         3  False       0.1  \n",
            "28  False         3  False       0.1  \n",
            "29  False         3  False       0.1  \n",
            "30  False         3  False       0.1  \n",
            "31  False         3  False       0.1  \n",
            "32  False         3  False       0.1  \n",
            "33  False         3  False       0.1  \n",
            "34  False         3  False       0.1  \n",
            "35   True         3   True       0.1  \n",
            "36   True         3   True       0.1  \n",
            "37   True         3   True       0.1  \n",
            "38   True         3   True       0.1  \n",
            "39   True         3   True       0.1  \n",
            "40   True         3   True       0.1  \n",
            "41   True         3   True       0.1  \n",
            "42   True         3   True       0.1  \n",
            "43   True         3   True       0.1  \n",
            "44   True         3   True       0.1  \n",
            "45   True         3   True       0.1  \n",
            "46   True         3   True       0.1  \n",
            "47   True         3   True       0.1  \n",
            "48   True         3   True       0.1  \n",
            "49   True         3   True       0.1  \n",
            "50   True         3   True       0.1  \n",
            "51   True         3   True       0.1  \n",
            "52   True         3   True       0.1  \n",
            "53   True         3   True       0.1  \n",
            "54   True         3   True       0.1  \n",
            "55   True         3   True       0.1  \n",
            "56   True         3   True       0.1  \n",
            "57   True         3   True       0.1  \n",
            "58   True         3   True       0.1  \n",
            "59   True         3   True       0.1  \n",
            "60   True         3   True       0.1  \n",
            "61   True         3   True       0.1  \n",
            "62   True         3   True       0.1  \n",
            "-----seeding 9, seed:1598141693\n",
            "Fold: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFL3hBPKcyub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#seed_jaccard = pd.read_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)\n",
        "\n",
        "\n",
        "#seed_jaccard['MSD'] = False\n",
        "#seed_jaccard['MSDrate'] = 0.5\n",
        "#seed_jaccard['MSDsample'] = 4\n",
        "#seed_jaccard['ML'] = False\n",
        "#seed_jaccard['MLn'] = 8\n",
        "#seed_jaccard['ML'] = False\n",
        "#seed_jaccard['MLR'] = False\n",
        "#seed_jaccard['MLRtimes'] = 3\n",
        "#seed_jaccard['SL'] = False\n",
        "#seed_jaccard['SLfactor'] = 0.1\n",
        "\n",
        "\n",
        "#seed_jaccard.to_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHBPq11q99H4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "no multi-sample dropout:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.3472 | Jaccard: 0.6409\n",
        "Epoch 1/3 |  val  | Loss: 1.6409 | Jaccard: 0.7040\n",
        "Epoch 2/3 | train | Loss: 1.5979 | Jaccard: 0.7133\n",
        "Epoch 2/3 |  val  | Loss: 1.5697 | Jaccard: 0.7138\n",
        "Epoch 3/3 | train | Loss: 1.4090 | Jaccard: 0.7414\n",
        "Epoch 3/3 |  val  | Loss: 1.5755 | Jaccard: 0.7171\n",
        "Fold: 2\n",
        "DEBUG skip fold:2\n",
        "CPU times: user 45.9 s, sys: 25.9 s, total: 1min 11s\n",
        "Wall time: 1min 16s\n",
        "\n",
        "with multi-sample dropout .2*5:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.2051 | Jaccard: 0.6481\n",
        "Epoch 1/3 |  val  | Loss: 1.5972 | Jaccard: 0.7131\n",
        "Epoch 2/3 | train | Loss: 1.4994 | Jaccard: 0.7283\n",
        "Epoch 2/3 |  val  | Loss: 1.5758 | Jaccard: 0.7133\n",
        "Epoch 3/3 | train | Loss: 1.3283 | Jaccard: 0.7484\n",
        "Epoch 3/3 |  val  | Loss: 1.5696 | Jaccard: 0.7183\n",
        "\n",
        "\n",
        "with multi-sample dropout and all layers:\n",
        "Epoch 1/3 | train | Loss: 2.2779 | Jaccard: 0.6477\n",
        "Epoch 1/3 |  val  | Loss: 1.5889 | Jaccard: 0.7085\n",
        "Epoch 2/3 | train | Loss: 1.5759 | Jaccard: 0.7224\n",
        "Epoch 2/3 |  val  | Loss: 1.5932 | Jaccard: 0.7108\n",
        "Epoch 3/3 | train | Loss: 1.3961 | Jaccard: 0.7394\n",
        "Epoch 3/3 |  val  | Loss: 1.5826 | Jaccard: 0.7175\n",
        "\n",
        "with multi-sample dropout and multi-learning rate 500:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.0589 | Jaccard: 0.6682\n",
        "Epoch 1/3 |  val  | Loss: 1.5709 | Jaccard: 0.7098\n",
        "Epoch 2/3 | train | Loss: 1.6758 | Jaccard: 0.7055\n",
        "Epoch 2/3 |  val  | Loss: 1.6627 | Jaccard: 0.6960\n",
        "Epoch 3/3 | train | Loss: 1.4640 | Jaccard: 0.7355\n",
        "Epoch 3/3 |  val  | Loss: 1.5935 | Jaccard: 0.7173\n",
        "\n",
        "\n",
        "multisample dropout .3*5:\n",
        "\n",
        "223.6s\n",
        "31\n",
        "Epoch 1/3 | train | Loss: 2.2905 | Jaccard: 0.6341\n",
        "283.8s\n",
        "32\n",
        "Epoch 1/3 |  val  | Loss: 1.6386 | Jaccard: 0.7064\n",
        "418.3s\n",
        "33\n",
        "Epoch 2/3 | train | Loss: 1.5642 | Jaccard: 0.7169\n",
        "479.0s\n",
        "34\n",
        "Epoch 2/3 |  val  | Loss: 1.5885 | Jaccard: 0.7112\n",
        "613.7s\n",
        "35\n",
        "Epoch 3/3 | train | Loss: 1.3810 | Jaccard: 0.7443\n",
        "674.8s\n",
        "36\n",
        "Epoch 3/3 |  val  | Loss: 1.5906 | Jaccard: 0.7222\n",
        "\n",
        "multisample dropout .3*8:\n",
        "\n",
        "221.7s\n",
        "31\n",
        "Epoch 1/3 | train | Loss: 2.2384 | Jaccard: 0.6376\n",
        "281.9s\n",
        "32\n",
        "Epoch 1/3 |  val  | Loss: 1.5844 | Jaccard: 0.7118\n",
        "416.4s\n",
        "33\n",
        "Epoch 2/3 | train | Loss: 1.5052 | Jaccard: 0.7242\n",
        "476.1s\n",
        "34\n",
        "Epoch 2/3 |  val  | Loss: 1.5753 | Jaccard: 0.7121\n",
        "610.1s\n",
        "35\n",
        "Epoch 3/3 | train | Loss: 1.3466 | Jaccard: 0.7469\n",
        "668.5s\n",
        "36\n",
        "Epoch 3/3 |  val  | Loss: 1.5563 | Jaccard: 0.7204\n",
        "\n",
        "ML4 layer + MSD.3*5\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.3425 | Jaccard: 0.6331\n",
        "Epoch 1/3 |  val  | Loss: 1.6264 | Jaccard: 0.7057\n",
        "Epoch 2/3 | train | Loss: 1.6000 | Jaccard: 0.7132\n",
        "Epoch 2/3 |  val  | Loss: 1.5563 | Jaccard: 0.7164\n",
        "Epoch 3/3 | train | Loss: 1.4459 | Jaccard: 0.7348\n",
        "Epoch 3/3 |  val  | Loss: 1.5472 | Jaccard: 0.7266\n",
        "\n",
        "multi-layer-4:\n",
        "\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.4450 | Jaccard: 0.6277\n",
        "Epoch 1/3 |  val  | Loss: 1.6236 | Jaccard: 0.7080\n",
        "Epoch 2/3 | train | Loss: 1.5886 | Jaccard: 0.7169\n",
        "Epoch 2/3 |  val  | Loss: 1.5905 | Jaccard: 0.7137\n",
        "Epoch 3/3 | train | Loss: 1.4059 | Jaccard: 0.7393\n",
        "Epoch 3/3 |  val  | Loss: 1.5506 | Jaccard: 0.7203\n",
        "\n",
        "ML8 layer + MSD.3*5\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.3470 | Jaccard: 0.6376\n",
        "Epoch 1/3 |  val  | Loss: 1.6091 | Jaccard: 0.7061\n",
        "Epoch 2/3 | train | Loss: 1.5642 | Jaccard: 0.7200\n",
        "Epoch 2/3 |  val  | Loss: 1.5585 | Jaccard: 0.7129\n",
        "Epoch 3/3 | train | Loss: 1.3812 | Jaccard: 0.7438\n",
        "Epoch 3/3 |  val  | Loss: 1.5480 | Jaccard: 0.7228\n",
        "\n",
        "ML8 layer + MSD.3*5 + Multi LR 3:\n",
        "Epoch 1/3 | train | Loss: 2.2526 | Jaccard: 0.6451\n",
        "Epoch 1/3 |  val  | Loss: 1.5831 | Jaccard: 0.7119\n",
        "Epoch 2/3 | train | Loss: 1.5375 | Jaccard: 0.7217\n",
        "Epoch 2/3 |  val  | Loss: 1.5731 | Jaccard: 0.7127\n",
        "Epoch 3/3 | train | Loss: 1.3621 | Jaccard: 0.7431\n",
        "Epoch 3/3 |  val  | Loss: 1.5527 | Jaccard: 0.7232\n",
        "\n",
        "ML8 layer + MSD.3*5 + Multi LR 8:\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.2921 | Jaccard: 0.6418\n",
        "Epoch 1/3 |  val  | Loss: 1.5859 | Jaccard: 0.7150\n",
        "Epoch 2/3 | train | Loss: 1.5481 | Jaccard: 0.7219\n",
        "Epoch 2/3 |  val  | Loss: 1.5601 | Jaccard: 0.7138\n",
        "Epoch 3/3 | train | Loss: 1.3870 | Jaccard: 0.7398\n",
        "Epoch 3/3 |  val  | Loss: 1.5497 | Jaccard: 0.7217\n",
        "\n",
        "multi-layer-8:\n",
        "Fold: 1\n",
        "Epoch 1/3 | train | Loss: 2.4506 | Jaccard: 0.6319\n",
        "Epoch 1/3 |  val  | Loss: 1.6295 | Jaccard: 0.7055\n",
        "Epoch 2/3 | train | Loss: 1.6330 | Jaccard: 0.7106\n",
        "Epoch 2/3 |  val  | Loss: 1.5786 | Jaccard: 0.7183\n",
        "Epoch 3/3 | train | Loss: 1.4762 | Jaccard: 0.7282\n",
        "Epoch 3/3 |  val  | Loss: 1.5342 | Jaccard: 0.7215\n",
        "\n",
        "multi-layer-all but embedding layer:\n",
        "\n",
        "Epoch 1/3 | train | Loss: 2.2776 | Jaccard: 0.6499\n",
        "Epoch 1/3 |  val  | Loss: 1.5854 | Jaccard: 0.7095\n",
        "Epoch 2/3 | train | Loss: 1.5341 | Jaccard: 0.7229\n",
        "Epoch 2/3 |  val  | Loss: 1.5664 | Jaccard: 0.7231\n",
        "Epoch 3/3 | train | Loss: 1.3625 | Jaccard: 0.7441\n",
        "Epoch 3/3 |  val  | Loss: 1.5995 | Jaccard: 0.7146\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PL1nfl-BmqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXPLORE = False\n",
        "\n",
        "if EXPLORE:\n",
        "\n",
        "    train_df_explore = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "    train_df_explore.head()\n",
        "    \n",
        "    train_df_explore['text'] = train_df_explore['text'].astype(str)\n",
        "    train_df_explore['selected_text'] = train_df_explore['selected_text'].astype(str)\n",
        "    \n",
        "    ds = TweetDataset(train_df_explore)\n",
        "    \n",
        "    \n",
        "    ds1 = TweetDataset(train_df_explore, use_fifth=False)\n",
        "    \n",
        "    same_count = 0\n",
        "    diff_count = 0\n",
        "    \n",
        "    for d1, d2 in zip(ds, ds1):\n",
        "        if d1['start_idx'] != d2['start_idx'] or  d1['end_idx'] != d2['end_idx']:\n",
        "            print(f\"tweet:[{d1['tweet']}] d1!=d2, select: [{d1['select']}]\")\n",
        "            print(\"d1:%s\" % d1['tweet'][d1['offsets'][d1['start_idx']][0]:d1['offsets'][d1['end_idx']][1]])\n",
        "            print(\"d2:%s\\n\" % d2['tweet'][d2['offsets'][d2['start_idx']][0]:d2['offsets'][d2['end_idx']][1]])\n",
        "            diff_count += 1\n",
        "        else:\n",
        "            same_count += 1\n",
        "    \n",
        "    print(f\"diff:{diff_count}, same:{same_count}\")\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ru7zVFpfmqQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(ds1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1x9WP5zhmqQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "if EXPLORE:\n",
        "    train_df_explore = train_df_explore.fillna(\" \")\n",
        "    train_df_explore[train_df_explore['text'].isna()]\n",
        "\n",
        "    print(train_df_explore.shape)\n",
        "\n",
        "    train_df_explore[train_df_explore['text'].str.contains(\"  \")].shape\n",
        "\n",
        "    half_word_ids = []\n",
        "\n",
        "    PRINT_CUT_OFF = 300\n",
        "\n",
        "    for aa, dd in zip(train_df_explore.iterrows(),ds) :\n",
        "        index, row = aa\n",
        "    \n",
        "        # first word in selected_text:\n",
        "        space_index = row['selected_text'].find(\" \")\n",
        "        first_word = \"\"\n",
        "        if space_index == -1:\n",
        "            first_word = row['selected_text']\n",
        "        else:\n",
        "            first_word = row['selected_text'][:space_index]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} first word:{first_word}\")\n",
        "        if len(first_word) == 0:\n",
        "            continue\n",
        "\n",
        "        # last word\n",
        "        space_index = row['selected_text'].rfind(\" \")\n",
        "        if space_index == -1:\n",
        "            last_word = row['selected_text']\n",
        "        else:\n",
        "            last_word = row['selected_text'][space_index+1:]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} last word:{last_word}\")\n",
        "\n",
        "        half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        not_found = False\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(first_word, id_start)\n",
        "            if id0 == -1:\n",
        "                # not found continue\n",
        "                not_found = True\n",
        "                break\n",
        "\n",
        "            if id0 == 0 or not row['text'][id0-1].isalpha():\n",
        "                half_word = False\n",
        "                break\n",
        "            else:\n",
        "                half_word = True\n",
        "\n",
        "            id_start = id0+len(row['text'])\n",
        "\n",
        "        if not_found:\n",
        "            continue\n",
        "\n",
        "        if half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half first word found in [{row['text']}], word:[{first_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][1]])\n",
        "                \n",
        "\n",
        "            half_word_ids.append(index)\n",
        "\n",
        "        # for last word:\n",
        "        last_half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(last_word, id_start)\n",
        "            if index == 50:\n",
        "                '''\n",
        "                tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "                    vocab_file='../input/roberta-base/vocab.json', \n",
        "                    merges_file='../input/roberta-base/merges.txt', \n",
        "                    lowercase=True,\n",
        "                    add_prefix_space=True)\n",
        "\n",
        "                tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "                encoding = tokenizer.encode(tweet)\n",
        "                print(f\"encoding:{encoding}\")\n",
        "                print(f\"encoding:{tweet}\")\n",
        "                print(f\"encoding:{encoding.ids}\")\n",
        "                print(f\"encoding:{encoding.tokens}\")\n",
        "                print(f\"encoding:{encoding.offsets}\")\n",
        "\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[34:41])]}\")\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[-7:])]}\")\n",
        "\n",
        "\n",
        "                print(f\"\\n{index}-id0:{id0}, found:{row['text'][id0:id0+len(last_word)]}; char:{row['text'][id0+len(last_word)]}, isalpha:{row['text'][id0+len(last_word)].isalpha()}\\n\")\n",
        "                '''\n",
        "                pass\n",
        "\n",
        "            if id0 == -1:\n",
        "                break\n",
        "            if id0+len(last_word)< len(row['text']) and not row['text'][id0+len(last_word)].isalpha():\n",
        "                last_half_word = False\n",
        "                break\n",
        "            elif id0+len(last_word) == len(row['text']):\n",
        "                last_half_word = False\n",
        "                break\n",
        "            else:\n",
        "                last_half_word = True\n",
        "\n",
        "            id_start = id0+len(last_word)\n",
        "\n",
        "        if last_half_word and not half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half last word found in [{row['text']}], word:[{last_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][0]])\n",
        "                \n",
        "            half_word_ids.append(index)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A64hmGU1mqRA",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rx8jYSScmqRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "\n",
        "test_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "test_df['text'] = test_df['text'].astype(str)\n",
        "test_loader = get_test_loader(test_df)\n",
        "predictions = []\n",
        "fold_predictions = []\n",
        "fold_prediction_scores = []\n",
        "\n",
        "for i in range(FOLD_COUNT):\n",
        "  fold_predictions.append([])\n",
        "  fold_prediction_scores.append([])\n",
        "\n",
        "models = []\n",
        "for fold in range(skf.n_splits):\n",
        "    \n",
        "    if DEBUG_FOLD:\n",
        "        if fold != 0:\n",
        "            print(f\"DEBUG infer skip fold:{fold}\")\n",
        "            continue\n",
        "            \n",
        "    model = TweetModel()\n",
        "    model.cuda()\n",
        "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "    \n",
        "same_count = 0\n",
        "diff_count = 0\n",
        "\n",
        "print(\"loading data...\")\n",
        "    \n",
        "fold_2_app_count = 0\n",
        "for data in test_loader:\n",
        "    ids = data['ids'].cuda()\n",
        "    masks = data['masks'].cuda()\n",
        "    tweet = data['tweet']\n",
        "    original_tweet = data['original_tweet']\n",
        "    offsets = data['offsets'].numpy()\n",
        "\n",
        "    start_logits_list = []\n",
        "    end_logits_list = []\n",
        "\n",
        "    #print(f\"{start_logits_list}\")\n",
        "    #print(f\"processing {data}\")\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            output = model(ids, masks)\n",
        "            start_logits_list.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
        "            end_logits_list.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
        "\n",
        "    start_logits = np.mean(start_logits_list, axis=0)\n",
        "    end_logits = np.mean(end_logits_list, axis=0)\n",
        "\n",
        "    \n",
        "    for i in range(len(ids)):    \n",
        "        start_pred = np.argmax(start_logits[i])\n",
        "        end_pred = np.argmax(end_logits[i])\n",
        "        \n",
        "        #print(f\"processing {i}\")\n",
        "        if start_pred > end_pred:\n",
        "            pred = tweet[i]\n",
        "        else:\n",
        "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "            try:\n",
        "                fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "            except:\n",
        "                fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                \n",
        "            \n",
        "            if pred!=fifth_pred:\n",
        "                diff_count += 1\n",
        "                #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "            else:\n",
        "                same_count += 1\n",
        "\n",
        "            pp_pred = pp(pred, original_tweet[i])\n",
        "            \n",
        "            pred = fifth_pred\n",
        "        predictions.append(pred)\n",
        "\n",
        "\n",
        "    # generate psuedo label lists from models:\n",
        "\n",
        "    #print(f\"len start {len(start_logits_list)}\")\n",
        "\n",
        "\n",
        "    for fold, start_logits, end_logits in zip(range(len(start_logits_list)), start_logits_list, end_logits_list):\n",
        "      #print(f\"fold: {fold} start_logits:{len(start_logits)} end_logits:{len(end_logits)}\")\n",
        "      for i in range(len(ids)):    \n",
        "          start_pred = np.argmax(start_logits[i])\n",
        "          start_pred_socre = np.amax(start_logits[i])\n",
        "          end_pred = np.argmax(end_logits[i])\n",
        "          end_pred_score = np.amax(end_logits[i])\n",
        "          \n",
        "          #print(f\"processing {i}\")\n",
        "          if start_pred > end_pred:\n",
        "              pred = tweet[i]\n",
        "          else:\n",
        "              pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "              try:\n",
        "                  fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "              except:\n",
        "                  fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                  \n",
        "              \n",
        "              if pred!=fifth_pred:\n",
        "                  diff_count += 1\n",
        "                  #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "              else:\n",
        "                  same_count += 1\n",
        "\n",
        "              pp_pred = pp(pred, original_tweet[i])\n",
        "              \n",
        "              pred = fifth_pred\n",
        "          fold_predictions[fold].append(pred)\n",
        "          fold_prediction_scores[fold].append((start_pred_socre + end_pred_score)/2.)\n",
        "\n",
        "print(f\"same:{same_count} diff:{diff_count}\")\n",
        "\n",
        "print(f\"fold_2_app_count:{fold_2_app_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L51vQ_V3XmCW",
        "colab_type": "text"
      },
      "source": [
        "# Gen psuedo label files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b3eS3TkXw2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if GEN_PSUEDO_LABELS:\n",
        "  ori_sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "\n",
        "  for fold, predictions, scores in zip(range(len(fold_predictions)), fold_predictions, fold_prediction_scores):\n",
        "    sub_df = ori_sub_df.copy(True)\n",
        "\n",
        "    print(sub_df)\n",
        "    print(len(fold_predictions[0]))\n",
        "    print(len(fold_predictions[1]))\n",
        "    print(len(fold_predictions[2]))\n",
        "    print(len(fold_predictions[3]))\n",
        "    print(len(fold_predictions[4]))\n",
        "    sub_df['selected_text'] = predictions\n",
        "    sub_df['score'] = scores\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "    sub_df = sub_df[sub_df.score > .35]\n",
        "\n",
        "    del sub_df['score']\n",
        "    sub_df.to_csv(\"psuedo_labels_fold_%d.csv\" % fold,  index=False)\n",
        "    sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0U922cmqRO",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0euGDHjmqRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "sub_df['selected_text'] = predictions\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}