{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "seed_finding_2-tweet-sentiment-roberta-with-pp-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyogavin/kaggle_experiments/blob/master/seed_finding_2_tweet_sentiment_roberta_with_pp_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SnNGHZ9mqPc",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuosFIsJmvAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "315710a8-8725-4ca2-8e83-8f94dddb2820"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE-fp6Obpkma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT_PATH=\"/content/drive/My Drive/kaggle\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vb_2PjzTym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2acd9e87-e851-4acb-8a0e-e507611801b5"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAtgWfjbyfoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flags:\n",
        "FOLD_COUNT = 5\n",
        "\n",
        "NOTEBOOK_ID = 2\n",
        "\n",
        "\n",
        "DEBUG_LOAD = False\n",
        "DEBUG_FOLD = False\n",
        "USE_MULTI_SAMPLE_DROPOUT = True #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L119\n",
        "USE_MULTI_SAMPLE_DROPOUT_RATE = 0.5\n",
        "USE_MULTI_SAMPLE_DROPOUT_SAMPLE = 4\n",
        "\n",
        "USE_BERT_ALL_LAYERS = True # https://www.kaggle.com/c/google-quest-challenge/discussion/129840\n",
        "#https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L76\n",
        "\n",
        "USE_BERT_LAST_N_LAYERS = 4\n",
        "\n",
        "USE_MULTIPLE_LEARNING_RATE = True #https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/master/step5_model3_roberta_code/model.py#L132\n",
        "USE_MULTIPLE_LEARNING_RATE_TIMES = 3\n",
        "\n",
        "\n",
        "GEN_PSUEDO_LABELS = False\n",
        "TRAIN_WITH_PSUEDO_LABELS = False\n",
        "\n",
        "USE_TRY_MULTI_SEEDING = True\n",
        "\n",
        "USE_SEQUENCE_BUCKETING = True\n",
        "USE_SMOOTH_LABELING = True\n",
        "LABEL_SMOOTH = 0.1\n",
        "\n",
        "\n",
        "PAD_ID = 1\n",
        "MAX_LEN = 96\n",
        "\n",
        "USE_SEED = 42"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvHmeSfLqaxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "06206d4a-aab0-4ffb-cac5-ec558af906a4"
      },
      "source": [
        "!pip install tokenizers transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 2.8MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 33.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=10f708e553263d0f5c9a49b80c94d77512c2caaf69b31ce29ea1b44c03922507\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "VP8YyNajmqPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JxIBCKatmqPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a1ccd10c-801d-4505-a41f-9cc7be0a1fa9"
      },
      "source": [
        "import re\n",
        "\n",
        "USE_PP = False\n",
        "\n",
        "def pp(filtered_output, real_tweet):\n",
        "    #if not USE_PP:\n",
        "    #    return filtered_output\n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "            if filtered_output.endswith(\"..\"):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n",
        "                return filtered_output\n",
        "            if filtered_output.endswith('!!'):\n",
        "                if real_tweet.startswith(\" \"):\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                else:\n",
        "                    st = real_tweet.find(filtered_output)\n",
        "                    fl = real_tweet.find(\"  \")\n",
        "                    if fl != -1 and fl < st:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
        "                    else:\n",
        "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n",
        "                return filtered_output\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "            start = text_annotetor.find(filtered_output)\n",
        "            end = start + len(filtered_output)\n",
        "            start -= 0\n",
        "            end += 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output = real_tweet[start:end]\n",
        "    return filtered_output\n",
        "\n",
        "def prp(filtered_output, real_tweet):\n",
        "    if not USE_PP:\n",
        "        return filtered_output\n",
        "    \n",
        "    filtered_output = ' '.join(filtered_output.split())\n",
        "    if len(real_tweet.split()) < 2:\n",
        "        filtered_output = real_tweet\n",
        "    else:\n",
        "        if len(filtered_output.split()) == 1:\n",
        "\n",
        "            st = real_tweet.find(filtered_output)\n",
        "            fl = real_tweet.find(\"  \")\n",
        "            end = st + len(filtered_output)\n",
        "            \n",
        "            #if fl != -1 and fl < st:\n",
        "            while st != -1 and end < len(real_tweet) and (real_tweet[end] == '.' or real_tweet[end] == '!'):\n",
        "                filtered_output = filtered_output + real_tweet[end]\n",
        "                end += 1\n",
        "                    \n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "        if real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            #print(filtered_output)\n",
        "            text_annotetor = ' '.join(real_tweet.split())\n",
        "            \n",
        "            \n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "        \n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "\n",
        "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
        "            striped_filtered_output = filtered_output.strip()\n",
        "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
        "\n",
        "            start = real_tweet.find(striped_filtered_output)\n",
        "            end = start + len(striped_filtered_output)\n",
        "            \n",
        "            start -= 0\n",
        "            end -= 2\n",
        "            flag = real_tweet.find(\"  \")\n",
        "            if flag < start:\n",
        "                filtered_output0 = text_annotetor[start:end]\n",
        "                if len(filtered_output0.strip()) != 0:\n",
        "                    filtered_output = filtered_output0\n",
        "    return filtered_output\n",
        "\n",
        "tweet = \"  ROFLMAO for the funny web portal  =D\"\n",
        "pred = \"funny\"\n",
        "answer = \"e funny\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "tweet = \" yea i just got outta one too....i want him back tho  but i feel the same way...i`m cool on dudes for a lil while\"\n",
        "pred = \"cool\"\n",
        "answer = \"m cool\"\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \"Ow... My shoulder muscle (I can`t remember the name :p) hurts... What did I do?  I don`t even know\"\n",
        "pred = \"hurts...\"\n",
        "answer = \"hurts..\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "tweet = \" yep... or it should b automatic that if u fall 4 someone that person does 2!or smthng like that... but the way it is sucks!\"\n",
        "pred = \"SUCKS!\"\n",
        "answer = \"SUCKS!\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n",
        "\n",
        "\n",
        "\n",
        "tweet = \" hi holly i`ll volunteer to try it out first for u! hope ur having a fab weekend xoxox...\"\n",
        "pred = \"fab\"\n",
        "answer = \"g a\"\n",
        "\n",
        "\n",
        "print(pp(pred, tweet))\n",
        "print(prp(answer, tweet))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e funny\n",
            "e funny\n",
            "m cool\n",
            "m cool\n",
            "hurts..\n",
            "hurts..\n",
            "SUCKS!\n",
            "SUCKS!\n",
            " fab \n",
            "g a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8-G9tYSXmqPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d26fcfd3-1511-48b5-9ba2-40c8ee2c39e9"
      },
      "source": [
        "\"  \" in tweet"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qb1WhQoSmqPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fifth_prp(select, tweet, sentiment, offsets):\n",
        "    #print('entering fifth prp')\n",
        "    ss = tweet.find(select)\n",
        "    if tweet[max(ss - 2, 0):ss] == '  ':\n",
        "        ss -= 2\n",
        "    if ss > 0  and tweet[ss - 1] == ' ':\n",
        "        ss -= 1\n",
        "\n",
        "    ee = ss + len(select)\n",
        "\n",
        "    if re.match(r' [^ ]', tweet) is not None:\n",
        "        ee -= 1\n",
        "\n",
        "    ss = max(0, ss)\n",
        "    if '  ' in tweet[:ss] and sentiment != 'neutral':\n",
        "        text1 = \" \".join(tweet.split())\n",
        "        sel = text1[ss:ee].strip()\n",
        "        if len(sel) > 1 and sel[-2] == ' ':\n",
        "            sel = sel[:-2]\n",
        "\n",
        "        select = sel\n",
        "\n",
        "    text1 = \" \"+\" \".join(tweet.split())\n",
        "    text2 = \" \".join(select.split()).lstrip(\".,;:\")\n",
        "\n",
        "    idx = text1.find(text2)\n",
        "    if idx != -1:\n",
        "        chars = np.zeros((len(text1)))\n",
        "        chars[idx:idx+len(text2)]=1\n",
        "        if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    else:\n",
        "        import pdb;pdb.set_trace()\n",
        "        chars = np.ones((len(text1)))\n",
        "        \n",
        "        \n",
        "    #tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    #        vocab_file='../input/roberta-base/vocab.json', \n",
        "    #        merges_file='../input/roberta-base/merges.txt', \n",
        "    #        lowercase=True,\n",
        "    #        add_prefix_space=True)\n",
        "    \n",
        "    #enc = tokenizer.encode(text1) \n",
        "\n",
        "    # ID_OFFSETS\n",
        "    #offsets = enc.offsets\n",
        "\n",
        "    # START END TOKENS\n",
        "    _toks = []\n",
        "\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.mean(chars[a:b])\n",
        "        #if (sm > 0.6 and chars[a] != 0):  # こうすると若干伸びるけど...\n",
        "        if (sm > 0.5 and chars[a] != 0): \n",
        "            _toks.append(i)\n",
        "            \n",
        "    #print(\"returnning fifth prep\")\n",
        "    return _toks[0], _toks[-1]\n",
        "\n",
        "    '''\n",
        "    toks = _toks\n",
        "    s_tok = sentiment_id[sentiments[k]]\n",
        "    input_ids[k, :len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask[k,:len(enc.ids)+3] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+2] = 1\n",
        "        end_tokens[k,toks[-1]+2] = 1  \n",
        "    '''"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85-gyl4mqP0",
        "colab_type": "text"
      },
      "source": [
        "# Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "pGNExtgrmqP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed = USE_SEED #42\n",
        "seed_everything(seed)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQXvA_r9mqP4",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QY9vlBLVmqP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, max_len=MAX_LEN, use_fifth=True):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.labeled = 'selected_text' in df\n",
        "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file= ROOT_PATH + '/input/roberta-base/vocab.json', \n",
        "            merges_file= ROOT_PATH + '/input/roberta-base/merges.txt', \n",
        "            lowercase=True,\n",
        "            add_prefix_space=True)\n",
        "        self.use_fifth = use_fifth\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = {}\n",
        "        row = self.df.iloc[index]\n",
        "        \n",
        "        #print(f\"getting {index}\")\n",
        "        \n",
        "        ids, masks, tweet, offsets, enc_offsets, padding_len = self.get_input_data(row)\n",
        "        data['ids'] = ids\n",
        "        data['original_tweet'] = row.text\n",
        "        data['masks'] = masks\n",
        "        data['tweet'] = tweet\n",
        "        data['offsets'] = offsets\n",
        "        #data['select'] = row.selected_text\n",
        "        data['sentiment'] = row.sentiment\n",
        "        \n",
        "        if self.labeled:\n",
        "            try:\n",
        "                #start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "                if self.use_fifth:\n",
        "                    start_idx, end_idx = fifth_prp(row.selected_text, row.text, row.sentiment, offsets)\n",
        "                else:\n",
        "                    start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
        "            \n",
        "            except:\n",
        "                #print(f\"tweet:[{tweet}], selected: [{row.selected_text}] prp: [{prp(row.selected_text, tweet)}]\")\n",
        "                start_idx = 4\n",
        "                end_idx = len(ids) - 4 - 2\n",
        "                \n",
        "            if USE_SEQUENCE_BUCKETING:\n",
        "              data['start_idx'] = min(start_idx, self.max_len - 1 - padding_len) # start_idx\n",
        "              data['end_idx'] = min(end_idx, self.max_len - 1 - padding_len) # end_idx\n",
        "            else:\n",
        "              data['start_idx'] = start_idx\n",
        "              data['end_idx'] = end_idx\n",
        "        \n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def get_input_data(self, row):\n",
        "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "        encoding = self.tokenizer.encode(tweet)\n",
        "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
        "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
        "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "                \n",
        "        pad_len = self.max_len - len(ids)\n",
        "        if pad_len > 0:\n",
        "            ids += [PAD_ID] * pad_len\n",
        "            offsets += [(0, 0)] * pad_len\n",
        "        \n",
        "        ids = torch.tensor(ids)\n",
        "        masks = torch.where(ids != PAD_ID, torch.tensor(1), torch.tensor(0))\n",
        "        offsets = torch.tensor(offsets)\n",
        "        \n",
        "        return ids, masks, tweet, offsets, encoding.offsets, pad_len\n",
        "        \n",
        "    def get_target_idx(self, row, tweet, offsets):\n",
        "        \n",
        "        #pp:\n",
        "        \n",
        "        selected_text = prp(row.selected_text.lower(), tweet)\n",
        "        #selected_text = row.selected_text\n",
        "        \n",
        "        if len(selected_text) == 0:\n",
        "            selected_text = row.selected_text\n",
        "        \n",
        "        selected_text = \" \" +  \" \".join(selected_text.lower().split())\n",
        "\n",
        "        len_st = len(selected_text) - 1\n",
        "        idx0 = None\n",
        "        idx1 = None\n",
        "\n",
        "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "                idx0 = ind\n",
        "                idx1 = ind + len_st - 1\n",
        "                break\n",
        "\n",
        "        char_targets = [0] * len(tweet)\n",
        "        if idx0 != None and idx1 != None:\n",
        "            for ct in range(idx0, idx1 + 1):\n",
        "                char_targets[ct] = 1\n",
        "\n",
        "        target_idx = []\n",
        "        for j, (offset1, offset2) in enumerate(offsets):\n",
        "            if sum(char_targets[offset1: offset2]) > 0:\n",
        "                target_idx.append(j)\n",
        "\n",
        "        start_idx = target_idx[0]\n",
        "        end_idx = target_idx[-1]\n",
        "        \n",
        "        return start_idx, end_idx\n",
        "        \n",
        "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    val_df = df.iloc[val_idx]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(train_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=2,\n",
        "        drop_last=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(val_df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)\n",
        "\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "    return dataloaders_dict\n",
        "\n",
        "def get_test_loader(df, batch_size=32):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(df), \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2)    \n",
        "    return loader"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Vm0BqKmqP8",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bQIG2zlRmqP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TweetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TweetModel, self).__init__()\n",
        "        \n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            ROOT_PATH + '/input/roberta-base/config.json', output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            ROOT_PATH + '/input/roberta-base/pytorch_model.bin', config=config)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.high_dropout = nn.Dropout(USE_MULTI_SAMPLE_DROPOUT_RATE)\n",
        "        self.fc = nn.Linear(config.hidden_size, 2)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "\n",
        "        if USE_BERT_LAST_N_LAYERS == -1:\n",
        "          n_weights = config.num_hidden_layers\n",
        "        else:\n",
        "          n_weights = USE_BERT_LAST_N_LAYERS #config.num_hidden_layers + 1\n",
        "\n",
        "        self.n_layers = n_weights\n",
        "\n",
        "\n",
        "        weights_init = torch.zeros(n_weights).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "\n",
        "        self.multi_layer_dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        if USE_SEQUENCE_BUCKETING:\n",
        "          #padding = torch.eq(input_ids, PAD_ID).int()\n",
        "          padding = torch.where(input_ids == PAD_ID, torch.ones_like(input_ids), torch.zeros_like(input_ids))\n",
        "          lens = MAX_LEN - torch.sum(padding, -1)\n",
        "          #max_len = MAX_LEN -1  #torch.max(lens) - 1\n",
        "          max_len = torch.max(lens)# - 1\n",
        "\n",
        "          #max_len = torch.clamp(max_len, min=10, max=MAX_LEN)\n",
        "\n",
        "          #input_ids_last = torch.unsqueeze(input_ids[:, -1], 1)\n",
        "          #attention_mask_last = torch.unsqueeze(attention_mask[:, -1], 1)\n",
        "          input_ids = input_ids[:, :max_len]\n",
        "          attention_mask = attention_mask[:, :max_len]\n",
        "          #input_ids = torch.cat([input_ids, input_ids_last], dim=1)\n",
        "          #attention_mask = torch.cat([attention_mask, attention_mask_last], dim=1)\n",
        "          #tok_ = tok[:, :max_len]\n",
        "\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
        "         \n",
        "        if not USE_BERT_ALL_LAYERS:\n",
        "          x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
        "          x = torch.mean(x, 0)\n",
        "        else:\n",
        "\n",
        "          #print(f\"layers: { [hs[i].shape for i in range(-USE_BERT_LAST_N_LAYERS, 0, 1)]}\")\n",
        "\n",
        "\n",
        "\n",
        "          x = torch.stack(\n",
        "              [self.multi_layer_dropout(layer[:, :, :]) for layer in [hs[i] for i in range(-self.n_layers, 0, 1)]], dim=3\n",
        "          )\n",
        "          x = (torch.softmax(self.layer_weights, dim=0) * x).sum(-1)\n",
        "\n",
        "        if not USE_MULTI_SAMPLE_DROPOUT:\n",
        "          x = self.dropout(x)\n",
        "          x = self.fc(x)\n",
        "        else:\n",
        "          # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
        "          x = torch.mean(\n",
        "              torch.stack(\n",
        "                  [self.fc(self.high_dropout(x)) for _ in range(USE_MULTI_SAMPLE_DROPOUT_SAMPLE)],\n",
        "                  dim=0,\n",
        "              ),\n",
        "              dim=0,\n",
        "          )\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "                \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teaVSs9UmqQD",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "60TWdOqKmqQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# from: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
        "# This loss is similar to CrossEntropyLoss, i.e. it expects logits. Don't use a softmax as your last layer therefore.\n",
        "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
        "                    device=targets.device) \\\n",
        "                .fill_(smoothing /(n_classes-1)) \\\n",
        "                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n",
        "            self.smoothing)\n",
        "        lsm = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            lsm = lsm * self.weight.unsqueeze(0)\n",
        "\n",
        "        loss = -(targets * lsm).sum(-1)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    if not USE_SMOOTH_LABELING:\n",
        "      ce_loss = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "      ce_loss = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTH)\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)    \n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jXmN6ImqQH",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5mHp1NUWmqQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_selected_text(text, start_idx, end_idx, offsets):\n",
        "    selected_text = \"\"\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            selected_text += \" \"\n",
        "    #selected_text = pp(selected_text, text)\n",
        "    \n",
        "    return selected_text\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
        "    start_pred = np.argmax(start_logits)\n",
        "    end_pred = np.argmax(end_logits)\n",
        "    if start_pred > end_pred:\n",
        "        pred = text\n",
        "    else:\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
        "        \n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
        "    \n",
        "    return jaccard(true, pred)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3LRjNPdmqQM",
        "colab_type": "text"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DxZrmVbJmqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    jc_ret = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            epoch_jaccard = 0.0\n",
        "            \n",
        "            \n",
        "            \n",
        "            for data in (dataloaders_dict[phase]):\n",
        "                ids = data['ids'].cuda()\n",
        "                masks = data['masks'].cuda()\n",
        "                tweet = data['tweet']\n",
        "                offsets = data['offsets'].numpy()\n",
        "                start_idx = data['start_idx'].cuda()\n",
        "                end_idx = data['end_idx'].cuda()\n",
        "                \n",
        "                #print((start_idx,end_idx))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    start_logits, end_logits = model(ids, masks)\n",
        "\n",
        "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    epoch_loss += loss.item() * len(ids)\n",
        "                    \n",
        "                    start_idx = start_idx.cpu().detach().numpy()\n",
        "                    end_idx = end_idx.cpu().detach().numpy()\n",
        "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
        "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
        "                    \n",
        "                    for i in range(len(ids)):                        \n",
        "                        jaccard_score = compute_jaccard_score(\n",
        "                            tweet[i],\n",
        "                            start_idx[i],\n",
        "                            end_idx[i],\n",
        "                            start_logits[i], \n",
        "                            end_logits[i], \n",
        "                            offsets[i])\n",
        "                        epoch_jaccard += jaccard_score\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
        "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
        "            \n",
        "            if epoch == num_epochs - 1:\n",
        "              jc_ret = epoch_jaccard\n",
        "    \n",
        "    torch.save(model.state_dict(), filename)\n",
        "    return jc_ret"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbZ1-zxSmqQX",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hWp_K8EymqQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1s4mfoomqQl",
        "colab_type": "text"
      },
      "source": [
        "# **post process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "twfns9m1mqQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def modify_punc_length(text, selected_text):\n",
        "    m = re.search(r'[!\\.\\?]+$', selected_text)        \n",
        "    if m is None:\n",
        "        return selected_text\n",
        "    \n",
        "    conti_punc = len(m.group())\n",
        "\n",
        "    if conti_punc >= 4:\n",
        "        selected_text = selected_text[:-(conti_punc-2)]\n",
        "    elif conti_punc == 1:# 元のtextを探しに行く\n",
        "        tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", selected_text)\n",
        "        pat = re.sub(r\" \", \" +\", tmp)\n",
        "        m = re.search(pat, text)\n",
        "        f_idx0 = m.start()\n",
        "        f_idx1 = m.end()\n",
        "\n",
        "        if f_idx1 != len(text) and text[f_idx1] in (\"!\", \".\", \"?\"):\n",
        "            f_idx1 += 1\n",
        "            selected_text = text[f_idx0:f_idx1]\n",
        "    return selected_text\n",
        "\n",
        "\n",
        "import math\n",
        "def postprocess(tweet, offsets, aa, bb, sentiment):\n",
        "    text0 = tweet\n",
        "    text1 = \" \" + \" \".join(tweet.split())\n",
        "    #enc = tokenizer.encode(text1)\n",
        "\n",
        "    #aa = np.argmax(start_tokens[k])\n",
        "    #bb = np.argmax(end_tokens[k])\n",
        "\n",
        "    ss = offsets[aa][0]\n",
        "    ee = offsets[bb][1] \n",
        "    #st = text1[ss:ee].strip()\n",
        "    from collections import namedtuple\n",
        "    Row = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\n",
        "    row = Row(\n",
        "        original_text=text0,\n",
        "        normalized_text=text1,\n",
        "        sentiment=sentiment,\n",
        "        y_start_char=ss,\n",
        "        y_end_char=ee,\n",
        "    )\n",
        "    \n",
        "    if row.original_text == '':\n",
        "        return row.normalized_text.strip()\n",
        "    original_text = row.original_text.replace('\\t', '')\n",
        "    y_start_char = row.y_start_char\n",
        "    y_end_char = row.y_end_char\n",
        "    try:\n",
        "        y_selected_text = row.normalized_text[y_start_char:y_end_char].strip()\n",
        "    except:\n",
        "        print(f\"err: {row.normalized_text} {(y_start_char,y_end_char)}\")\n",
        "    if (y_end_char < len(row.normalized_text) and row.sentiment != 'neutral' and\n",
        "        y_selected_text[-1] == '.' and\n",
        "        (row.normalized_text[y_end_char] == '.' or \n",
        "         y_selected_text[-2] == '.')):\n",
        "        y_selected_text = re.sub('\\.+$', '..', y_selected_text)\n",
        "\n",
        "    tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", y_selected_text)\n",
        "    pat = re.sub(r\" \", \" +\", tmp)\n",
        "    m = re.search(pat, original_text)\n",
        "    if m is None:\n",
        "        print(row.normalized_text[y_start_char:y_end_char].strip())\n",
        "        print(row.normalized_text)\n",
        "        print(y_selected_text)\n",
        "    ss2 = m.start()\n",
        "    ee2 = m.end()\n",
        "    \n",
        "    # 'neutral' およびほぼ文書全体が抜き出されるもの\n",
        "    if row.sentiment == 'neutral' or ((ee2 - ss2) / len(original_text) > 0.75 and  (ee2 - ss2) > 9):\n",
        "        if len(original_text) > 0 and original_text[0] != '_' and ss2 < 5:\n",
        "            ss2 = 0 \n",
        "        if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "            ee2 += 1\n",
        "        st =  original_text[ss2:ee2].lstrip(' ½¿')\n",
        "        y_selected_text = st #re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "                \n",
        "    else:\n",
        "        if original_text[:int((ss2+ee2) * 0.5) + 1].count('  ') > 0:\n",
        "            ss = y_start_char\n",
        "            ee = y_end_char + 1\n",
        "            if ss > 1 and original_text[ss-1:ss+1] == '..' and  original_text[ss+1] != '.':\n",
        "                ss -= 1\n",
        "            st = original_text[ss:ee]#.lstrip(' ½¿')\n",
        "            y_selected_text = re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n",
        "        else:\n",
        "            if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n",
        "                ee2 += 1\n",
        "            # 先頭の空白分後退\n",
        "            if  original_text[0] == ' ':\n",
        "                ss2 -= 1\n",
        "\n",
        "            y_selected_text = original_text[ss2:ee2].strip(' ½')\n",
        "\n",
        "            if row.normalized_text[:y_end_char + 5] == \" \" + row.original_text[:ee2 + 4]: # 簡単のため、長さが同じ場合に限定している\n",
        "                y_selected_text = modify_punc_length(original_text, y_selected_text)\n",
        "            \n",
        "            \n",
        "    return y_selected_text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P54ebs-lq5Z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "354f9bc1-804c-4afe-e637-c78df04c7584"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 501200538 Aug 17 18:52 '/content/drive/My Drive/kaggle/input/roberta-base/pytorch_model.bin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j8BSIJWumqQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fea794c5-a1c7-4ee5-ef18-075f791d8883"
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "if DEBUG_LOAD:\n",
        "    train_df = train_df.head(n=1000)\n",
        "train_df['text'] = train_df['text'].astype(str)\n",
        "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
        "\n",
        "seed_jaccard = pd.read_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)\n",
        "seeds = seed_jaccard['seed'].tolist()\n",
        "kfoldseeds = seed_jaccard['kfoldseed'].tolist()\n",
        "CVs = seed_jaccard['CV'].tolist()\n",
        "\n",
        "MSDs = seed_jaccard['MSD'].tolist()\n",
        "\n",
        "SBs = seed_jaccard['SB'].tolist()\n",
        "\n",
        "MSDrates = seed_jaccard['MSDrate'].tolist()\n",
        "MSDsamples = seed_jaccard['MSDsample'].tolist()\n",
        "MLs = seed_jaccard['ML'].tolist()\n",
        "MLns = seed_jaccard['MLn'].tolist()\n",
        "MLs = seed_jaccard['ML'].tolist()\n",
        "MLRs = seed_jaccard['MLR'].tolist()\n",
        "MLRtimess = seed_jaccard['MLRtimes'].tolist()\n",
        "SLs = seed_jaccard['SL'].tolist()\n",
        "SLfactors = seed_jaccard['SLfactor'].tolist()\n",
        "\n",
        "seeding_id = 0\n",
        "\n",
        "if not USE_TRY_MULTI_SEEDING:\n",
        "  skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "while(True):\n",
        "  jc_sum = []\n",
        "  seed = int(time.time())\n",
        "\n",
        "  seeding_id += 1\n",
        "\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    print(f\"-----seeding {seeding_id}, seed:{seed}\")\n",
        "    seed_everything(seed)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
        "      print(f'Fold: {fold}')\n",
        "\n",
        "      if TRAIN_WITH_PSUEDO_LABELS:\n",
        "            pl_df= pd.read_csv(\"psuedo_labels_fold_%d.csv\" % (fold - 1))\n",
        "            if DEBUG_LOAD:\n",
        "                pl_df = pl_df.head(n=1000)\n",
        "            pl_df['text'] = pl_df['text'].astype(str)\n",
        "            pl_df['selected_text'] = pl_df['selected_text'].astype(str)\n",
        "            #del pl_df['score']\n",
        "            train_df = train_df.append(pl_df)\n",
        "\n",
        "      \n",
        "      if DEBUG_FOLD:\n",
        "          if fold != 1:\n",
        "              print(f\"DEBUG skip fold:{fold}\")\n",
        "              continue\n",
        "\n",
        "      model = TweetModel()\n",
        "\n",
        "      prefix = \"roberta\"\n",
        "      def is_backbone(n):\n",
        "          return prefix in n\n",
        "\n",
        "      lr=3e-5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if USE_MULTIPLE_LEARNING_RATE:\n",
        "        params = list(model.named_parameters())\n",
        "\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lr},\n",
        "            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\": lr * USE_MULTIPLE_LEARNING_RATE_TIMES},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "          optimizer_grouped_parameters, lr=lr, betas=(0.9, 0.999)\n",
        "        )\n",
        "      else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "      criterion = loss_fn    \n",
        "      dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
        "\n",
        "      jc = train_model(\n",
        "          model, \n",
        "          dataloaders_dict,\n",
        "          criterion, \n",
        "          optimizer, \n",
        "          num_epochs,\n",
        "          f'roberta_fold{fold}.pth')\n",
        "      \n",
        "      jc_sum.append(jc)\n",
        "\n",
        "  print(\"average jaccard: {:.4f}\".format(sum(jc_sum)/len(jc_sum)))\n",
        "  if USE_TRY_MULTI_SEEDING:\n",
        "    seeds.append(seed)\n",
        "    kfoldseeds.append(seed)\n",
        "    CVs.append(sum(jc_sum)/len(jc_sum))\n",
        "\n",
        "    MSDs.append(USE_MULTI_SAMPLE_DROPOUT)\n",
        "    MSDrates.append(USE_MULTI_SAMPLE_DROPOUT_RATE)\n",
        "    MSDsamples.append(USE_MULTI_SAMPLE_DROPOUT_SAMPLE)\n",
        "    MLs.append(USE_BERT_ALL_LAYERS)\n",
        "    MLns.append(USE_BERT_LAST_N_LAYERS)\n",
        "    MLRs.append(USE_MULTIPLE_LEARNING_RATE)\n",
        "    MLRtimess.append(USE_MULTIPLE_LEARNING_RATE_TIMES)\n",
        "    SLs.append(USE_SMOOTH_LABELING)\n",
        "    SLfactors.append(LABEL_SMOOTH)\n",
        "    SBs.append(USE_SEQUENCE_BUCKETING)\n",
        "\n",
        "\n",
        "    seed_jaccard = pd.DataFrame({'seed':seeds,\n",
        "                                 'CV': CVs, \n",
        "                                 'kfoldseed':kfoldseeds,\n",
        "                                 'MSD':MSDs,\n",
        "                                 'MSDrate':MSDrates,\n",
        "                                 'MSDsample':MSDsamples,\n",
        "                                 'ML':MLs,\n",
        "                                 'MLn':MLns,\n",
        "                                 'MLR':MLRs,\n",
        "                                 'MLRtimes':MLRtimess,\n",
        "                                 'SL':SLs,\n",
        "                                 'SLfactor':SLfactors,\n",
        "                                 'SB':SBs\n",
        "                                 })\n",
        "\n",
        "    seed_jaccard.to_csv(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "    seed_jaccard.to_csv('seeds_cv_%d.csv' % NOTEBOOK_ID, index=False)\n",
        "\n",
        "    print(ROOT_PATH +  '/input/seeds_finding/seeds_cv_%d.csv saved' % NOTEBOOK_ID)\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "      print(seed_jaccard)\n",
        "\n",
        "  if not USE_TRY_MULTI_SEEDING:\n",
        "    break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----seeding 1, seed:1598112722\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0513 | Jaccard: 0.6721\n",
            "Epoch 1/3 |  val  | Loss: 2.7321 | Jaccard: 0.7137\n",
            "Epoch 2/3 | train | Loss: 2.6447 | Jaccard: 0.7292\n",
            "Epoch 2/3 |  val  | Loss: 2.7026 | Jaccard: 0.7172\n",
            "Epoch 3/3 | train | Loss: 2.5295 | Jaccard: 0.7497\n",
            "Epoch 3/3 |  val  | Loss: 2.7285 | Jaccard: 0.7142\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1178 | Jaccard: 0.6651\n",
            "Epoch 1/3 |  val  | Loss: 2.6856 | Jaccard: 0.7203\n",
            "Epoch 2/3 | train | Loss: 2.6665 | Jaccard: 0.7264\n",
            "Epoch 2/3 |  val  | Loss: 2.6574 | Jaccard: 0.7322\n",
            "Epoch 3/3 | train | Loss: 2.5545 | Jaccard: 0.7436\n",
            "Epoch 3/3 |  val  | Loss: 2.6467 | Jaccard: 0.7286\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0934 | Jaccard: 0.6678\n",
            "Epoch 1/3 |  val  | Loss: 2.6906 | Jaccard: 0.7253\n",
            "Epoch 2/3 | train | Loss: 2.6687 | Jaccard: 0.7269\n",
            "Epoch 2/3 |  val  | Loss: 2.6687 | Jaccard: 0.7263\n",
            "Epoch 3/3 | train | Loss: 2.5580 | Jaccard: 0.7428\n",
            "Epoch 3/3 |  val  | Loss: 2.6499 | Jaccard: 0.7264\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.0788 | Jaccard: 0.6709\n",
            "Epoch 1/3 |  val  | Loss: 2.7082 | Jaccard: 0.7256\n",
            "Epoch 2/3 | train | Loss: 2.6526 | Jaccard: 0.7278\n",
            "Epoch 2/3 |  val  | Loss: 2.6675 | Jaccard: 0.7257\n",
            "Epoch 3/3 | train | Loss: 2.5349 | Jaccard: 0.7467\n",
            "Epoch 3/3 |  val  | Loss: 2.6919 | Jaccard: 0.7261\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0905 | Jaccard: 0.6695\n",
            "Epoch 1/3 |  val  | Loss: 2.6983 | Jaccard: 0.7309\n",
            "Epoch 2/3 | train | Loss: 2.6890 | Jaccard: 0.7217\n",
            "Epoch 2/3 |  val  | Loss: 2.6490 | Jaccard: 0.7297\n",
            "Epoch 3/3 | train | Loss: 2.5656 | Jaccard: 0.7402\n",
            "Epoch 3/3 |  val  | Loss: 2.7060 | Jaccard: 0.7303\n",
            "average jaccard: 0.7251\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "-----seeding 2, seed:1598114730\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1741 | Jaccard: 0.6538\n",
            "Epoch 1/3 |  val  | Loss: 2.7248 | Jaccard: 0.7142\n",
            "Epoch 2/3 | train | Loss: 2.6820 | Jaccard: 0.7229\n",
            "Epoch 2/3 |  val  | Loss: 2.7059 | Jaccard: 0.7146\n",
            "Epoch 3/3 | train | Loss: 2.5723 | Jaccard: 0.7450\n",
            "Epoch 3/3 |  val  | Loss: 2.7064 | Jaccard: 0.7279\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0939 | Jaccard: 0.6702\n",
            "Epoch 1/3 |  val  | Loss: 2.6921 | Jaccard: 0.7200\n",
            "Epoch 2/3 | train | Loss: 2.6882 | Jaccard: 0.7239\n",
            "Epoch 2/3 |  val  | Loss: 2.6628 | Jaccard: 0.7311\n",
            "Epoch 3/3 | train | Loss: 2.5894 | Jaccard: 0.7395\n",
            "Epoch 3/3 |  val  | Loss: 2.7336 | Jaccard: 0.7155\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0535 | Jaccard: 0.6699\n",
            "Epoch 1/3 |  val  | Loss: 2.7481 | Jaccard: 0.7185\n",
            "Epoch 2/3 | train | Loss: 2.6381 | Jaccard: 0.7286\n",
            "Epoch 2/3 |  val  | Loss: 2.7157 | Jaccard: 0.7203\n",
            "Epoch 3/3 | train | Loss: 2.5076 | Jaccard: 0.7530\n",
            "Epoch 3/3 |  val  | Loss: 2.7643 | Jaccard: 0.7216\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.2086 | Jaccard: 0.6520\n",
            "Epoch 1/3 |  val  | Loss: 2.7469 | Jaccard: 0.7107\n",
            "Epoch 2/3 | train | Loss: 2.7125 | Jaccard: 0.7195\n",
            "Epoch 2/3 |  val  | Loss: 2.6586 | Jaccard: 0.7245\n",
            "Epoch 3/3 | train | Loss: 2.6025 | Jaccard: 0.7352\n",
            "Epoch 3/3 |  val  | Loss: 2.6923 | Jaccard: 0.7291\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0487 | Jaccard: 0.6733\n",
            "Epoch 1/3 |  val  | Loss: 2.6890 | Jaccard: 0.7163\n",
            "Epoch 2/3 | train | Loss: 2.6502 | Jaccard: 0.7290\n",
            "Epoch 2/3 |  val  | Loss: 2.6800 | Jaccard: 0.7215\n",
            "Epoch 3/3 | train | Loss: 2.5310 | Jaccard: 0.7477\n",
            "Epoch 3/3 |  val  | Loss: 2.6493 | Jaccard: 0.7204\n",
            "average jaccard: 0.7229\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "-----seeding 3, seed:1598116705\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1595 | Jaccard: 0.6547\n",
            "Epoch 1/3 |  val  | Loss: 2.7455 | Jaccard: 0.7177\n",
            "Epoch 2/3 | train | Loss: 2.7041 | Jaccard: 0.7191\n",
            "Epoch 2/3 |  val  | Loss: 2.6821 | Jaccard: 0.7264\n",
            "Epoch 3/3 | train | Loss: 2.5862 | Jaccard: 0.7363\n",
            "Epoch 3/3 |  val  | Loss: 2.6779 | Jaccard: 0.7238\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0861 | Jaccard: 0.6656\n",
            "Epoch 1/3 |  val  | Loss: 2.6758 | Jaccard: 0.7312\n",
            "Epoch 2/3 | train | Loss: 2.6565 | Jaccard: 0.7263\n",
            "Epoch 2/3 |  val  | Loss: 2.6653 | Jaccard: 0.7305\n",
            "Epoch 3/3 | train | Loss: 2.5381 | Jaccard: 0.7464\n",
            "Epoch 3/3 |  val  | Loss: 2.6379 | Jaccard: 0.7315\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0869 | Jaccard: 0.6671\n",
            "Epoch 1/3 |  val  | Loss: 2.6876 | Jaccard: 0.7244\n",
            "Epoch 2/3 | train | Loss: 2.6689 | Jaccard: 0.7245\n",
            "Epoch 2/3 |  val  | Loss: 2.6608 | Jaccard: 0.7250\n",
            "Epoch 3/3 | train | Loss: 2.5511 | Jaccard: 0.7444\n",
            "Epoch 3/3 |  val  | Loss: 2.6664 | Jaccard: 0.7292\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1011 | Jaccard: 0.6651\n",
            "Epoch 1/3 |  val  | Loss: 2.7166 | Jaccard: 0.7197\n",
            "Epoch 2/3 | train | Loss: 2.6551 | Jaccard: 0.7287\n",
            "Epoch 2/3 |  val  | Loss: 2.7114 | Jaccard: 0.7221\n",
            "Epoch 3/3 | train | Loss: 2.5435 | Jaccard: 0.7483\n",
            "Epoch 3/3 |  val  | Loss: 2.7466 | Jaccard: 0.7160\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1437 | Jaccard: 0.6592\n",
            "Epoch 1/3 |  val  | Loss: 2.7696 | Jaccard: 0.7058\n",
            "Epoch 2/3 | train | Loss: 2.6782 | Jaccard: 0.7237\n",
            "Epoch 2/3 |  val  | Loss: 2.6981 | Jaccard: 0.7274\n",
            "Epoch 3/3 | train | Loss: 2.5514 | Jaccard: 0.7448\n",
            "Epoch 3/3 |  val  | Loss: 2.7292 | Jaccard: 0.7260\n",
            "average jaccard: 0.7253\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "-----seeding 4, seed:1598118700\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0454 | Jaccard: 0.6753\n",
            "Epoch 1/3 |  val  | Loss: 2.6942 | Jaccard: 0.7158\n",
            "Epoch 2/3 | train | Loss: 2.6483 | Jaccard: 0.7257\n",
            "Epoch 2/3 |  val  | Loss: 2.6498 | Jaccard: 0.7265\n",
            "Epoch 3/3 | train | Loss: 2.5196 | Jaccard: 0.7518\n",
            "Epoch 3/3 |  val  | Loss: 2.6583 | Jaccard: 0.7281\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0427 | Jaccard: 0.6734\n",
            "Epoch 1/3 |  val  | Loss: 2.6865 | Jaccard: 0.7249\n",
            "Epoch 2/3 | train | Loss: 2.6456 | Jaccard: 0.7294\n",
            "Epoch 2/3 |  val  | Loss: 2.6533 | Jaccard: 0.7297\n",
            "Epoch 3/3 | train | Loss: 2.5454 | Jaccard: 0.7494\n",
            "Epoch 3/3 |  val  | Loss: 2.6805 | Jaccard: 0.7330\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1008 | Jaccard: 0.6653\n",
            "Epoch 1/3 |  val  | Loss: 2.6964 | Jaccard: 0.7282\n",
            "Epoch 2/3 | train | Loss: 2.6831 | Jaccard: 0.7246\n",
            "Epoch 2/3 |  val  | Loss: 2.7010 | Jaccard: 0.7264\n",
            "Epoch 3/3 | train | Loss: 2.5497 | Jaccard: 0.7436\n",
            "Epoch 3/3 |  val  | Loss: 2.6927 | Jaccard: 0.7228\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.0817 | Jaccard: 0.6720\n",
            "Epoch 1/3 |  val  | Loss: 2.7059 | Jaccard: 0.7266\n",
            "Epoch 2/3 | train | Loss: 2.6609 | Jaccard: 0.7278\n",
            "Epoch 2/3 |  val  | Loss: 2.6994 | Jaccard: 0.7226\n",
            "Epoch 3/3 | train | Loss: 2.5595 | Jaccard: 0.7475\n",
            "Epoch 3/3 |  val  | Loss: 2.7098 | Jaccard: 0.7281\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0964 | Jaccard: 0.6700\n",
            "Epoch 1/3 |  val  | Loss: 2.7847 | Jaccard: 0.7020\n",
            "Epoch 2/3 | train | Loss: 2.6837 | Jaccard: 0.7216\n",
            "Epoch 2/3 |  val  | Loss: 2.7430 | Jaccard: 0.7147\n",
            "Epoch 3/3 | train | Loss: 2.5537 | Jaccard: 0.7433\n",
            "Epoch 3/3 |  val  | Loss: 2.7731 | Jaccard: 0.7094\n",
            "average jaccard: 0.7243\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "-----seeding 5, seed:1598120701\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1792 | Jaccard: 0.6512\n",
            "Epoch 1/3 |  val  | Loss: 2.7488 | Jaccard: 0.7180\n",
            "Epoch 2/3 | train | Loss: 2.6856 | Jaccard: 0.7257\n",
            "Epoch 2/3 |  val  | Loss: 2.6739 | Jaccard: 0.7175\n",
            "Epoch 3/3 | train | Loss: 2.5786 | Jaccard: 0.7419\n",
            "Epoch 3/3 |  val  | Loss: 2.6708 | Jaccard: 0.7208\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1874 | Jaccard: 0.6621\n",
            "Epoch 1/3 |  val  | Loss: 2.7949 | Jaccard: 0.6992\n",
            "Epoch 2/3 | train | Loss: 2.7211 | Jaccard: 0.7167\n",
            "Epoch 2/3 |  val  | Loss: 2.6510 | Jaccard: 0.7257\n",
            "Epoch 3/3 | train | Loss: 2.5698 | Jaccard: 0.7402\n",
            "Epoch 3/3 |  val  | Loss: 2.6824 | Jaccard: 0.7299\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1165 | Jaccard: 0.6717\n",
            "Epoch 1/3 |  val  | Loss: 2.6928 | Jaccard: 0.7132\n",
            "Epoch 2/3 | train | Loss: 2.6428 | Jaccard: 0.7266\n",
            "Epoch 2/3 |  val  | Loss: 2.7063 | Jaccard: 0.7246\n",
            "Epoch 3/3 | train | Loss: 2.5275 | Jaccard: 0.7495\n",
            "Epoch 3/3 |  val  | Loss: 2.6807 | Jaccard: 0.7242\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1199 | Jaccard: 0.6608\n",
            "Epoch 1/3 |  val  | Loss: 2.7123 | Jaccard: 0.7126\n",
            "Epoch 2/3 | train | Loss: 2.6747 | Jaccard: 0.7257\n",
            "Epoch 2/3 |  val  | Loss: 2.6966 | Jaccard: 0.7158\n",
            "Epoch 3/3 | train | Loss: 2.5602 | Jaccard: 0.7421\n",
            "Epoch 3/3 |  val  | Loss: 2.6936 | Jaccard: 0.7291\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1153 | Jaccard: 0.6615\n",
            "Epoch 1/3 |  val  | Loss: 2.6852 | Jaccard: 0.7268\n",
            "Epoch 2/3 | train | Loss: 2.6662 | Jaccard: 0.7252\n",
            "Epoch 2/3 |  val  | Loss: 2.6713 | Jaccard: 0.7234\n",
            "Epoch 3/3 | train | Loss: 2.8322 | Jaccard: 0.7118\n",
            "Epoch 3/3 |  val  | Loss: 2.6874 | Jaccard: 0.7214\n",
            "average jaccard: 0.7251\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "-----seeding 6, seed:1598122712\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0722 | Jaccard: 0.6716\n",
            "Epoch 1/3 |  val  | Loss: 2.7012 | Jaccard: 0.7204\n",
            "Epoch 2/3 | train | Loss: 2.6446 | Jaccard: 0.7275\n",
            "Epoch 2/3 |  val  | Loss: 2.6587 | Jaccard: 0.7309\n",
            "Epoch 3/3 | train | Loss: 2.5371 | Jaccard: 0.7489\n",
            "Epoch 3/3 |  val  | Loss: 2.6908 | Jaccard: 0.7329\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0734 | Jaccard: 0.6724\n",
            "Epoch 1/3 |  val  | Loss: 2.7263 | Jaccard: 0.7112\n",
            "Epoch 2/3 | train | Loss: 2.6735 | Jaccard: 0.7273\n",
            "Epoch 2/3 |  val  | Loss: 2.6973 | Jaccard: 0.7204\n",
            "Epoch 3/3 | train | Loss: 2.6028 | Jaccard: 0.7353\n",
            "Epoch 3/3 |  val  | Loss: 2.7365 | Jaccard: 0.7161\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0635 | Jaccard: 0.6736\n",
            "Epoch 1/3 |  val  | Loss: 2.7064 | Jaccard: 0.7228\n",
            "Epoch 2/3 | train | Loss: 2.6457 | Jaccard: 0.7294\n",
            "Epoch 2/3 |  val  | Loss: 2.7083 | Jaccard: 0.7229\n",
            "Epoch 3/3 | train | Loss: 2.5253 | Jaccard: 0.7477\n",
            "Epoch 3/3 |  val  | Loss: 2.7056 | Jaccard: 0.7120\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1090 | Jaccard: 0.6650\n",
            "Epoch 1/3 |  val  | Loss: 2.6987 | Jaccard: 0.7189\n",
            "Epoch 2/3 | train | Loss: 2.6656 | Jaccard: 0.7238\n",
            "Epoch 2/3 |  val  | Loss: 2.6535 | Jaccard: 0.7269\n",
            "Epoch 3/3 | train | Loss: 2.5419 | Jaccard: 0.7469\n",
            "Epoch 3/3 |  val  | Loss: 2.6947 | Jaccard: 0.7291\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0966 | Jaccard: 0.6694\n",
            "Epoch 1/3 |  val  | Loss: 2.7060 | Jaccard: 0.7134\n",
            "Epoch 2/3 | train | Loss: 2.6641 | Jaccard: 0.7273\n",
            "Epoch 2/3 |  val  | Loss: 2.6734 | Jaccard: 0.7243\n",
            "Epoch 3/3 | train | Loss: 2.5622 | Jaccard: 0.7443\n",
            "Epoch 3/3 |  val  | Loss: 2.6780 | Jaccard: 0.7215\n",
            "average jaccard: 0.7223\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "-----seeding 7, seed:1598124700\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0907 | Jaccard: 0.6708\n",
            "Epoch 1/3 |  val  | Loss: 2.6964 | Jaccard: 0.7173\n",
            "Epoch 2/3 | train | Loss: 2.6643 | Jaccard: 0.7247\n",
            "Epoch 2/3 |  val  | Loss: 2.6956 | Jaccard: 0.7198\n",
            "Epoch 3/3 | train | Loss: 2.5465 | Jaccard: 0.7457\n",
            "Epoch 3/3 |  val  | Loss: 2.6961 | Jaccard: 0.7177\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1671 | Jaccard: 0.6575\n",
            "Epoch 1/3 |  val  | Loss: 2.7417 | Jaccard: 0.7138\n",
            "Epoch 2/3 | train | Loss: 2.6720 | Jaccard: 0.7253\n",
            "Epoch 2/3 |  val  | Loss: 2.6881 | Jaccard: 0.7270\n",
            "Epoch 3/3 | train | Loss: 2.5491 | Jaccard: 0.7470\n",
            "Epoch 3/3 |  val  | Loss: 2.7267 | Jaccard: 0.7171\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1044 | Jaccard: 0.6669\n",
            "Epoch 1/3 |  val  | Loss: 2.8596 | Jaccard: 0.6994\n",
            "Epoch 2/3 | train | Loss: 2.7299 | Jaccard: 0.7115\n",
            "Epoch 2/3 |  val  | Loss: 2.7219 | Jaccard: 0.7092\n",
            "Epoch 3/3 | train | Loss: 2.5945 | Jaccard: 0.7356\n",
            "Epoch 3/3 |  val  | Loss: 2.7717 | Jaccard: 0.7126\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1967 | Jaccard: 0.6528\n",
            "Epoch 1/3 |  val  | Loss: 2.7635 | Jaccard: 0.6966\n",
            "Epoch 2/3 | train | Loss: 2.7173 | Jaccard: 0.7182\n",
            "Epoch 2/3 |  val  | Loss: 2.6938 | Jaccard: 0.7220\n",
            "Epoch 3/3 | train | Loss: 2.6141 | Jaccard: 0.7357\n",
            "Epoch 3/3 |  val  | Loss: 2.7732 | Jaccard: 0.7148\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1388 | Jaccard: 0.6612\n",
            "Epoch 1/3 |  val  | Loss: 2.7150 | Jaccard: 0.7207\n",
            "Epoch 2/3 | train | Loss: 2.8028 | Jaccard: 0.7059\n",
            "Epoch 2/3 |  val  | Loss: 2.7465 | Jaccard: 0.7167\n",
            "Epoch 3/3 | train | Loss: 2.6823 | Jaccard: 0.7254\n",
            "Epoch 3/3 |  val  | Loss: 2.6632 | Jaccard: 0.7319\n",
            "average jaccard: 0.7188\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "-----seeding 8, seed:1598126686\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1056 | Jaccard: 0.6634\n",
            "Epoch 1/3 |  val  | Loss: 2.7192 | Jaccard: 0.7252\n",
            "Epoch 2/3 | train | Loss: 2.6621 | Jaccard: 0.7260\n",
            "Epoch 2/3 |  val  | Loss: 2.6863 | Jaccard: 0.7251\n",
            "Epoch 3/3 | train | Loss: 2.5599 | Jaccard: 0.7435\n",
            "Epoch 3/3 |  val  | Loss: 2.6699 | Jaccard: 0.7292\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0988 | Jaccard: 0.6677\n",
            "Epoch 1/3 |  val  | Loss: 2.8328 | Jaccard: 0.7138\n",
            "Epoch 2/3 | train | Loss: 2.6543 | Jaccard: 0.7278\n",
            "Epoch 2/3 |  val  | Loss: 2.6708 | Jaccard: 0.7223\n",
            "Epoch 3/3 | train | Loss: 2.5375 | Jaccard: 0.7479\n",
            "Epoch 3/3 |  val  | Loss: 2.6861 | Jaccard: 0.7193\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0902 | Jaccard: 0.6715\n",
            "Epoch 1/3 |  val  | Loss: 2.6955 | Jaccard: 0.7216\n",
            "Epoch 2/3 | train | Loss: 2.6596 | Jaccard: 0.7227\n",
            "Epoch 2/3 |  val  | Loss: 2.6970 | Jaccard: 0.7302\n",
            "Epoch 3/3 | train | Loss: 2.5385 | Jaccard: 0.7470\n",
            "Epoch 3/3 |  val  | Loss: 2.6973 | Jaccard: 0.7237\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.0968 | Jaccard: 0.6720\n",
            "Epoch 1/3 |  val  | Loss: 2.7215 | Jaccard: 0.7135\n",
            "Epoch 2/3 | train | Loss: 2.6565 | Jaccard: 0.7295\n",
            "Epoch 2/3 |  val  | Loss: 2.7087 | Jaccard: 0.7265\n",
            "Epoch 3/3 | train | Loss: 2.6225 | Jaccard: 0.7337\n",
            "Epoch 3/3 |  val  | Loss: 2.6871 | Jaccard: 0.7193\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0616 | Jaccard: 0.6704\n",
            "Epoch 1/3 |  val  | Loss: 2.6958 | Jaccard: 0.7242\n",
            "Epoch 2/3 | train | Loss: 2.6571 | Jaccard: 0.7256\n",
            "Epoch 2/3 |  val  | Loss: 2.6749 | Jaccard: 0.7184\n",
            "Epoch 3/3 | train | Loss: 2.5345 | Jaccard: 0.7484\n",
            "Epoch 3/3 |  val  | Loss: 2.7030 | Jaccard: 0.7299\n",
            "average jaccard: 0.7243\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "-----seeding 9, seed:1598128679\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0974 | Jaccard: 0.6657\n",
            "Epoch 1/3 |  val  | Loss: 2.7550 | Jaccard: 0.7115\n",
            "Epoch 2/3 | train | Loss: 2.6919 | Jaccard: 0.7212\n",
            "Epoch 2/3 |  val  | Loss: 2.7032 | Jaccard: 0.7238\n",
            "Epoch 3/3 | train | Loss: 2.5599 | Jaccard: 0.7450\n",
            "Epoch 3/3 |  val  | Loss: 2.6993 | Jaccard: 0.7220\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1256 | Jaccard: 0.6640\n",
            "Epoch 1/3 |  val  | Loss: 2.7254 | Jaccard: 0.7208\n",
            "Epoch 2/3 | train | Loss: 2.6728 | Jaccard: 0.7217\n",
            "Epoch 2/3 |  val  | Loss: 2.6580 | Jaccard: 0.7235\n",
            "Epoch 3/3 | train | Loss: 2.5557 | Jaccard: 0.7452\n",
            "Epoch 3/3 |  val  | Loss: 2.7047 | Jaccard: 0.7211\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1774 | Jaccard: 0.6523\n",
            "Epoch 1/3 |  val  | Loss: 2.7140 | Jaccard: 0.7175\n",
            "Epoch 2/3 | train | Loss: 2.6973 | Jaccard: 0.7205\n",
            "Epoch 2/3 |  val  | Loss: 2.6700 | Jaccard: 0.7337\n",
            "Epoch 3/3 | train | Loss: 2.5576 | Jaccard: 0.7424\n",
            "Epoch 3/3 |  val  | Loss: 2.6684 | Jaccard: 0.7264\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1165 | Jaccard: 0.6630\n",
            "Epoch 1/3 |  val  | Loss: 2.7158 | Jaccard: 0.7201\n",
            "Epoch 2/3 | train | Loss: 2.6698 | Jaccard: 0.7256\n",
            "Epoch 2/3 |  val  | Loss: 2.7049 | Jaccard: 0.7181\n",
            "Epoch 3/3 | train | Loss: 2.5564 | Jaccard: 0.7428\n",
            "Epoch 3/3 |  val  | Loss: 2.7082 | Jaccard: 0.7229\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0926 | Jaccard: 0.6662\n",
            "Epoch 1/3 |  val  | Loss: 2.7207 | Jaccard: 0.7252\n",
            "Epoch 2/3 | train | Loss: 2.6448 | Jaccard: 0.7266\n",
            "Epoch 2/3 |  val  | Loss: 2.6793 | Jaccard: 0.7322\n",
            "Epoch 3/3 | train | Loss: 2.5201 | Jaccard: 0.7501\n",
            "Epoch 3/3 |  val  | Loss: 2.7136 | Jaccard: 0.7313\n",
            "average jaccard: 0.7247\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "-----seeding 10, seed:1598130645\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0941 | Jaccard: 0.6697\n",
            "Epoch 1/3 |  val  | Loss: 2.6913 | Jaccard: 0.7125\n",
            "Epoch 2/3 | train | Loss: 2.6669 | Jaccard: 0.7257\n",
            "Epoch 2/3 |  val  | Loss: 2.6492 | Jaccard: 0.7237\n",
            "Epoch 3/3 | train | Loss: 2.5461 | Jaccard: 0.7462\n",
            "Epoch 3/3 |  val  | Loss: 2.6907 | Jaccard: 0.7282\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1235 | Jaccard: 0.6652\n",
            "Epoch 1/3 |  val  | Loss: 2.7362 | Jaccard: 0.7065\n",
            "Epoch 2/3 | train | Loss: 2.6572 | Jaccard: 0.7261\n",
            "Epoch 2/3 |  val  | Loss: 2.6938 | Jaccard: 0.7180\n",
            "Epoch 3/3 | train | Loss: 2.5496 | Jaccard: 0.7463\n",
            "Epoch 3/3 |  val  | Loss: 2.7217 | Jaccard: 0.7165\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0791 | Jaccard: 0.6739\n",
            "Epoch 1/3 |  val  | Loss: 2.6623 | Jaccard: 0.7235\n",
            "Epoch 2/3 | train | Loss: 2.6611 | Jaccard: 0.7258\n",
            "Epoch 2/3 |  val  | Loss: 2.6614 | Jaccard: 0.7309\n",
            "Epoch 3/3 | train | Loss: 2.5519 | Jaccard: 0.7467\n",
            "Epoch 3/3 |  val  | Loss: 2.6508 | Jaccard: 0.7250\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1491 | Jaccard: 0.6617\n",
            "Epoch 1/3 |  val  | Loss: 2.7946 | Jaccard: 0.7044\n",
            "Epoch 2/3 | train | Loss: 2.6827 | Jaccard: 0.7226\n",
            "Epoch 2/3 |  val  | Loss: 2.7195 | Jaccard: 0.7186\n",
            "Epoch 3/3 | train | Loss: 2.5641 | Jaccard: 0.7412\n",
            "Epoch 3/3 |  val  | Loss: 2.7188 | Jaccard: 0.7245\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1210 | Jaccard: 0.6613\n",
            "Epoch 1/3 |  val  | Loss: 2.6878 | Jaccard: 0.7243\n",
            "Epoch 2/3 | train | Loss: 2.6589 | Jaccard: 0.7258\n",
            "Epoch 2/3 |  val  | Loss: 2.6854 | Jaccard: 0.7262\n",
            "Epoch 3/3 | train | Loss: 2.6086 | Jaccard: 0.7383\n",
            "Epoch 3/3 |  val  | Loss: 2.6888 | Jaccard: 0.7272\n",
            "average jaccard: 0.7243\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "67  1598130645  0.724265  1598130645   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "67   True         3   True       0.1   True  \n",
            "-----seeding 11, seed:1598132616\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0865 | Jaccard: 0.6663\n",
            "Epoch 1/3 |  val  | Loss: 2.7608 | Jaccard: 0.7259\n",
            "Epoch 2/3 | train | Loss: 2.6632 | Jaccard: 0.7238\n",
            "Epoch 2/3 |  val  | Loss: 2.6758 | Jaccard: 0.7307\n",
            "Epoch 3/3 | train | Loss: 2.5477 | Jaccard: 0.7431\n",
            "Epoch 3/3 |  val  | Loss: 2.6814 | Jaccard: 0.7279\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0613 | Jaccard: 0.6738\n",
            "Epoch 1/3 |  val  | Loss: 2.9554 | Jaccard: 0.6985\n",
            "Epoch 2/3 | train | Loss: 2.6709 | Jaccard: 0.7240\n",
            "Epoch 2/3 |  val  | Loss: 2.7330 | Jaccard: 0.7159\n",
            "Epoch 3/3 | train | Loss: 2.5468 | Jaccard: 0.7435\n",
            "Epoch 3/3 |  val  | Loss: 2.7250 | Jaccard: 0.7264\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1006 | Jaccard: 0.6668\n",
            "Epoch 1/3 |  val  | Loss: 2.7598 | Jaccard: 0.7166\n",
            "Epoch 2/3 | train | Loss: 2.6615 | Jaccard: 0.7259\n",
            "Epoch 2/3 |  val  | Loss: 2.6643 | Jaccard: 0.7300\n",
            "Epoch 3/3 | train | Loss: 2.5462 | Jaccard: 0.7489\n",
            "Epoch 3/3 |  val  | Loss: 2.7364 | Jaccard: 0.7297\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1398 | Jaccard: 0.6631\n",
            "Epoch 1/3 |  val  | Loss: 2.6898 | Jaccard: 0.7198\n",
            "Epoch 2/3 | train | Loss: 2.6771 | Jaccard: 0.7265\n",
            "Epoch 2/3 |  val  | Loss: 2.6757 | Jaccard: 0.7212\n",
            "Epoch 3/3 | train | Loss: 2.5577 | Jaccard: 0.7465\n",
            "Epoch 3/3 |  val  | Loss: 2.6760 | Jaccard: 0.7292\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1274 | Jaccard: 0.6672\n",
            "Epoch 1/3 |  val  | Loss: 2.7143 | Jaccard: 0.7222\n",
            "Epoch 2/3 | train | Loss: 2.6594 | Jaccard: 0.7260\n",
            "Epoch 2/3 |  val  | Loss: 2.6727 | Jaccard: 0.7174\n",
            "Epoch 3/3 | train | Loss: 2.5380 | Jaccard: 0.7485\n",
            "Epoch 3/3 |  val  | Loss: 2.6924 | Jaccard: 0.7217\n",
            "average jaccard: 0.7270\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "67  1598130645  0.724265  1598130645   True      0.5          4   True    4   \n",
            "68  1598132616  0.726972  1598132616   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "67   True         3   True       0.1   True  \n",
            "68   True         3   True       0.1   True  \n",
            "-----seeding 12, seed:1598134584\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1088 | Jaccard: 0.6604\n",
            "Epoch 1/3 |  val  | Loss: 2.6855 | Jaccard: 0.7213\n",
            "Epoch 2/3 | train | Loss: 2.6667 | Jaccard: 0.7239\n",
            "Epoch 2/3 |  val  | Loss: 2.6682 | Jaccard: 0.7235\n",
            "Epoch 3/3 | train | Loss: 2.5569 | Jaccard: 0.7410\n",
            "Epoch 3/3 |  val  | Loss: 2.6578 | Jaccard: 0.7324\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1353 | Jaccard: 0.6622\n",
            "Epoch 1/3 |  val  | Loss: 2.6876 | Jaccard: 0.7278\n",
            "Epoch 2/3 | train | Loss: 2.6618 | Jaccard: 0.7254\n",
            "Epoch 2/3 |  val  | Loss: 2.6905 | Jaccard: 0.7257\n",
            "Epoch 3/3 | train | Loss: 2.5376 | Jaccard: 0.7474\n",
            "Epoch 3/3 |  val  | Loss: 2.7052 | Jaccard: 0.7338\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.0541 | Jaccard: 0.6727\n",
            "Epoch 1/3 |  val  | Loss: 2.6654 | Jaccard: 0.7264\n",
            "Epoch 2/3 | train | Loss: 2.6637 | Jaccard: 0.7268\n",
            "Epoch 2/3 |  val  | Loss: 2.6706 | Jaccard: 0.7190\n",
            "Epoch 3/3 | train | Loss: 2.5555 | Jaccard: 0.7450\n",
            "Epoch 3/3 |  val  | Loss: 2.6446 | Jaccard: 0.7248\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.0594 | Jaccard: 0.6740\n",
            "Epoch 1/3 |  val  | Loss: 2.7350 | Jaccard: 0.7114\n",
            "Epoch 2/3 | train | Loss: 2.6442 | Jaccard: 0.7309\n",
            "Epoch 2/3 |  val  | Loss: 2.6923 | Jaccard: 0.7168\n",
            "Epoch 3/3 | train | Loss: 2.5213 | Jaccard: 0.7508\n",
            "Epoch 3/3 |  val  | Loss: 2.7195 | Jaccard: 0.7214\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1010 | Jaccard: 0.6683\n",
            "Epoch 1/3 |  val  | Loss: 2.7575 | Jaccard: 0.7101\n",
            "Epoch 2/3 | train | Loss: 2.6687 | Jaccard: 0.7259\n",
            "Epoch 2/3 |  val  | Loss: 2.6653 | Jaccard: 0.7306\n",
            "Epoch 3/3 | train | Loss: 2.5349 | Jaccard: 0.7480\n",
            "Epoch 3/3 |  val  | Loss: 2.7338 | Jaccard: 0.7303\n",
            "average jaccard: 0.7285\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "67  1598130645  0.724265  1598130645   True      0.5          4   True    4   \n",
            "68  1598132616  0.726972  1598132616   True      0.5          4   True    4   \n",
            "69  1598134584  0.728547  1598134584   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "67   True         3   True       0.1   True  \n",
            "68   True         3   True       0.1   True  \n",
            "69   True         3   True       0.1   True  \n",
            "-----seeding 13, seed:1598136552\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1084 | Jaccard: 0.6684\n",
            "Epoch 1/3 |  val  | Loss: 2.7235 | Jaccard: 0.7207\n",
            "Epoch 2/3 | train | Loss: 2.7218 | Jaccard: 0.7159\n",
            "Epoch 2/3 |  val  | Loss: 2.7455 | Jaccard: 0.7086\n",
            "Epoch 3/3 | train | Loss: 2.6158 | Jaccard: 0.7286\n",
            "Epoch 3/3 |  val  | Loss: 2.7454 | Jaccard: 0.7128\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0970 | Jaccard: 0.6668\n",
            "Epoch 1/3 |  val  | Loss: 2.7288 | Jaccard: 0.7186\n",
            "Epoch 2/3 | train | Loss: 2.6674 | Jaccard: 0.7252\n",
            "Epoch 2/3 |  val  | Loss: 2.6953 | Jaccard: 0.7253\n",
            "Epoch 3/3 | train | Loss: 2.5465 | Jaccard: 0.7493\n",
            "Epoch 3/3 |  val  | Loss: 2.7096 | Jaccard: 0.7284\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.2312 | Jaccard: 0.6496\n",
            "Epoch 1/3 |  val  | Loss: 3.1183 | Jaccard: 0.6817\n",
            "Epoch 2/3 | train | Loss: 2.8441 | Jaccard: 0.7004\n",
            "Epoch 2/3 |  val  | Loss: 2.7183 | Jaccard: 0.7227\n",
            "Epoch 3/3 | train | Loss: 2.6299 | Jaccard: 0.7303\n",
            "Epoch 3/3 |  val  | Loss: 2.6794 | Jaccard: 0.7263\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.0929 | Jaccard: 0.6706\n",
            "Epoch 1/3 |  val  | Loss: 2.6906 | Jaccard: 0.7197\n",
            "Epoch 2/3 | train | Loss: 2.6585 | Jaccard: 0.7281\n",
            "Epoch 2/3 |  val  | Loss: 2.6587 | Jaccard: 0.7247\n",
            "Epoch 3/3 | train | Loss: 2.5358 | Jaccard: 0.7490\n",
            "Epoch 3/3 |  val  | Loss: 2.7041 | Jaccard: 0.7199\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.0961 | Jaccard: 0.6651\n",
            "Epoch 1/3 |  val  | Loss: 2.7396 | Jaccard: 0.7204\n",
            "Epoch 2/3 | train | Loss: 2.6483 | Jaccard: 0.7280\n",
            "Epoch 2/3 |  val  | Loss: 2.6738 | Jaccard: 0.7245\n",
            "Epoch 3/3 | train | Loss: 2.5604 | Jaccard: 0.7437\n",
            "Epoch 3/3 |  val  | Loss: 2.6762 | Jaccard: 0.7205\n",
            "average jaccard: 0.7216\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "67  1598130645  0.724265  1598130645   True      0.5          4   True    4   \n",
            "68  1598132616  0.726972  1598132616   True      0.5          4   True    4   \n",
            "69  1598134584  0.728547  1598134584   True      0.5          4   True    4   \n",
            "70  1598136552  0.721560  1598136552   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "67   True         3   True       0.1   True  \n",
            "68   True         3   True       0.1   True  \n",
            "69   True         3   True       0.1   True  \n",
            "70   True         3   True       0.1   True  \n",
            "-----seeding 14, seed:1598138520\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.0480 | Jaccard: 0.6749\n",
            "Epoch 1/3 |  val  | Loss: 2.6994 | Jaccard: 0.7232\n",
            "Epoch 2/3 | train | Loss: 2.6515 | Jaccard: 0.7253\n",
            "Epoch 2/3 |  val  | Loss: 2.6829 | Jaccard: 0.7236\n",
            "Epoch 3/3 | train | Loss: 2.5294 | Jaccard: 0.7469\n",
            "Epoch 3/3 |  val  | Loss: 2.6732 | Jaccard: 0.7239\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.0858 | Jaccard: 0.6685\n",
            "Epoch 1/3 |  val  | Loss: 2.7190 | Jaccard: 0.7173\n",
            "Epoch 2/3 | train | Loss: 2.6711 | Jaccard: 0.7253\n",
            "Epoch 2/3 |  val  | Loss: 2.6933 | Jaccard: 0.7184\n",
            "Epoch 3/3 | train | Loss: 2.5636 | Jaccard: 0.7435\n",
            "Epoch 3/3 |  val  | Loss: 2.6955 | Jaccard: 0.7205\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1283 | Jaccard: 0.6588\n",
            "Epoch 1/3 |  val  | Loss: 2.6753 | Jaccard: 0.7293\n",
            "Epoch 2/3 | train | Loss: 2.6584 | Jaccard: 0.7248\n",
            "Epoch 2/3 |  val  | Loss: 2.6513 | Jaccard: 0.7287\n",
            "Epoch 3/3 | train | Loss: 2.5353 | Jaccard: 0.7470\n",
            "Epoch 3/3 |  val  | Loss: 2.6473 | Jaccard: 0.7237\n",
            "Fold: 4\n",
            "Epoch 1/3 | train | Loss: 3.1017 | Jaccard: 0.6661\n",
            "Epoch 1/3 |  val  | Loss: 2.7264 | Jaccard: 0.7215\n",
            "Epoch 2/3 | train | Loss: 2.6641 | Jaccard: 0.7245\n",
            "Epoch 2/3 |  val  | Loss: 2.6715 | Jaccard: 0.7213\n",
            "Epoch 3/3 | train | Loss: 2.5435 | Jaccard: 0.7458\n",
            "Epoch 3/3 |  val  | Loss: 2.6639 | Jaccard: 0.7267\n",
            "Fold: 5\n",
            "Epoch 1/3 | train | Loss: 3.1441 | Jaccard: 0.6570\n",
            "Epoch 1/3 |  val  | Loss: 2.7506 | Jaccard: 0.7197\n",
            "Epoch 2/3 | train | Loss: 2.6633 | Jaccard: 0.7266\n",
            "Epoch 2/3 |  val  | Loss: 2.7129 | Jaccard: 0.7194\n",
            "Epoch 3/3 | train | Loss: 2.5577 | Jaccard: 0.7440\n",
            "Epoch 3/3 |  val  | Loss: 2.7127 | Jaccard: 0.7269\n",
            "average jaccard: 0.7243\n",
            "/content/drive/My Drive/kaggle/input/seeds_finding/seeds_cv_2.csv saved\n",
            "          seed        CV   kfoldseed    MSD  MSDrate  MSDsample     ML  MLn  \\\n",
            "0   1597861868  0.724700          42  False      0.5          4  False    8   \n",
            "1   1597865404  0.723200          42  False      0.5          4  False    8   \n",
            "2   1597868937  0.725100          42  False      0.5          4  False    8   \n",
            "3   1597872469  0.723600          42  False      0.5          4  False    8   \n",
            "4   1597875998  0.721700          42  False      0.5          4  False    8   \n",
            "5   1597879523  0.722600          42  False      0.5          4  False    8   \n",
            "6   1597883058  0.720100          42  False      0.5          4  False    8   \n",
            "7   1597886598  0.722500          42  False      0.5          4  False    8   \n",
            "8   1597890135  0.719100          42  False      0.5          4  False    8   \n",
            "9   1597893672  0.724300          42  False      0.5          4  False    8   \n",
            "10  1597934993  0.727409          42  False      0.5          4  False    8   \n",
            "11  1597938555  0.724628          42  False      0.5          4  False    8   \n",
            "12  1597942089  0.723826          42  False      0.5          4  False    8   \n",
            "13  1597945635  0.725811          42  False      0.5          4  False    8   \n",
            "14  1597949181  0.722312          42  False      0.5          4  False    8   \n",
            "15  1597952720  0.726381          42  False      0.5          4  False    8   \n",
            "16  1597956254  0.712057          42  False      0.5          4  False    8   \n",
            "17  1597959787  0.726692          42  False      0.5          4  False    8   \n",
            "18  1597972729  0.718928          42  False      0.5          4  False    8   \n",
            "19  1597976281  0.721085          42  False      0.5          4  False    8   \n",
            "20  1597979806  0.724882          42  False      0.5          4  False    8   \n",
            "21  1597983330  0.725509          42  False      0.5          4  False    8   \n",
            "22  1597986857  0.720778          42  False      0.5          4  False    8   \n",
            "23  1597990398  0.722155          42  False      0.5          4  False    8   \n",
            "24  1597993923  0.725980          42  False      0.5          4  False    8   \n",
            "25  1597997448  0.727870          42  False      0.5          4  False    8   \n",
            "26  1598000976  0.722581          42  False      0.5          4  False    8   \n",
            "27  1598004519  0.723912          42  False      0.5          4  False    8   \n",
            "28  1598008047  0.723061          42  False      0.5          4  False    8   \n",
            "29  1598011569  0.725062          42  False      0.5          4  False    8   \n",
            "30  1598015117  0.721427          42  False      0.5          4  False    8   \n",
            "31  1598026008  0.725204  1598026008  False      0.5          4  False    8   \n",
            "32  1598029832  0.726231  1598029832  False      0.5          4  False    8   \n",
            "33  1598035261  0.723477  1598035261   True      0.5          4   True    4   \n",
            "34  1598038148  0.688531  1598038148   True      0.5          4   True    4   \n",
            "35  1598041102  0.726292  1598041102   True      0.5          4   True    4   \n",
            "36  1598044075  0.726274  1598044075   True      0.5          4   True    4   \n",
            "37  1598047050  0.723214  1598047050   True      0.5          4   True    4   \n",
            "38  1598050006  0.724705  1598050006   True      0.5          4   True    4   \n",
            "39  1598052922  0.725561  1598052922   True      0.5          4   True    4   \n",
            "40  1598055834  0.725697  1598055834   True      0.5          4   True    4   \n",
            "41  1598058795  0.720373  1598058795   True      0.5          4   True    4   \n",
            "42  1598061776  0.725639  1598061776   True      0.5          4   True    4   \n",
            "43  1598064738  0.720871  1598064738   True      0.5          4   True    4   \n",
            "44  1598067714  0.725709  1598067714   True      0.5          4   True    4   \n",
            "45  1598070685  0.724698  1598070685   True      0.5          4   True    4   \n",
            "46  1598073672  0.724749  1598073672   True      0.5          4   True    4   \n",
            "47  1598076675  0.721908  1598076675   True      0.5          4   True    4   \n",
            "48  1598079681  0.727865  1598079681   True      0.5          4   True    4   \n",
            "49  1598082690  0.716810  1598082690   True      0.5          4   True    4   \n",
            "50  1598085700  0.725604  1598085700   True      0.5          4   True    4   \n",
            "51  1598088694  0.723130  1598088694   True      0.5          4   True    4   \n",
            "52  1598091663  0.724898  1598091663   True      0.5          4   True    4   \n",
            "53  1598094635  0.724707  1598094635   True      0.5          4   True    4   \n",
            "54  1598097611  0.724062  1598097611   True      0.5          4   True    4   \n",
            "55  1598100603  0.724984  1598100603   True      0.5          4   True    4   \n",
            "56  1598103588  0.725538  1598103588   True      0.5          4   True    4   \n",
            "57  1598106587  0.728119  1598106587   True      0.5          4   True    4   \n",
            "58  1598112722  0.725116  1598112722   True      0.5          4   True    4   \n",
            "59  1598114730  0.722902  1598114730   True      0.5          4   True    4   \n",
            "60  1598116705  0.725286  1598116705   True      0.5          4   True    4   \n",
            "61  1598118700  0.724285  1598118700   True      0.5          4   True    4   \n",
            "62  1598120701  0.725065  1598120701   True      0.5          4   True    4   \n",
            "63  1598122712  0.722292  1598122712   True      0.5          4   True    4   \n",
            "64  1598124700  0.718809  1598124700   True      0.5          4   True    4   \n",
            "65  1598126686  0.724285  1598126686   True      0.5          4   True    4   \n",
            "66  1598128679  0.724719  1598128679   True      0.5          4   True    4   \n",
            "67  1598130645  0.724265  1598130645   True      0.5          4   True    4   \n",
            "68  1598132616  0.726972  1598132616   True      0.5          4   True    4   \n",
            "69  1598134584  0.728547  1598134584   True      0.5          4   True    4   \n",
            "70  1598136552  0.721560  1598136552   True      0.5          4   True    4   \n",
            "71  1598138520  0.724322  1598138520   True      0.5          4   True    4   \n",
            "\n",
            "      MLR  MLRtimes     SL  SLfactor     SB  \n",
            "0   False         3  False       0.1  False  \n",
            "1   False         3  False       0.1  False  \n",
            "2   False         3  False       0.1  False  \n",
            "3   False         3  False       0.1  False  \n",
            "4   False         3  False       0.1  False  \n",
            "5   False         3  False       0.1  False  \n",
            "6   False         3  False       0.1  False  \n",
            "7   False         3  False       0.1  False  \n",
            "8   False         3  False       0.1  False  \n",
            "9   False         3  False       0.1  False  \n",
            "10  False         3  False       0.1  False  \n",
            "11  False         3  False       0.1  False  \n",
            "12  False         3  False       0.1  False  \n",
            "13  False         3  False       0.1  False  \n",
            "14  False         3  False       0.1  False  \n",
            "15  False         3  False       0.1  False  \n",
            "16  False         3  False       0.1  False  \n",
            "17  False         3  False       0.1  False  \n",
            "18  False         3  False       0.1  False  \n",
            "19  False         3  False       0.1  False  \n",
            "20  False         3  False       0.1  False  \n",
            "21  False         3  False       0.1  False  \n",
            "22  False         3  False       0.1  False  \n",
            "23  False         3  False       0.1  False  \n",
            "24  False         3  False       0.1  False  \n",
            "25  False         3  False       0.1  False  \n",
            "26  False         3  False       0.1  False  \n",
            "27  False         3  False       0.1  False  \n",
            "28  False         3  False       0.1  False  \n",
            "29  False         3  False       0.1  False  \n",
            "30  False         3  False       0.1  False  \n",
            "31  False         3  False       0.1  False  \n",
            "32  False         3  False       0.1  False  \n",
            "33   True         3   True       0.1   True  \n",
            "34   True         3   True       0.1   True  \n",
            "35   True         3   True       0.1   True  \n",
            "36   True         3   True       0.1   True  \n",
            "37   True         3   True       0.1   True  \n",
            "38   True         3   True       0.1   True  \n",
            "39   True         3   True       0.1   True  \n",
            "40   True         3   True       0.1   True  \n",
            "41   True         3   True       0.1   True  \n",
            "42   True         3   True       0.1   True  \n",
            "43   True         3   True       0.1   True  \n",
            "44   True         3   True       0.1   True  \n",
            "45   True         3   True       0.1   True  \n",
            "46   True         3   True       0.1   True  \n",
            "47   True         3   True       0.1   True  \n",
            "48   True         3   True       0.1   True  \n",
            "49   True         3   True       0.1   True  \n",
            "50   True         3   True       0.1   True  \n",
            "51   True         3   True       0.1   True  \n",
            "52   True         3   True       0.1   True  \n",
            "53   True         3   True       0.1   True  \n",
            "54   True         3   True       0.1   True  \n",
            "55   True         3   True       0.1   True  \n",
            "56   True         3   True       0.1   True  \n",
            "57   True         3   True       0.1   True  \n",
            "58   True         3   True       0.1   True  \n",
            "59   True         3   True       0.1   True  \n",
            "60   True         3   True       0.1   True  \n",
            "61   True         3   True       0.1   True  \n",
            "62   True         3   True       0.1   True  \n",
            "63   True         3   True       0.1   True  \n",
            "64   True         3   True       0.1   True  \n",
            "65   True         3   True       0.1   True  \n",
            "66   True         3   True       0.1   True  \n",
            "67   True         3   True       0.1   True  \n",
            "68   True         3   True       0.1   True  \n",
            "69   True         3   True       0.1   True  \n",
            "70   True         3   True       0.1   True  \n",
            "71   True         3   True       0.1   True  \n",
            "-----seeding 15, seed:1598140489\n",
            "Fold: 1\n",
            "Epoch 1/3 | train | Loss: 3.1289 | Jaccard: 0.6595\n",
            "Epoch 1/3 |  val  | Loss: 2.7117 | Jaccard: 0.7275\n",
            "Epoch 2/3 | train | Loss: 2.7319 | Jaccard: 0.7176\n",
            "Epoch 2/3 |  val  | Loss: 3.6224 | Jaccard: 0.6451\n",
            "Epoch 3/3 | train | Loss: 2.6908 | Jaccard: 0.7260\n",
            "Epoch 3/3 |  val  | Loss: 2.7182 | Jaccard: 0.7206\n",
            "Fold: 2\n",
            "Epoch 1/3 | train | Loss: 3.1014 | Jaccard: 0.6675\n",
            "Epoch 1/3 |  val  | Loss: 2.7145 | Jaccard: 0.7214\n",
            "Epoch 2/3 | train | Loss: 2.6867 | Jaccard: 0.7260\n",
            "Epoch 2/3 |  val  | Loss: 2.6583 | Jaccard: 0.7263\n",
            "Epoch 3/3 | train | Loss: 2.5707 | Jaccard: 0.7413\n",
            "Epoch 3/3 |  val  | Loss: 2.6594 | Jaccard: 0.7225\n",
            "Fold: 3\n",
            "Epoch 1/3 | train | Loss: 3.1246 | Jaccard: 0.6634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-04defc85bed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\ntrain_df = pd.read_csv(ROOT_PATH +  \\'/input/tweet-sentiment-extraction/train.csv\\')\\n\\nif DEBUG_LOAD:\\n    train_df = train_df.head(n=1000)\\ntrain_df[\\'text\\'] = train_df[\\'text\\'].astype(str)\\ntrain_df[\\'selected_text\\'] = train_df[\\'selected_text\\'].astype(str)\\n\\nseed_jaccard = pd.read_csv(ROOT_PATH +  \"/input/seeds_finding/seeds_cv_%d.csv\" % NOTEBOOK_ID)\\nseeds = seed_jaccard[\\'seed\\'].tolist()\\nkfoldseeds = seed_jaccard[\\'kfoldseed\\'].tolist()\\nCVs = seed_jaccard[\\'CV\\'].tolist()\\n\\nMSDs = seed_jaccard[\\'MSD\\'].tolist()\\n\\nSBs = seed_jaccard[\\'SB\\'].tolist()\\n\\nMSDrates = seed_jaccard[\\'MSDrate\\'].tolist()\\nMSDsamples = seed_jaccard[\\'MSDsample\\'].tolist()\\nMLs = seed_jaccard[\\'ML\\'].tolist()\\nMLns = seed_jaccard[\\'MLn\\'].tolist()\\nMLs = seed_jaccard[\\'ML\\'].tolist()\\nMLRs = seed_jaccard[\\'MLR\\'].tolist()\\nMLRtimess = seed_jaccard[\\'MLRtimes\\'].tolist()\\nSLs = seed_jaccard[\\'SL\\'].tolist()\\nSLfactors = seed_jaccard[\\'SLfactor\\'].tolist()\\n\\nseeding_id = 0\\n\\nif not USE_TRY_MULTI_SEEDING:\\n  skf = StratifiedKFold(n_splits=2 if DEBUG_FOLD else FOLD_COUNT, shuffle=True, random_state=seed)\\n\\n\\nwhile(True):\\n  jc_sum = []\\n  seed = int(time.time())\\n\\n  seeding_id += 1\\n\\n  if USE_TRY_MULTI_SEEDING:\\n    print(f\"-----seeding {seeding_id}, seed:{seed}\")\\n    seed_everything(seed)\\n\\n    skf = Str...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-4b94836dd6bb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders_dict, criterion, optimizer, num_epochs, filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-752e6efb6eef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0;31m#tok_ = tok[:, :max_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUSE_BERT_ALL_LAYERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFL3hBPKcyub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PL1nfl-BmqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXPLORE = False\n",
        "\n",
        "if EXPLORE:\n",
        "\n",
        "    train_df_explore = pd.read_csv(ROOT_PATH +  '/input/tweet-sentiment-extraction/train.csv')\n",
        "\n",
        "    train_df_explore.head()\n",
        "    \n",
        "    train_df_explore['text'] = train_df_explore['text'].astype(str)\n",
        "    train_df_explore['selected_text'] = train_df_explore['selected_text'].astype(str)\n",
        "    \n",
        "    ds = TweetDataset(train_df_explore)\n",
        "    \n",
        "    \n",
        "    ds1 = TweetDataset(train_df_explore, use_fifth=False)\n",
        "    \n",
        "    same_count = 0\n",
        "    diff_count = 0\n",
        "    \n",
        "    for d1, d2 in zip(ds, ds1):\n",
        "        if d1['start_idx'] != d2['start_idx'] or  d1['end_idx'] != d2['end_idx']:\n",
        "            print(f\"tweet:[{d1['tweet']}] d1!=d2, select: [{d1['select']}]\")\n",
        "            print(\"d1:%s\" % d1['tweet'][d1['offsets'][d1['start_idx']][0]:d1['offsets'][d1['end_idx']][1]])\n",
        "            print(\"d2:%s\\n\" % d2['tweet'][d2['offsets'][d2['start_idx']][0]:d2['offsets'][d2['end_idx']][1]])\n",
        "            diff_count += 1\n",
        "        else:\n",
        "            same_count += 1\n",
        "    \n",
        "    print(f\"diff:{diff_count}, same:{same_count}\")\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ru7zVFpfmqQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(ds1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1x9WP5zhmqQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "if EXPLORE:\n",
        "    train_df_explore = train_df_explore.fillna(\" \")\n",
        "    train_df_explore[train_df_explore['text'].isna()]\n",
        "\n",
        "    print(train_df_explore.shape)\n",
        "\n",
        "    train_df_explore[train_df_explore['text'].str.contains(\"  \")].shape\n",
        "\n",
        "    half_word_ids = []\n",
        "\n",
        "    PRINT_CUT_OFF = 300\n",
        "\n",
        "    for aa, dd in zip(train_df_explore.iterrows(),ds) :\n",
        "        index, row = aa\n",
        "    \n",
        "        # first word in selected_text:\n",
        "        space_index = row['selected_text'].find(\" \")\n",
        "        first_word = \"\"\n",
        "        if space_index == -1:\n",
        "            first_word = row['selected_text']\n",
        "        else:\n",
        "            first_word = row['selected_text'][:space_index]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} first word:{first_word}\")\n",
        "        if len(first_word) == 0:\n",
        "            continue\n",
        "\n",
        "        # last word\n",
        "        space_index = row['selected_text'].rfind(\" \")\n",
        "        if space_index == -1:\n",
        "            last_word = row['selected_text']\n",
        "        else:\n",
        "            last_word = row['selected_text'][space_index+1:]\n",
        "\n",
        "        if index < 4:\n",
        "            print(f\"selected_text:{row['selected_text']} last word:{last_word}\")\n",
        "\n",
        "        half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        not_found = False\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(first_word, id_start)\n",
        "            if id0 == -1:\n",
        "                # not found continue\n",
        "                not_found = True\n",
        "                break\n",
        "\n",
        "            if id0 == 0 or not row['text'][id0-1].isalpha():\n",
        "                half_word = False\n",
        "                break\n",
        "            else:\n",
        "                half_word = True\n",
        "\n",
        "            id_start = id0+len(row['text'])\n",
        "\n",
        "        if not_found:\n",
        "            continue\n",
        "\n",
        "        if half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half first word found in [{row['text']}], word:[{first_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][1]])\n",
        "                \n",
        "\n",
        "            half_word_ids.append(index)\n",
        "\n",
        "        # for last word:\n",
        "        last_half_word = False\n",
        "        #find match in original text:\n",
        "        id_start = 0\n",
        "\n",
        "        while id_start < len(row['text']):\n",
        "            id0 = row['text'].find(last_word, id_start)\n",
        "            if index == 50:\n",
        "                '''\n",
        "                tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "                    vocab_file='../input/roberta-base/vocab.json', \n",
        "                    merges_file='../input/roberta-base/merges.txt', \n",
        "                    lowercase=True,\n",
        "                    add_prefix_space=True)\n",
        "\n",
        "                tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "                encoding = tokenizer.encode(tweet)\n",
        "                print(f\"encoding:{encoding}\")\n",
        "                print(f\"encoding:{tweet}\")\n",
        "                print(f\"encoding:{encoding.ids}\")\n",
        "                print(f\"encoding:{encoding.tokens}\")\n",
        "                print(f\"encoding:{encoding.offsets}\")\n",
        "\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[34:41])]}\")\n",
        "                print(f\"list: {[ord(i) for i in list(row.text[-7:])]}\")\n",
        "\n",
        "\n",
        "                print(f\"\\n{index}-id0:{id0}, found:{row['text'][id0:id0+len(last_word)]}; char:{row['text'][id0+len(last_word)]}, isalpha:{row['text'][id0+len(last_word)].isalpha()}\\n\")\n",
        "                '''\n",
        "                pass\n",
        "\n",
        "            if id0 == -1:\n",
        "                break\n",
        "            if id0+len(last_word)< len(row['text']) and not row['text'][id0+len(last_word)].isalpha():\n",
        "                last_half_word = False\n",
        "                break\n",
        "            elif id0+len(last_word) == len(row['text']):\n",
        "                last_half_word = False\n",
        "                break\n",
        "            else:\n",
        "                last_half_word = True\n",
        "\n",
        "            id_start = id0+len(last_word)\n",
        "\n",
        "        if last_half_word and not half_word:\n",
        "            if index < PRINT_CUT_OFF:\n",
        "                print(f\"\\n\\nindex:{index} tweet-[{row['text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} selected-[{row['selected_text']}]\")\n",
        "                print(f\"\\n\\nindex:{index} half last word found in [{row['text']}], word:[{last_word}]\")\n",
        "                print(f\"\\n\\nindex:{index} pp-[{prp(row.selected_text, row.text)}]\")\n",
        "                print(dd['tweet'][dd['offsets'][dd['start_idx']+4][0]:dd['offsets'][dd['end_idx']+4][0]])\n",
        "                \n",
        "            half_word_ids.append(index)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A64hmGU1mqRA",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rx8jYSScmqRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "\n",
        "test_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "test_df['text'] = test_df['text'].astype(str)\n",
        "test_loader = get_test_loader(test_df)\n",
        "predictions = []\n",
        "fold_predictions = []\n",
        "fold_prediction_scores = []\n",
        "\n",
        "for i in range(FOLD_COUNT):\n",
        "  fold_predictions.append([])\n",
        "  fold_prediction_scores.append([])\n",
        "\n",
        "models = []\n",
        "for fold in range(skf.n_splits):\n",
        "    \n",
        "    if DEBUG_FOLD:\n",
        "        if fold != 0:\n",
        "            print(f\"DEBUG infer skip fold:{fold}\")\n",
        "            continue\n",
        "            \n",
        "    model = TweetModel()\n",
        "    model.cuda()\n",
        "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "    \n",
        "same_count = 0\n",
        "diff_count = 0\n",
        "\n",
        "print(\"loading data...\")\n",
        "    \n",
        "fold_2_app_count = 0\n",
        "for data in test_loader:\n",
        "    ids = data['ids'].cuda()\n",
        "    masks = data['masks'].cuda()\n",
        "    tweet = data['tweet']\n",
        "    original_tweet = data['original_tweet']\n",
        "    offsets = data['offsets'].numpy()\n",
        "\n",
        "    start_logits_list = []\n",
        "    end_logits_list = []\n",
        "\n",
        "    #print(f\"{start_logits_list}\")\n",
        "    #print(f\"processing {data}\")\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            output = model(ids, masks)\n",
        "            start_logits_list.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
        "            end_logits_list.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
        "\n",
        "    start_logits = np.mean(start_logits_list, axis=0)\n",
        "    end_logits = np.mean(end_logits_list, axis=0)\n",
        "\n",
        "    \n",
        "    for i in range(len(ids)):    \n",
        "        start_pred = np.argmax(start_logits[i])\n",
        "        end_pred = np.argmax(end_logits[i])\n",
        "        \n",
        "        #print(f\"processing {i}\")\n",
        "        if start_pred > end_pred:\n",
        "            pred = tweet[i]\n",
        "        else:\n",
        "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "            try:\n",
        "                fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "            except:\n",
        "                fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                \n",
        "            \n",
        "            if pred!=fifth_pred:\n",
        "                diff_count += 1\n",
        "                #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "            else:\n",
        "                same_count += 1\n",
        "\n",
        "            pp_pred = pp(pred, original_tweet[i])\n",
        "            \n",
        "            pred = fifth_pred\n",
        "        predictions.append(pred)\n",
        "\n",
        "\n",
        "    # generate psuedo label lists from models:\n",
        "\n",
        "    #print(f\"len start {len(start_logits_list)}\")\n",
        "\n",
        "\n",
        "    for fold, start_logits, end_logits in zip(range(len(start_logits_list)), start_logits_list, end_logits_list):\n",
        "      #print(f\"fold: {fold} start_logits:{len(start_logits)} end_logits:{len(end_logits)}\")\n",
        "      for i in range(len(ids)):    \n",
        "          start_pred = np.argmax(start_logits[i])\n",
        "          start_pred_socre = np.amax(start_logits[i])\n",
        "          end_pred = np.argmax(end_logits[i])\n",
        "          end_pred_score = np.amax(end_logits[i])\n",
        "          \n",
        "          #print(f\"processing {i}\")\n",
        "          if start_pred > end_pred:\n",
        "              pred = tweet[i]\n",
        "          else:\n",
        "              pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "\n",
        "              try:\n",
        "                  fifth_pred = postprocess(original_tweet[i], offsets[i], start_pred, end_pred, data['sentiment'])\n",
        "              except:\n",
        "                  fifth_pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
        "                  \n",
        "              \n",
        "              if pred!=fifth_pred:\n",
        "                  diff_count += 1\n",
        "                  #print(f\"tweet:[{tweet[i]}], [{pred}]/[{fifth_pred}]\")\n",
        "              else:\n",
        "                  same_count += 1\n",
        "\n",
        "              pp_pred = pp(pred, original_tweet[i])\n",
        "              \n",
        "              pred = fifth_pred\n",
        "          fold_predictions[fold].append(pred)\n",
        "          fold_prediction_scores[fold].append((start_pred_socre + end_pred_score)/2.)\n",
        "\n",
        "print(f\"same:{same_count} diff:{diff_count}\")\n",
        "\n",
        "print(f\"fold_2_app_count:{fold_2_app_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L51vQ_V3XmCW",
        "colab_type": "text"
      },
      "source": [
        "# Gen psuedo label files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b3eS3TkXw2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if GEN_PSUEDO_LABELS:\n",
        "  ori_sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv')\n",
        "\n",
        "  for fold, predictions, scores in zip(range(len(fold_predictions)), fold_predictions, fold_prediction_scores):\n",
        "    sub_df = ori_sub_df.copy(True)\n",
        "\n",
        "    print(sub_df)\n",
        "    print(len(fold_predictions[0]))\n",
        "    print(len(fold_predictions[1]))\n",
        "    print(len(fold_predictions[2]))\n",
        "    print(len(fold_predictions[3]))\n",
        "    print(len(fold_predictions[4]))\n",
        "    sub_df['selected_text'] = predictions\n",
        "    sub_df['score'] = scores\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "    #sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "    sub_df = sub_df[sub_df.score > .35]\n",
        "\n",
        "    del sub_df['score']\n",
        "    sub_df.to_csv(\"psuedo_labels_fold_%d.csv\" % fold,  index=False)\n",
        "    sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0U922cmqRO",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0euGDHjmqRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "sub_df['selected_text'] = predictions\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "#sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}